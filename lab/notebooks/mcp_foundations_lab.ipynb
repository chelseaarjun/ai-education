{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Model Context Protocol (MCP) Foundations Lab\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "1. **Understand MCP Architecture** - The three-layer system: Host, Client, Server\n",
    "2. **Transform Tool Integration** - From direct function calls to standardized protocol\n",
    "3. **Build MCP Client** - Hands-on development using MCP Python SDK\n",
    "4. **Integrate with Agents** - Adapt CourseAssistantAgent to use MCP protocol\n",
    "5. **Experience Protocol Benefits** - Modularity, standardization, and process isolation\n",
    "\n",
    "## 🏗️ The Transformation Journey\n",
    "\n",
    "**Today's Mission:** Transform our direct tool integration into a standardized protocol approach!\n",
    "\n",
    "```\n",
    "BEFORE: Direct Tool Integration\n",
    "┌─────────────────┐\n",
    "│ CourseAssistant │──► search_content_tool()\n",
    "│ Agent           │\n",
    "└─────────────────┘\n",
    "\n",
    "AFTER: MCP Protocol Integration  \n",
    "┌─────────────────┐    ┌──────────┐    ┌──────────┐\n",
    "│ CourseAssistant │◄──►│   MCP    │◄──►│   MCP    │\n",
    "│ Agent (Host)    │    │ Client   │    │ Server   │\n",
    "└─────────────────┘    └──────────┘    └──────────┘\n",
    "                                            │\n",
    "                                       search_content_tool()\n",
    "```\n",
    "\n",
    "## 🧠 The \"Universal USB\" Analogy\n",
    "\n",
    "Think of MCP as **\"Universal USB for AI Tools\"**:\n",
    "- **Before USB**: Every device needed custom cables and drivers\n",
    "- **After USB**: Standardized interface, plug-and-play compatibility\n",
    "- **Before MCP**: Every AI app integrates tools differently\n",
    "- **After MCP**: Standardized protocol, tools work with any MCP-compatible app\n",
    "\n",
    "## ⏱️ Lab Timeline (60 minutes)\n",
    "- **Section 1**: Setup & Understanding MCP (10 min)\n",
    "- **Section 2**: Server Overview & Startup (10 min) \n",
    "- **Section 3**: Building MCP Client (20 min)\n",
    "- **Section 4**: Host Integration (15 min)\n",
    "- **Section 5**: Testing & Benefits Reflection (5 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Understanding MCP (10 minutes)\n",
    "\n",
    "Let's set up our environment and understand the MCP mental model before diving into the implementation!\n",
    "\n",
    "**[!NOTE]**\n",
    "> This notebook is designed to run in the `ai-education` Conda environment.\n",
    "> - If you have not already, open a terminal and run:\n",
    ">   ```\n",
    ">   conda env create -f environment.yml\n",
    ">   conda activate ai-education\n",
    ">   ````\n",
    ">   (Optional) Run the command below to register the environment as a Jupyter kernel and then select the \"Python (ai-education)\" kernel from the Jupyter kernel menu. \n",
    ">   ```\n",
    ">   python -m ipykernel install --user --name ai-education --display-name \"Python (ai-education)\"\n",
    ">   ```\n",
    "\n",
    "**AWS Credentials Setup:**\n",
    "- Set your credentials as environment variables in a cell (do NOT share credentials) unless you are using **AWS SageMaker** then Credentials are pre-configured in your environment. No action needed unless you want to override the default region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyBinder users: set your credentials here (do NOT share real keys)\n",
    "import os\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY'\n",
    "# os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'  # or your region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Libraries imported successfully!\n",
      "🕐 Lab start time: 01:19:53\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# MCP imports\n",
    "from mcp import ClientSession\n",
    "from mcp.client.stdio import stdio_client, StdioServerParameters\n",
    "\n",
    "# Course content dependencies (same as agents lab)\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "print(\"📚 Libraries imported successfully!\")\n",
    "print(\"🕐 Lab start time:\", datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Configuration & Course Content Setup\n",
    "\n",
    "Let's reuse the same course embeddings from the agents lab and set up our AWS configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌎 AWS Region: us-west-2\n",
      "📁 Embeddings file: ../embeddings/course_embeddings.json\n",
      "🧠 LLM Model: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "✅ Connected to AWS Bedrock successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration (same as agents lab)\n",
    "AWS_REGION = \"us-west-2\"\n",
    "EMBEDDINGS_FILE = \"../embeddings/course_embeddings.json\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v2:0\"\n",
    "LLM_MODEL = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "MCP_SERVER_PATH = \"../scripts/mcp_course_content_server.py\"\n",
    "\n",
    "print(f\"🌎 AWS Region: {AWS_REGION}\")\n",
    "print(f\"📁 Embeddings file: {EMBEDDINGS_FILE}\")\n",
    "print(f\"🧠 LLM Model: {LLM_MODEL}\")\n",
    "\n",
    "# Initialize AWS Bedrock client\n",
    "try:\n",
    "    bedrock_client = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "    print(\"✅ Connected to AWS Bedrock successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to AWS Bedrock: {e}\")\n",
    "    print(\"Please ensure your AWS credentials are configured correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Understanding Course Content (Quick Review)\n",
    "\n",
    "Let's quickly verify our course embeddings are available - same data we used in the agents lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded embeddings from ../embeddings/course_embeddings.json\n",
      "\n",
      "📈 Content Statistics:\n",
      "   📄 Files processed: 6\n",
      "   📝 Content chunks: 43\n",
      "   📊 Total words: 18,800\n",
      "   🧠 Embedding dimension: 1024\n",
      "\n",
      "🎯 Ready for MCP transformation!\n"
     ]
    }
   ],
   "source": [
    "def load_and_verify_embeddings(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load and verify course embeddings are available\"\"\"\n",
    "    try:\n",
    "        if not Path(file_path).exists():\n",
    "            print(f\"❌ Embeddings file not found: {file_path}\")\n",
    "            print(\"Please ensure you have the course embeddings from the agents lab\")\n",
    "            return {}\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"✅ Loaded embeddings from {file_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading embeddings: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load and verify embeddings\n",
    "embeddings_data = load_and_verify_embeddings(EMBEDDINGS_FILE)\n",
    "\n",
    "if embeddings_data:\n",
    "    metadata = embeddings_data['metadata']\n",
    "    chunks = embeddings_data['chunks']\n",
    "    \n",
    "    print(f\"\\n📈 Content Statistics:\")\n",
    "    print(f\"   📄 Files processed: {len(metadata['processed_files'])}\")\n",
    "    print(f\"   📝 Content chunks: {metadata['chunk_count']}\")\n",
    "    print(f\"   📊 Total words: {metadata['total_words']:,}\")\n",
    "    print(f\"   🧠 Embedding dimension: {metadata['embedding_dimension']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Ready for MCP transformation!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Cannot proceed without course embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 MCP Mental Model: The Three-Layer Architecture\n",
    "\n",
    "Before we start building, let's understand the key roles in MCP:\n",
    "\n",
    "### **🏠 Host (Your AI Application)**\n",
    "- **Role**: The orchestrator - decides what to do and when\n",
    "- **Example**: CourseAssistantAgent that needs to search content\n",
    "- **Responsibility**: Business logic, user interaction, tool orchestration\n",
    "\n",
    "### **📡 Client (Protocol Handler)**  \n",
    "- **Role**: The translator - converts between host requests and protocol messages\n",
    "- **Example**: MCP client that handles JSON-RPC communication\n",
    "- **Responsibility**: Protocol handling, connection management, error translation\n",
    "\n",
    "### **🔧 Server (Tool Provider)**\n",
    "- **Role**: The worker - provides actual functionality through standardized interface\n",
    "- **Example**: Server that exposes course content search as an MCP tool\n",
    "- **Responsibility**: Tool implementation, input validation, result formatting\n",
    "\n",
    "### **🔍 Why This Separation Matters:**\n",
    "- **Modularity**: Each component can be developed and deployed independently\n",
    "- **Standardization**: Tools work with any MCP-compatible application\n",
    "- **Security**: Clear boundaries between application logic and tool execution\n",
    "- **Scalability**: Servers can run anywhere (local process, remote service, containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🎉 Section 1 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ✅ MCP Python SDK installed and ready\n",
    "- ✅ Course embeddings loaded and verified\n",
    "- ✅ Clear understanding of MCP's three-layer architecture\n",
    "- ✅ Mental model of protocol benefits vs direct integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Server Overview & Startup (10 minutes)\n",
    "\n",
    "Now let's examine our pre-built MCP server and get it running! This server transforms our course content search into a standardized MCP tool.\n",
    "\n",
    "## 📄 The Pre-Built Server\n",
    "\n",
    "We've created a complete `course_content_server.py` that you'll find in your lab files. Let's examine its key components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Key Server Components Explained\n",
    "\n",
    "Let's understand the important parts of our MCP server:\n",
    "\n",
    "### **1. FastMCP Server Creation**\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "mcp = FastMCP(\"course-content-server\")\n",
    "```\n",
    "- Creates a standardized MCP server with automatic protocol handling\n",
    "- Server name identifies it in MCP communications\n",
    "\n",
    "### **2. Tool Definition with Decorators** \n",
    "```python\n",
    "@mcp.tool()\n",
    "def search_content(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"Tool description for LLMs\"\"\"\n",
    "    # Implementation here\n",
    "```\n",
    "- `@mcp.tool()` decorator automatically exposes function as MCP tool\n",
    "- Type hints become JSON schema for validation\n",
    "- Docstring becomes tool description for LLMs\n",
    "\n",
    "### **3. Process Isolation**\n",
    "- Server runs as separate Python process\n",
    "- Communicates via stdio (stdin/stdout)\n",
    "- Clear security and resource boundaries\n",
    "\n",
    "### **4. Same Search Logic**\n",
    "- Reuses identical FAISS and embedding code from agents lab\n",
    "- Same AWS Bedrock integration\n",
    "- Same course content data\n",
    "\n",
    "**The key insight:** We've wrapped our existing search functionality in a standardized protocol interface!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Server Startup Test\n",
    "\n",
    "Let's start our MCP server and verify it initializes correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing MCP server startup...\n",
      "📍 Starting server process...\n",
      "⏳ Waiting for server initialization...\n",
      "✅ Server started successfully and is running\n",
      "🔍 Server process ID: 68020\n",
      "🛑 Server stopped cleanly\n",
      "\n",
      "🎯 Server test successful! Ready to build MCP client.\n"
     ]
    }
   ],
   "source": [
    "def test_server_startup():\n",
    "    \"\"\"Test that our MCP server starts up correctly\"\"\"\n",
    "    print(\"🚀 Testing MCP server startup...\")\n",
    "    \n",
    "    try:\n",
    "        # Get absolute path to the server file\n",
    "        server_path = os.path.abspath(MCP_SERVER_PATH)\n",
    "        \n",
    "        # Start server process\n",
    "        print(\"📍 Starting server process...\")\n",
    "        server_process = subprocess.Popen(\n",
    "            [\"python\", server_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Give it time to initialize\n",
    "        print(\"⏳ Waiting for server initialization...\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Check if process is still running (didn't crash)\n",
    "        if server_process.poll() is None:\n",
    "            print(\"✅ Server started successfully and is running\")\n",
    "            print(\"🔍 Server process ID:\", server_process.pid)\n",
    "            \n",
    "            # Terminate the server\n",
    "            server_process.terminate()\n",
    "            server_process.wait(timeout=5)\n",
    "            print(\"🛑 Server stopped cleanly\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            # Server crashed\n",
    "            stdout, stderr = server_process.communicate()\n",
    "            print(\"❌ Server failed to start\")\n",
    "            print(\"STDOUT:\", stdout[:500] if stdout else \"None\")\n",
    "            print(\"STDERR:\", stderr[:500] if stderr else \"None\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing server startup: {e}\")\n",
    "        \n",
    "        # Clean up any remaining process\n",
    "        try:\n",
    "            if 'server_process' in locals():\n",
    "                server_process.terminate()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "startup_success = test_server_startup()\n",
    "\n",
    "if startup_success:\n",
    "    print(\"\\n🎯 Server test successful! Ready to build MCP client.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Server test failed. Please check AWS credentials and embeddings file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 What Just Happened?\n",
    "\n",
    "We've successfully demonstrated:\n",
    "\n",
    "1. **Process Isolation**: The server runs as a separate Python process\n",
    "2. **Standardized Interface**: FastMCP handles all the protocol details\n",
    "3. **Tool Exposure**: Our search function is now available as an MCP tool\n",
    "4. **Same Functionality**: Identical search capabilities as the agents lab\n",
    "\n",
    "**Key Insight**: The server transforms our function into a network service that speaks a standardized protocol!\n",
    "\n",
    "**🎉 Section 2 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ✅ Understanding of MCP server architecture using FastMCP\n",
    "- ✅ Working server that exposes course content search as standardized tool\n",
    "- ✅ Verification that server starts up and initializes correctly\n",
    "- ✅ Foundation ready for client development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Building MCP Client (20 minutes)\n",
    "\n",
    "Now for the main event - building our MCP client! This is where you'll gain hands-on experience with the protocol communication patterns.\n",
    "\n",
    "## 🎯 Client Role Recap\n",
    "\n",
    "The MCP client acts as a **protocol translator**:\n",
    "- **Converts** host requests into MCP protocol messages\n",
    "- **Manages** connection lifecycle with the server process  \n",
    "- **Handles** async communication via stdio transport\n",
    "- **Translates** MCP responses back to host-friendly format\n",
    "\n",
    "## 🔧 Client Implementation Strategy\n",
    "\n",
    "We'll build a client class that:\n",
    "1. **Connects** to our server via stdio transport\n",
    "2. **Discovers** available tools through capability negotiation\n",
    "3. **Invokes** tools using the MCP protocol\n",
    "4. **Handles** errors and connection management\n",
    "\n",
    "Let's start building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCP Client class implemented!\n",
      "🎯 Ready to test client functionality\n"
     ]
    }
   ],
   "source": [
    "class CourseContentMCPClient:\n",
    "    \"\"\"\n",
    "    MCP Client for course content search\n",
    "    \n",
    "    This client demonstrates the key patterns for MCP communication:\n",
    "    - Server connection via stdio transport\n",
    "    - Tool discovery and capability negotiation\n",
    "    - Async tool invocation\n",
    "    - Error handling and connection management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, server_script_path: str = MCP_SERVER_PATH):\n",
    "        \"\"\"\n",
    "        Initialize the MCP client\n",
    "        \n",
    "        Args:\n",
    "            server_script_path: Path to the MCP server script\n",
    "        \"\"\"\n",
    "        self.server_params = StdioServerParameters(\n",
    "            command=\"python\",\n",
    "            args=[server_script_path]\n",
    "        )\n",
    "        self.available_tools = []\n",
    "        print(f\"📡 MCP Client initialized for server: {server_script_path}\")\n",
    "    \n",
    "    async def discover_tools(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Discover available tools from the server\n",
    "        \n",
    "        This demonstrates the MCP capability negotiation process.\n",
    "        \n",
    "        Returns:\n",
    "            List of available tool names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"🔍 Discovering available tools from server...\")\n",
    "            \n",
    "            # Connect to server via stdio transport\n",
    "            async with stdio_client(self.server_params) as (read_stream, write_stream):\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    # Initialize the connection\n",
    "                    await session.initialize()\n",
    "                    print(\"✅ Connection established with server\")\n",
    "                    \n",
    "                    # List available tools\n",
    "                    tools_response = await session.list_tools()\n",
    "                    \n",
    "                    # Extract tool names\n",
    "                    tool_names = [tool.name for tool in tools_response.tools]\n",
    "                    self.available_tools = tool_names\n",
    "                    \n",
    "                    print(f\"🎯 Found {len(tool_names)} available tools:\")\n",
    "                    for tool in tools_response.tools:\n",
    "                        print(f\"   - {tool.name}: {tool.description}\")\n",
    "                    \n",
    "                    return tool_names\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error discovering tools: {e}\")\n",
    "            return []\n",
    "    \n",
    "    async def search_content(self, query: str, max_results: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Search course content using the MCP search_content tool\n",
    "        \n",
    "        This demonstrates the core MCP tool invocation pattern.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            max_results: Maximum number of results\n",
    "            \n",
    "        Returns:\n",
    "            Search results as formatted string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"🔍 Searching for: '{query[:50]}{'...' if len(query) > 50 else ''}'\")\n",
    "            \n",
    "            # Connect to server\n",
    "            async with stdio_client(self.server_params) as (read_stream, write_stream):\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    # Initialize connection\n",
    "                    await session.initialize()\n",
    "                    \n",
    "                    # Call the search_content tool\n",
    "                    result = await session.call_tool(\n",
    "                        \"search_content\",\n",
    "                        {\n",
    "                            \"query\": query,\n",
    "                            \"max_results\": max_results\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Extract the content from the result\n",
    "                    if result.content:\n",
    "                        # MCP results come wrapped in content objects\n",
    "                        content = result.content[0].text if result.content else \"No content returned\"\n",
    "                        print(\"✅ Search completed successfully\")\n",
    "                        return content\n",
    "                    else:\n",
    "                        print(\"⚠️ No content in search result\")\n",
    "                        return \"No results found\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error during search: {e}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    async def get_server_status(self) -> str:\n",
    "        \"\"\"\n",
    "        Get server status using the MCP get_server_status tool\n",
    "        \n",
    "        This demonstrates tool invocation for server health checks.\n",
    "        \n",
    "        Returns:\n",
    "            Server status information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"📊 Checking server status...\")\n",
    "            \n",
    "            async with stdio_client(self.server_params) as (read_stream, write_stream):\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    await session.initialize()\n",
    "                    \n",
    "                    result = await session.call_tool(\"get_server_status\", {})\n",
    "                    \n",
    "                    if result.content:\n",
    "                        content = result.content[0].text if result.content else \"No status available\"\n",
    "                        return content\n",
    "                    else:\n",
    "                        return \"No status information available\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error checking server status: {e}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "print(\"✅ MCP Client class implemented!\")\n",
    "print(\"🎯 Ready to test client functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Testing Our MCP Client\n",
    "\n",
    "Let's test our client by connecting to the server and discovering tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 MCP Client initialized for server: ../scripts/mcp_course_content_server.py\n",
      "🔍 Testing Tool Discovery\n",
      "========================================\n",
      "🔍 Discovering available tools from server...\n",
      "✅ Connection established with server\n",
      "🎯 Found 2 available tools:\n",
      "   - search_content: \n",
      "    Search course content using semantic similarity\n",
      "    \n",
      "    This tool searches through AI/ML course materials to find relevant content\n",
      "    based on the provided query. It uses vector embeddings and semantic similarity\n",
      "    to return the most relevant sections.\n",
      "    \n",
      "    Args:\n",
      "        query: The search query - what you want to find in the course content\n",
      "        max_results: Maximum number of results to return (default: 3, max: 10)\n",
      "    \n",
      "    Returns:\n",
      "        Formatted search results with titles, sources, and content snippets\n",
      "    \n",
      "   - get_server_status: \n",
      "    Get the current status of the course content server\n",
      "    \n",
      "    Returns information about the server state, including whether it's properly\n",
      "    initialized and ready to handle search requests.\n",
      "    \n",
      "    Returns:\n",
      "        Server status information\n",
      "    \n",
      "\n",
      "✅ Successfully discovered 2 tools!\n",
      "🎯 Client is ready for tool invocation\n"
     ]
    }
   ],
   "source": [
    "# Create client instance\n",
    "mcp_client = CourseContentMCPClient()\n",
    "\n",
    "# Test tool discovery\n",
    "print(\"🔍 Testing Tool Discovery\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Run async function in notebook\n",
    "available_tools = await mcp_client.discover_tools()\n",
    "\n",
    "if available_tools:\n",
    "    print(f\"\\n✅ Successfully discovered {len(available_tools)} tools!\")\n",
    "    print(\"🎯 Client is ready for tool invocation\")\n",
    "else:\n",
    "    print(\"\\n❌ Tool discovery failed\")\n",
    "    print(\"Please ensure the server is working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Testing Tool Invocation\n",
    "\n",
    "Now let's test actual tool calls through the MCP protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Testing Server Status Check\n",
      "========================================\n",
      "📊 Checking server status...\n",
      "Server Status Response:\n",
      "✅ Server Status: READY\n",
      "📊 Content chunks loaded: 43\n",
      "🌎 AWS Region: us-west-2\n",
      "🧠 Embedding Model: amazon.titan-embed-text-v2:0\n",
      "🔍 Ready to handle search requests\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔍 Testing Content Search\n",
      "========================================\n",
      "🔍 Searching for: 'What are the key differences between agents and LL...'\n",
      "✅ Search completed successfully\n",
      "Search Query: What are the key differences between agents and LLMs?\n",
      "\n",
      "Search Results:\n",
      "------------------------------\n",
      "Found 2 relevant content sections for 'What are the key differences between agents and LLMs?':\n",
      "\n",
      "1. **From LLMs to Agents: Why Go Further?** (Relevance: 0.628)\n",
      "   Source: agents.html\n",
      "   Content: ## From LLMs to Agents: Why Go Further?\n",
      "\n",
      "While LLMs are incredibly versatile, many real-world applications require more than just language understanding. This is where LLM-powered agents come in.\n",
      "\n",
      "🤖 **Agentic LLM application** is a software system that wraps around the LLM, operating in a loop—observing i...\n"
     ]
    }
   ],
   "source": [
    "# Test server status check\n",
    "print(\"📊 Testing Server Status Check\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "status_result = await mcp_client.get_server_status()\n",
    "print(\"Server Status Response:\")\n",
    "print(status_result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test content search\n",
    "print(\"🔍 Testing Content Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_query = \"What are the key differences between agents and LLMs?\"\n",
    "search_result = await mcp_client.search_content(test_query, max_results=2)\n",
    "\n",
    "print(\"Search Query:\", test_query)\n",
    "print(\"\\nSearch Results:\")\n",
    "print(\"-\" * 30)\n",
    "print(search_result[:500] + \"...\" if len(search_result) > 500 else search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Understanding the MCP Communication Flow\n",
    "\n",
    "Let's examine what just happened:\n",
    "\n",
    "### **1. Connection Establishment**\n",
    "```python\n",
    "async with stdio_client(server_params) as (read_stream, write_stream):\n",
    "    async with ClientSession(read_stream, write_stream) as session:\n",
    "```\n",
    "- **stdio_client**: Creates subprocess and connects via stdin/stdout\n",
    "- **ClientSession**: Manages MCP protocol lifecycle\n",
    "\n",
    "### **2. Capability Negotiation**  \n",
    "```python\n",
    "await session.initialize()\n",
    "tools_response = await session.list_tools()\n",
    "```\n",
    "- **initialize()**: Handshake between client and server\n",
    "- **list_tools()**: Discover what tools the server provides\n",
    "\n",
    "### **3. Tool Invocation**\n",
    "```python\n",
    "result = await session.call_tool(\"search_content\", {\"query\": query})\n",
    "```\n",
    "- **call_tool()**: JSON-RPC call with tool name and arguments\n",
    "- **result.content**: Server response wrapped in MCP format\n",
    "\n",
    "### **🔑 Key Differences from Direct Tool Calls:**\n",
    "- **Async**: All operations are asynchronous (vs sync function calls)\n",
    "- **Protocol**: Structured JSON-RPC communication (vs Python function calls)\n",
    "- **Process Boundary**: Client and server in separate processes (vs same process)\n",
    "- **Discovery**: Tools are discovered at runtime (vs compile-time imports)\n",
    "- **Standardization**: Any MCP client can use any MCP server (vs tight coupling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🎉 Section 3 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ✅ **Working MCP client** that communicates with server via protocol\n",
    "- ✅ **Understanding of async patterns** and protocol communication\n",
    "- ✅ **Experience with tool discovery** and capability negotiation\n",
    "- ✅ **Hands-on MCP integration** ready for host application\n",
    "- ✅ **Protocol vs direct integration** comparison and insights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Host Integration (15 minutes)\n",
    "\n",
    "Time for the final transformation! Let's integrate our MCP client with the CourseAssistantAgent and see the complete protocol-based system in action.\n",
    "\n",
    "## 🎯 The Transformation Goal\n",
    "\n",
    "We're adapting the CourseAssistantAgent from the agents lab to use MCP protocol instead of direct tool calls:\n",
    "\n",
    "**Before (Agents Lab):**\n",
    "```python\n",
    "search_results = search_content_tool(user_input)  # Direct function call\n",
    "response = self._call_llm(prompt)  # Sync operation\n",
    "```\n",
    "\n",
    "**After (MCP Protocol):**\n",
    "```python\n",
    "search_results = await mcp_client.search_content(user_input)  # Protocol call\n",
    "response = self._call_llm(prompt)  # LLM call stays the same\n",
    "```\n",
    "\n",
    "## 🔧 Building the MCP-Enabled Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCP-enabled CourseAssistantAgent implemented!\n",
      "🎯 Ready for testing and comparison\n"
     ]
    }
   ],
   "source": [
    "class MCPCourseAssistantAgent:\n",
    "    \"\"\"\n",
    "    CourseAssistantAgent adapted to use MCP protocol\n",
    "    \n",
    "    This demonstrates how to transform an agent from direct tool integration\n",
    "    to standardized MCP protocol integration while maintaining the same\n",
    "    core functionality and user experience.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mcp_client: CourseContentMCPClient):\n",
    "        \"\"\"\n",
    "        Initialize agent with MCP client\n",
    "        \n",
    "        Args:\n",
    "            mcp_client: Configured MCP client for tool communication\n",
    "        \"\"\"\n",
    "        self.mcp_client = mcp_client\n",
    "        \n",
    "        # AWS Bedrock client for LLM calls (same as agents lab)\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "        \n",
    "        print(\"🤖 MCP-enabled Course Assistant Agent initialized!\")\n",
    "        print(\"   📡 Using MCP protocol for tool communication\")\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:\n",
    "        \"\"\"\n",
    "        Helper method to call Claude via Bedrock\n",
    "        \n",
    "        This is identical to the agents lab implementation - \n",
    "        LLM integration doesn't change when using MCP!\n",
    "        \"\"\"\n",
    "        try:\n",
    "            request_body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "\n",
    "            response = self.bedrock_client.invoke_model(\n",
    "                modelId=LLM_MODEL,\n",
    "                body=json.dumps(request_body),\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            return response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error calling LLM: {str(e)}\"\n",
    "    \n",
    "    async def decide_and_act(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Main agent decision cycle using MCP protocol\n",
    "        \n",
    "        Key differences from agents lab version:\n",
    "        1. Method is now async (MCP requires async)\n",
    "        2. Tool calls go through MCP client instead of direct functions\n",
    "        3. Same orchestration logic and LLM integration\n",
    "        \n",
    "        Args:\n",
    "            user_input: The user's question or request\n",
    "            \n",
    "        Returns:\n",
    "            Agent's response\n",
    "        \"\"\"\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return \"I'd be happy to help! Please ask me a question about the course content.\"\n",
    "        \n",
    "        # OBSERVE: Analyze the user's input (same as agents lab)\n",
    "        print(f\"🔍 Agent observing: '{user_input}'\")\n",
    "        \n",
    "        # PLAN: Simple plan - search content then respond (same logic as agents lab)\n",
    "        print(\"📋 Agent planning: Will search content via MCP and provide response\")\n",
    "        \n",
    "        # ACT: Execute the plan\n",
    "        \n",
    "        # Step 1: Search for relevant content using MCP protocol\n",
    "        print(\"⚡ Agent acting: Searching course content via MCP...\")\n",
    "        try:\n",
    "            search_results = await self.mcp_client.search_content(user_input, max_results=3)\n",
    "        except Exception as e:\n",
    "            search_results = f\"Error accessing search tool: {e}\"\n",
    "        \n",
    "        # Step 2: Generate response using search results (same as agents lab)\n",
    "        print(\"⚡ Agent acting: Generating response with found content...\")\n",
    "        \n",
    "        response_prompt = f\"\"\"You are a helpful course assistant for an AI/ML education program.\n",
    "\n",
    "A student asked: \"{user_input}\"\n",
    "\n",
    "Here's relevant content from the course materials:\n",
    "{search_results}\n",
    "\n",
    "Based on this content, provide a clear, helpful answer to the student's question. \n",
    "Focus on the key concepts and make it educational. If the search results don't \n",
    "contain relevant information, say so politely and offer to help with other topics.\n",
    "\n",
    "Keep your response concise but complete (2-3 paragraphs maximum).\"\"\"\n",
    "        \n",
    "        main_response = self._call_llm(response_prompt, max_tokens=400)\n",
    "        \n",
    "        return main_response\n",
    "\n",
    "print(\"✅ MCP-enabled CourseAssistantAgent implemented!\")\n",
    "print(\"🎯 Ready for testing and comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Testing the MCP-Enabled Agent\n",
    "\n",
    "Let's test our MCP-enabled agent and see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 MCP-enabled Course Assistant Agent initialized!\n",
      "   📡 Using MCP protocol for tool communication\n",
      "🤖 Testing MCP-Enabled Course Assistant Agent\n",
      "\n",
      "User: What are the main components of an AI agent?\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔍 Agent observing: 'What are the main components of an AI agent?'\n",
      "📋 Agent planning: Will search content via MCP and provide response\n",
      "⚡ Agent acting: Searching course content via MCP...\n",
      "🔍 Searching for: 'What are the main components of an AI agent?'\n",
      "✅ Search completed successfully\n",
      "⚡ Agent acting: Generating response with found content...\n",
      "\n",
      "Agent Response:\n",
      "--------------------\n",
      "I notice that the provided course materials don't directly answer the question about the main components of an AI agent. However, I can provide a clear, foundational answer based on standard AI principles:\n",
      "\n",
      "An AI agent typically consists of four main components: sensors, actuators, knowledge base, and reasoning engine. Sensors allow the agent to perceive its environment (like a chatbot receiving text input or a robot's cameras). Actuators enable the agent to act on its environment (like generating text responses or moving robot arms). The knowledge base contains the information and rules the agent uses to operate (such as an LLM's training data or a set of predefined rules). Finally, the reasoning engine processes inputs and decides what actions to take based on the knowledge base.\n",
      "\n",
      "From the course materials, we can see that modern AI agents can be extended with tools that serve as additional capabilities, similar to giving the agent new \"limbs and senses.\" These tools allow agents to interact with external systems and perform specific tasks beyond their core capabilities.\n",
      "\n",
      "Would you like me to explain any of these components in more detail?\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the MCP-enabled agent\n",
    "mcp_agent = MCPCourseAssistantAgent(mcp_client)\n",
    "\n",
    "print(\"🤖 Testing MCP-Enabled Course Assistant Agent\\n\")\n",
    "\n",
    "# Test with a course-related question\n",
    "test_question = \"What are the main components of an AI agent?\"\n",
    "\n",
    "print(f\"User: {test_question}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the agent (note: await because it's now async)\n",
    "response = await mcp_agent.decide_and_act(test_question)\n",
    "\n",
    "print(f\"\\nAgent Response:\")\n",
    "print(\"-\" * 20)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Real-World Integration Patterns\n",
    "\n",
    "Let's test a few more scenarios to understand when MCP shines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Multiple Question Testing\n",
      "\n",
      "🎯 Test 1: How does prompt engineering improve AI outputs?\n",
      "--------------------------------------------------\n",
      "🔍 Agent observing: 'How does prompt engineering improve AI outputs?'\n",
      "📋 Agent planning: Will search content via MCP and provide response\n",
      "⚡ Agent acting: Searching course content via MCP...\n",
      "🔍 Searching for: 'How does prompt engineering improve AI outputs?'\n",
      "✅ Search completed successfully\n",
      "⚡ Agent acting: Generating response with found content...\n",
      "Response: Based on the course materials, prompt engineering improves AI outputs by providing better context and instructions to AI models through carefully craf...\n",
      "\n",
      "\n",
      "🎯 Test 2: What makes MCP different from direct tool integration?\n",
      "--------------------------------------------------\n",
      "🔍 Agent observing: 'What makes MCP different from direct tool integration?'\n",
      "📋 Agent planning: Will search content via MCP and provide response\n",
      "⚡ Agent acting: Searching course content via MCP...\n",
      "🔍 Searching for: 'What makes MCP different from direct tool integrat...'\n",
      "✅ Search completed successfully\n",
      "⚡ Agent acting: Generating response with found content...\n",
      "Response: Based on the course materials, I can explain a key differentiating aspect of MCP from direct tool integration: its architectural approach and standard...\n",
      "\n",
      "\n",
      "🎯 Test 3: What are the benefits of using vector embeddings?\n",
      "--------------------------------------------------\n",
      "🔍 Agent observing: 'What are the benefits of using vector embeddings?'\n",
      "📋 Agent planning: Will search content via MCP and provide response\n",
      "⚡ Agent acting: Searching course content via MCP...\n",
      "🔍 Searching for: 'What are the benefits of using vector embeddings?'\n",
      "✅ Search completed successfully\n",
      "⚡ Agent acting: Generating response with found content...\n",
      "Response: Based on the course materials, vector embeddings offer several key benefits by representing words or concepts as numerical vectors in a mathematical s...\n",
      "\n",
      "\n",
      "✅ All tests completed successfully!\n",
      "🎯 MCP integration working as expected\n"
     ]
    }
   ],
   "source": [
    "# Test multiple questions to see the agent in action\n",
    "print(\"🧪 Multiple Question Testing\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"How does prompt engineering improve AI outputs?\",\n",
    "    \"What makes MCP different from direct tool integration?\",\n",
    "    \"What are the benefits of using vector embeddings?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"🎯 Test {i}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = await mcp_agent.decide_and_act(question)\n",
    "    \n",
    "    # Show first 150 characters of response\n",
    "    preview = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    print(f\"Response: {preview}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"✅ All tests completed successfully!\")\n",
    "print(\"🎯 MCP integration working as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🎉 Section 4 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ✅ **Complete MCP-enabled CourseAssistantAgent** with same functionality as agents lab\n",
    "- ✅ **Understanding of async agent patterns** and protocol integration\n",
    "- ✅ **Side-by-side comparison** of direct vs MCP approaches\n",
    "- ✅ **Real-world integration experience** with working examples\n",
    "- ✅ **Insight into when to use MCP** vs direct integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Benefits & Production Considerations (5 minutes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 MCP Benefits Demonstrated\n",
    "\n",
    "Throughout this lab, we've experienced the key benefits of MCP:\n",
    "\n",
    "### **1. 🔧 Modularity**\n",
    "- **Server Development**: Independent of client applications\n",
    "- **Tool Updates**: Server changes don't break existing clients\n",
    "- **Language Independence**: Server could be Python, client could be JavaScript\n",
    "\n",
    "### **2. 📐 Standardization** \n",
    "- **Universal Interface**: Any MCP client can use any MCP server\n",
    "- **Tool Discovery**: Runtime discovery vs compile-time dependencies\n",
    "- **Protocol Evolution**: Versioned protocol for future compatibility\n",
    "\n",
    "### **3. 🛡️ Security & Isolation**\n",
    "- **Process Boundaries**: Server runs in separate process with clear resource limits\n",
    "- **Controlled Communication**: Only structured protocol messages, no direct memory access\n",
    "- **Error Isolation**: Server crashes don't take down the host application\n",
    "\n",
    "### **4. 🚀 Scalability**\n",
    "- **Remote Deployment**: Servers can run anywhere (local, remote, containers)\n",
    "- **Load Distribution**: Multiple server instances for high availability\n",
    "- **Transport Flexibility**: stdio (development) → HTTP (production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Production Considerations\n",
    "\n",
    "When moving to production, consider these MCP deployment patterns:\n",
    "\n",
    "### **🌐 Transport Upgrades**\n",
    "- **Development**: stdio transport (what we used)\n",
    "- **Production**: HTTP transport with authentication\n",
    "- **Enterprise**: OAuth 2.1 with proper token management\n",
    "\n",
    "### **⚖️ Scaling Strategies**\n",
    "- **Single Server**: Good for development and small deployments\n",
    "- **Load Balanced**: Multiple server instances behind load balancer\n",
    "- **Microservices**: Different tools as separate MCP services\n",
    "- **Container Orchestration**: Kubernetes deployments with auto-scaling\n",
    "\n",
    "### **🔒 Security Enhancements**\n",
    "- **Authentication**: API keys or OAuth for server access\n",
    "- **Authorization**: Role-based access to different tools\n",
    "- **Rate Limiting**: Prevent abuse of expensive operations\n",
    "- **Audit Logging**: Track all tool invocations for compliance\n",
    "\n",
    "### **📈 Monitoring & Observability**\n",
    "- **Health Checks**: Server status and tool availability\n",
    "- **Performance Metrics**: Latency, throughput, error rates\n",
    "- **Tool Usage Analytics**: Which tools are used most frequently\n",
    "- **Error Tracking**: Protocol errors, tool failures, timeouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 When to Choose MCP vs Direct Integration\n",
    "\n",
    "**✅ Choose MCP When:**\n",
    "- Building tools that multiple applications will use\n",
    "- Need strong security boundaries between app and tools\n",
    "- Tools are complex, resource-intensive, or stateful\n",
    "- Team structure benefits from independent development\n",
    "- Future interoperability is important\n",
    "\n",
    "**✅ Choose Direct Integration When:**\n",
    "- Rapid prototyping and simple applications\n",
    "- Performance is critical and latency matters\n",
    "- Tools are simple, stateless functions\n",
    "- Single team developing both app and tools\n",
    "- Debugging simplicity is paramount\n",
    "\n",
    "**🎯 Hybrid Approach:**\n",
    "Many production systems use both - direct integration for core functionality and MCP for extensible tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Lab Complete!\n",
    "\n",
    "**Congratulations!** You've successfully transformed a direct tool integration into a standardized MCP protocol system.\n",
    "\n",
    "### **🏆 What You've Accomplished:**\n",
    "\n",
    "**🔧 Technical Skills:**\n",
    "- ✅ Built MCP client using Python SDK with proper async patterns\n",
    "- ✅ Integrated MCP protocol into agent orchestration\n",
    "- ✅ Handled process management, connections, and error scenarios\n",
    "- ✅ Experienced tool discovery and capability negotiation\n",
    "\n",
    "**🧠 Conceptual Understanding:**\n",
    "- ✅ MCP three-layer architecture (Host, Client, Server)\n",
    "- ✅ Protocol vs direct integration trade-offs\n",
    "- ✅ Benefits of standardization and modularity\n",
    "- ✅ Production deployment considerations\n",
    "\n",
    "**🚀 Real-World Readiness:**\n",
    "- ✅ Know when to choose MCP vs direct integration\n",
    "- ✅ Can integrate MCP into existing applications\n",
    "- ✅ Understand scaling and security patterns\n",
    "- ✅ Ready to build with protocol standards\n",
    "\n",
    "### **🌟 The Bigger Picture**\n",
    "\n",
    "You've experienced the shift from **custom integrations** to **standardized protocols** - the same evolution that transformed computing with standards like USB, HTTP, and TCP/IP.\n",
    "\n",
    "**MCP represents the future of AI tool integration** - where any tool can work with any application through a common protocol.\n",
    "\n",
    "**🎯 You're now equipped to build AI applications that leverage the growing ecosystem of MCP-compatible tools and services!**\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing the MCP Foundations Lab!** 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
