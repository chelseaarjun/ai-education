{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Building AI Agents - Foundations Lab\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "1. **Understand agent orchestration** - How agents coordinate tools, memory, and decision-making\n",
    "2. **Experience the transformation** from workflow to autonomous agent\n",
    "3. **Implement practical tools** - Content search and intelligent follow-up generation\n",
    "4. **Integrate episodic memory** - How agents remember and use conversation history\n",
    "5. **Recognize LLM-controlled decision-making** - The key to true agency\n",
    "\n",
    "## üèóÔ∏è What We're Building\n",
    "A **Personal Assistant ChatBot** that evolves from a simple workflow into an autonomous agent:\n",
    "- Search through course content to answer questions\n",
    "- Remember conversation history for better context\n",
    "- **Autonomously decide** when to generate thoughtful follow-up questions\n",
    "- Make intelligent decisions about tool usage\n",
    "\n",
    "## ‚è±Ô∏è Lab Timeline (100 minutes)\n",
    "- **Section 1**: Setup & Data Loading (10 min)\n",
    "- **Section 2**: Content Search Tool (15 min) \n",
    "- **Section 3**: Agent Orchestration (20 min)\n",
    "- **Section 4**: Memory Integration (15 min)\n",
    "- **Section 5**: LLM-Controlled Follow-up Questions (25 min)\n",
    "- **Section 6**: Workflow vs Agent Reflection (15 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Data Loading (10 minutes)\n",
    "\n",
    "First, let's set up our environment and understand how our course content was prepared for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# MyBinder users: set your credentials here (do NOT share real keys)\n",
    "#import os\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY'\n",
    "# os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'  # or your region\n",
    "\n",
    "# Install required packages (run this cell first)\n",
    "#!pip install -r ../../requirements.txt --quiet\n",
    "#!conda install -y conda-forge::faiss-cpu --quiet\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Libraries imported successfully!\n",
      "üïê Lab start time: 19:33:53\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(\"üïê Lab start time:\", datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration\n",
    "\n",
    "Set up AWS Bedrock connection and file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåé AWS Region: us-west-2\n",
      "üìÅ Embeddings file: ../embeddings/course_embeddings.json\n",
      "üß† LLM Model: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "‚úÖ Connected to AWS Bedrock successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "AWS_REGION = \"us-west-2\"  # Change if you prefer a different region\n",
    "EMBEDDINGS_FILE = \"../embeddings/course_embeddings.json\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v2:0\"\n",
    "LLM_MODEL = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "print(f\"üåé AWS Region: {AWS_REGION}\")\n",
    "print(f\"üìÅ Embeddings file: {EMBEDDINGS_FILE}\")\n",
    "print(f\"üß† LLM Model: {LLM_MODEL}\")\n",
    "\n",
    "# Initialize AWS Bedrock client\n",
    "try:\n",
    "    bedrock_client = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "    print(\"‚úÖ Connected to AWS Bedrock successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to AWS Bedrock: {e}\")\n",
    "    print(\"Please ensure your AWS credentials are configured correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Understanding Our Course Content Data\n",
    "\n",
    "Before we build our agent, let's understand how our course content was prepared. The embeddings were created using the script at `../course_embeddings_generator.py`.\n",
    "\n",
    "**How the embeddings were generated:**\n",
    "1. **HTML Extraction**: Converted HTML pages to clean text using `html2text`\n",
    "2. **Semantic Chunking**: Split content by `<section>` elements for logical units\n",
    "3. **Vectorization**: Created embeddings using AWS Bedrock Titan Embeddings\n",
    "4. **Storage**: Saved as structured JSON with metadata\n",
    "\n",
    "Let's load and explore this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embeddings from ../embeddings/course_embeddings.json\n",
      "\n",
      "üìà Content Statistics:\n",
      "   üìÑ Files processed: 6\n",
      "   üìù Content chunks: 43\n",
      "   üìä Total words: 18,800\n",
      "   üß† Embedding dimension: 1024\n",
      "   üïê Created: 2025-05-25T07:43:34\n",
      "\n",
      "üìö Sample content chunks:\n",
      "   1. Welcome to AI Foundations (38 words) - index.html\n",
      "   2. Understanding Generative AI (358 words) - index.html\n",
      "   3. The Evolution of Artificial Intelligence (277 words) - index.html\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-generated embeddings\n",
    "def load_course_embeddings(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load embeddings and metadata from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded embeddings from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Embeddings file not found: {file_path}\")\n",
    "        print(\"Please ensure you've run the embedding generation script first\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading embeddings: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load the data\n",
    "embeddings_data = load_course_embeddings(EMBEDDINGS_FILE)\n",
    "\n",
    "if embeddings_data:\n",
    "    metadata = embeddings_data['metadata']\n",
    "    chunks = embeddings_data['chunks']\n",
    "    \n",
    "    print(f\"\\nüìà Content Statistics:\")\n",
    "    print(f\"   üìÑ Files processed: {len(metadata['processed_files'])}\")\n",
    "    print(f\"   üìù Content chunks: {metadata['chunk_count']}\")\n",
    "    print(f\"   üìä Total words: {metadata['total_words']:,}\")\n",
    "    print(f\"   üß† Embedding dimension: {metadata['embedding_dimension']}\")\n",
    "    print(f\"   üïê Created: {metadata['created_at'][:19]}\")\n",
    "    \n",
    "    print(f\"\\nüìö Sample content chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"   {i+1}. {chunk['title']} ({chunk['word_count']} words) - {chunk['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Creating the Search Index\n",
    "\n",
    "Now let's create a FAISS (Facebook AI Similarity Search) index from our embeddings. This will enable fast semantic search over our course content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created FAISS index with 43 vectors\n",
      "üìê Vector dimension: 1024\n",
      "\n",
      "üéØ Search index ready! We can now find relevant content for any query.\n"
     ]
    }
   ],
   "source": [
    "def create_search_index(embeddings_data: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Create FAISS index for fast similarity search\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (faiss_index, chunks_list)\n",
    "    \"\"\"\n",
    "    if not embeddings_data:\n",
    "        return None, []\n",
    "    \n",
    "    chunks = embeddings_data['chunks']\n",
    "    \n",
    "    # Extract embeddings as numpy array\n",
    "    embeddings_matrix = np.array([chunk['embedding'] for chunk in chunks], dtype=np.float32)\n",
    "    \n",
    "    # Create FAISS index (using Inner Product for cosine similarity)\n",
    "    dimension = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product index\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings_matrix)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    index.add(embeddings_matrix)\n",
    "    \n",
    "    print(f\"‚úÖ Created FAISS index with {index.ntotal} vectors\")\n",
    "    print(f\"üìê Vector dimension: {dimension}\")\n",
    "    \n",
    "    return index, chunks\n",
    "\n",
    "# Create the search index\n",
    "search_index, content_chunks = create_search_index(embeddings_data)\n",
    "\n",
    "if search_index:\n",
    "    print(f\"\\nüéØ Search index ready! We can now find relevant content for any query.\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create search index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Quick Search Test\n",
    "\n",
    "Let's test our search index with a simple query to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Search results for: 'What are the key characteristics of large language models?'\n",
      "--------------------------------------------------\n",
      "\n",
      "1. **Module 1: Understanding Large Language Models** (Score: 0.685)\n",
      "   Source: llm.html\n",
      "   Preview: ## Module 1: Understanding Large Language Models\n",
      "\n",
      "Large Language Models (LLMs) are sophisticated AI systems, trained on vast amounts of text data, tha...\n",
      "\n",
      "2. **6. LLM Evolution & Architectural Advances** (Score: 0.561)\n",
      "   Source: llm.html\n",
      "   Preview: ## 6\\. LLM Evolution & Architectural Advances\n",
      "\n",
      "#### Early LLM Development (2017-2022)\n",
      "\n",
      "The modern Large Language Model era began with the 2017 paper \"...\n"
     ]
    }
   ],
   "source": [
    "def quick_search_test(query: str, top_k: int = 2):\n",
    "    \"\"\"Test the search functionality with a sample query\"\"\"\n",
    "    \n",
    "    if not search_index:\n",
    "        print(\"‚ùå Search index not available\")\n",
    "        return\n",
    "    \n",
    "    # Create query embedding\n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=EMBEDDING_MODEL,\n",
    "            body=json.dumps({\"inputText\": query})\n",
    "        )\n",
    "        query_embedding = json.loads(response['body'].read())['embedding']\n",
    "        \n",
    "        # Convert to numpy and normalize\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = search_index.search(query_vector, top_k)\n",
    "        \n",
    "        print(f\"üîç Search results for: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            chunk = content_chunks[idx]\n",
    "            print(f\"\\n{i+1}. **{chunk['title']}** (Score: {score:.3f})\")\n",
    "            print(f\"   Source: {chunk['source']}\")\n",
    "            print(f\"   Preview: {chunk['content'][:150]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search test failed: {e}\")\n",
    "\n",
    "# Test with a course-related query\n",
    "quick_search_test(\"What are the key characteristics of large language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Section 1 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ Course content loaded and indexed for search\n",
    "- ‚úÖ Understanding of how embeddings enable semantic search  \n",
    "- ‚úÖ A working search system ready for agent integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Content Search Tool Implementation (15 minutes)\n",
    "\n",
    "Now let's build our first tool - the content search capability. This demonstrates the **retrieval tool pattern** where we find and return existing information.\n",
    "\n",
    "## üõ†Ô∏è Tool Design Principles\n",
    "\n",
    "Good agent tools should:\n",
    "1. **Do one job well** - Clear, focused purpose\n",
    "2. **Have clean interfaces** - Easy for agents to understand and use\n",
    "3. **Provide useful outputs** - Structured, informative results\n",
    "4. **Handle errors gracefully** - Helpful error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Content search tool implemented!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Clean data structure for search results\"\"\"\n",
    "    content: str\n",
    "    title: str\n",
    "    source: str\n",
    "    relevance_score: float\n",
    "\n",
    "def search_content_tool(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Search course content using semantic similarity\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    if not search_index or not content_chunks:\n",
    "        return \"Error: Search index not available\"\n",
    "    \n",
    "    if not query.strip():\n",
    "        return \"Error: Search query cannot be empty\"\n",
    "    \n",
    "    try:\n",
    "        # Create query embedding\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=EMBEDDING_MODEL,\n",
    "            body=json.dumps({\"inputText\": query})\n",
    "        )\n",
    "        query_embedding = json.loads(response['body'].read())['embedding']\n",
    "        \n",
    "        # Convert to numpy and normalize for cosine similarity\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search for similar content\n",
    "        scores, indices = search_index.search(query_vector, max_results)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if score > 0.3:  # Only include reasonably relevant results\n",
    "                chunk = content_chunks[idx]\n",
    "                results.append(SearchResult(\n",
    "                    content=chunk['content'][:500] + \"...\" if len(chunk['content']) > 500 else chunk['content'],\n",
    "                    title=chunk['title'],\n",
    "                    source=chunk['source'],\n",
    "                    relevance_score=float(score)\n",
    "                ))\n",
    "        \n",
    "        if not results:\n",
    "            return f\"No relevant content found for query: '{query}'\"\n",
    "        \n",
    "        # Format output for the agent\n",
    "        output = f\"Found {len(results)} relevant content sections for '{query}':\\n\\n\"\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            output += f\"{i}. **{result.title}** (Relevance: {result.relevance_score:.3f})\\n\"\n",
    "            output += f\"   Source: {result.source}\\n\"\n",
    "            output += f\"   Content: {result.content}\\n\\n\"\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching content: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Content search tool implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Content Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Content Search Tool\n",
      "\n",
      "Query: What is prompt engineering?\n",
      "============================================================\n",
      "Found 2 relevant content sections for 'What is prompt engineering?':\n",
      "\n",
      "1. **1. Prompt Engineering Overview** (Relevance: 0.620)\n",
      "   Source: prompts.html\n",
      "   Content: ## 1\\. Prompt Engineering Overview\n",
      "\n",
      "Input (Prompt) AI Model (Processing) Output (Response)\n",
      "\n",
      "### 1.1 What are Prompts?\n",
      "\n",
      "A **prompt** is th...\n",
      "\n",
      "\n",
      "Query: How do LLMs work?\n",
      "============================================================\n",
      "Found 2 relevant content sections for 'How do LLMs work?':\n",
      "\n",
      "1. **Module 1: Understanding Large Language Models** (Relevance: 0.520)\n",
      "   Source: llm.html\n",
      "   Content: ## Module 1: Understanding Large Language Models\n",
      "\n",
      "Large Language Models (LLMs) are sophisticated AI systems, trained on vast amounts of ...\n",
      "\n",
      "\n",
      "Query: What makes agents different from simple LLM applications?\n",
      "============================================================\n",
      "Found 2 relevant content sections for 'What makes agents different from simple LLM applications?':\n",
      "\n",
      "1. **From LLMs to Agents: Why Go Further?** (Relevance: 0.696)\n",
      "   Source: agents.html\n",
      "   Content: ## From LLMs to Agents: Why Go Further?\n",
      "\n",
      "While LLMs are incredibly versatile, many real-world applicat...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the content search tool\n",
    "print(\"üîç Testing Content Search Tool\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is prompt engineering?\",\n",
    "    \"How do LLMs work?\",\n",
    "    \"What makes agents different from simple LLM applications?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    result = search_content_tool(query, max_results=2)\n",
    "    print(result[:300] + \"...\" if len(result) > 300 else result)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Section 2 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ A working content search tool with clean interface design\n",
    "- ‚úÖ Understanding of retrieval tool patterns\n",
    "- ‚úÖ Error handling and structured output formatting\n",
    "- ‚úÖ Foundation ready for agent integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Agent Orchestration (20 minutes)\n",
    "\n",
    "Now let's build our first **agent** that can intelligently use the search tool! \n",
    "\n",
    "## üß† Agent Fundamentals\n",
    "\n",
    "Our agent will implement the core decision cycle:\n",
    "1. **Observe**: Analyze user input and current context\n",
    "2. **Plan**: Decide what actions to take\n",
    "3. **Act**: Execute the plan using available tools\n",
    "\n",
    "**Key insight**: For now, the orchestration logic will be **developer-defined**. We'll see how this evolves later in the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Course Assistant Agent initialized!\n",
      "   Available tools: ['search_content']\n",
      "\n",
      "‚úÖ Agent ready for testing!\n"
     ]
    }
   ],
   "source": [
    "class CourseAssistantAgent:\n",
    "    \"\"\"\n",
    "    A personal assistant agent for course content\n",
    "    \n",
    "    This agent demonstrates the core pattern of AI applications:\n",
    "    - Observe: Analyze user input\n",
    "    - Plan: Decide which tools to use\n",
    "    - Act: Execute the plan and generate a response\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Tool registry - the agent's available capabilities\n",
    "        self.tools = {\n",
    "            \"search_content\": search_content_tool\n",
    "        }\n",
    "        \n",
    "        # Initialize without memory for now (we'll add this in Section 4)\n",
    "        self.memory = None\n",
    "        \n",
    "        print(\"ü§ñ Course Assistant Agent initialized!\")\n",
    "        print(f\"   Available tools: {list(self.tools.keys())}\")\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Helper method to call Claude via Bedrock\"\"\"\n",
    "        try:\n",
    "            # Create the request body\n",
    "            request_body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "\n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=LLM_MODEL,\n",
    "                body=json.dumps(request_body),\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            return response_body['content'][0]['text'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error calling LLM: {str(e)}\"\n",
    "    \n",
    "    def decide_and_act(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        The main agent decision cycle: Observe ‚Üí Plan ‚Üí Act\n",
    "        \n",
    "        Args:\n",
    "            user_input: The user's question or request\n",
    "            \n",
    "        Returns:\n",
    "            Agent's response\n",
    "        \"\"\"\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return \"I'd be happy to help! Please ask me a question about the course content.\"\n",
    "        \n",
    "        # OBSERVE: Analyze the user's input\n",
    "        print(f\"üîç Agent observing: '{user_input}'\")\n",
    "        \n",
    "        # PLAN: For now, simple plan - always search first, then respond\n",
    "        print(\"üìã Agent planning: Will search content and provide comprehensive response\")\n",
    "        \n",
    "        # ACT: Execute the plan\n",
    "        \n",
    "        # Step 1: Search for relevant content\n",
    "        print(\"‚ö° Agent acting: Searching course content...\")\n",
    "        search_results = self.tools[\"search_content\"](user_input, max_results=3)\n",
    "        \n",
    "        # Step 2: Generate response using search results\n",
    "        print(\"‚ö° Agent acting: Generating response with found content...\")\n",
    "        response_prompt = f\"\"\"You are a helpful course assistant for an AI/ML education program.\n",
    "\n",
    "A student asked: \"{user_input}\"\n",
    "\n",
    "Here's relevant content from the course materials:\n",
    "{search_results}\n",
    "\n",
    "Based on this content, provide a clear, helpful answer to the student's question. \n",
    "Focus on the key concepts and make it educational. If the search results don't \n",
    "contain relevant information, say so politely and offer to help with other topics.\n",
    "\n",
    "Keep your response concise but complete (2-3 paragraphs maximum).\"\"\"\n",
    "        \n",
    "        main_response = self._call_llm(response_prompt, max_tokens=400)\n",
    "        \n",
    "        return main_response\n",
    "\n",
    "# Create our agent\n",
    "agent = CourseAssistantAgent()\n",
    "print(\"\\n‚úÖ Agent ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Basic Agent\n",
      "\n",
      "User: What are the main differences between LLMs and AI agents?\n",
      "\n",
      "============================================================\n",
      "\n",
      "üîç Agent observing: 'What are the main differences between LLMs and AI agents?'\n",
      "üìã Agent planning: Will search content and provide comprehensive response\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with found content...\n",
      "Agent: Based on the course materials, I can explain the key differences between LLMs and AI agents:\n",
      "\n",
      "The main distinction is that LLMs are primarily language processing systems that can understand and generate text, reason about information, and answer questions - but they operate within specific boundaries. They lack persistent memory, can't access external tools/data, and don't take autonomous actions. Think of an LLM as a sophisticated language processor that responds to prompts but doesn't actively do anything beyond text generation.\n",
      "\n",
      "AI agents, on the other hand, are more dynamic systems that build upon LLMs by adding action-oriented capabilities. An agent operates in a continuous loop where it observes its environment, uses the LLM's reasoning abilities to make decisions, and then takes concrete actions to achieve specific goals. Agents can interact with external tools, maintain memory of past interactions, and autonomously work toward objectives. You can think of an agent as a software system that wraps around an LLM to give it the ability to actually do things in the world, not just process language.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "User: Can you explain what prompt engineering is?\n",
      "\n",
      "============================================================\n",
      "\n",
      "üîç Agent observing: 'Can you explain what prompt engineering is?'\n",
      "üìã Agent planning: Will search content and provide comprehensive response\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with found content...\n",
      "Agent: Based on the course materials, I can provide a clear explanation of prompt engineering:\n",
      "\n",
      "Prompt engineering is the art and science of effectively communicating with AI language models to get desired results. At its core, it involves crafting prompts - the input text that tells an AI system what you want it to do. A prompt is essentially a sequence of tokens (words, characters, or subwords) that provides context and instructions to guide the AI model's response.\n",
      "\n",
      "Think of prompt engineering as creating an effective interface between human intent and AI capability. It's become a crucial skill in the AI era, allowing people to build sophisticated AI applications even without extensive programming knowledge. The goal is to learn how to structure and phrase your inputs in ways that help the AI model better understand and execute your intended task.\n",
      "\n",
      "Would you like to learn more about specific prompt engineering techniques or see some practical examples? I'd be happy to explore those aspects with you.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "User: How do embeddings work in AI applications?\n",
      "\n",
      "============================================================\n",
      "\n",
      "üîç Agent observing: 'How do embeddings work in AI applications?'\n",
      "üìã Agent planning: Will search content and provide comprehensive response\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with found content...\n",
      "Agent: Let me explain how embeddings work in AI applications:\n",
      "\n",
      "Embeddings are numerical representations that capture the meaning of tokens (like words or phrases) in a mathematical space. Think of them as converting meaning into numbers in a way that similar concepts end up \"closer\" to each other in that space. For example, in an embedding space, words like \"happy\" and \"joyful\" would be positioned near each other because they have similar meanings, while \"happy\" and \"sad\" would be far apart because they have opposite meanings.\n",
      "\n",
      "This mathematical representation is incredibly useful in AI applications because it allows computers to understand and process semantic relationships. When AI systems need to understand text, compare meanings, or make decisions based on similarity, they can use these embedding values to perform calculations that reflect real-world meaning relationships.\n",
      "\n",
      "Note: The course materials don't provide detailed technical implementation details about embeddings. If you'd like to learn more about specific aspects like how embeddings are generated, their dimensionality, or specific applications, please let me know and I can help with those topics.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Testing Basic Agent\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the main differences between LLMs and AI agents?\",\n",
    "    \"Can you explain what prompt engineering is?\",\n",
    "    \"How do embeddings work in AI applications?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"User: {question}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    response = agent.decide_and_act(question)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Current Agent Behavior\n",
    "\n",
    "**Notice the pattern:** Our agent currently follows a **predictable workflow**:\n",
    "1. Always searches for content\n",
    "2. Always generates a response based on search results\n",
    "3. Never varies from this pattern\n",
    "\n",
    "**This is \"intelligent workflow automation\" rather than autonomous decision-making.**\n",
    "\n",
    "The orchestration logic is **hardcoded by us** (the developers), not dynamically determined by the LLM based on context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Section 3 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ A working agent that orchestrates tool usage\n",
    "- ‚úÖ Understanding of the agent decision cycle (Observe ‚Üí Plan ‚Üí Act)\n",
    "- ‚úÖ Experience with developer-controlled orchestration patterns\n",
    "- ‚úÖ Foundation ready for memory integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Memory Integration (15 minutes)\n",
    "\n",
    "Now let's give our agent **memory**! This transforms it from a stateless system into one that can maintain context across multiple interactions.\n",
    "\n",
    "## üß† Why Memory Matters\n",
    "\n",
    "Without memory, each interaction is isolated:\n",
    "- ‚ùå \"What did we just discuss?\"\n",
    "- ‚ùå \"Can you elaborate on that previous point?\"\n",
    "- ‚ùå Building on previous conversations\n",
    "\n",
    "With memory, agents become context-aware:\n",
    "- ‚úÖ Remembers conversation history\n",
    "- ‚úÖ Builds on previous discussions\n",
    "- ‚úÖ Provides continuity across interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Implementing Episodic Memory\n",
    "\n",
    "We'll implement simple **episodic memory** - storing and retrieving conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Episodic memory class implemented!\n"
     ]
    }
   ],
   "source": [
    "class EpisodicMemory:\n",
    "    \"\"\"\n",
    "    Simple episodic memory for storing conversation history\n",
    "    \n",
    "    In production, this might use a database, but for learning\n",
    "    purposes, we'll use in-memory storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversations = []  # List of conversation turns\n",
    "        print(\"üß† Episodic memory initialized\")\n",
    "    \n",
    "    def store_interaction(self, user_input: str, agent_response: str):\n",
    "        \"\"\"\n",
    "        Store a conversation turn\n",
    "        \n",
    "        Args:\n",
    "            user_input: What the user said\n",
    "            agent_response: How the agent responded\n",
    "        \"\"\"\n",
    "        interaction = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"user\": user_input,\n",
    "            \"agent\": agent_response,\n",
    "            \"turn_number\": len(self.conversations) + 1\n",
    "        }\n",
    "        \n",
    "        self.conversations.append(interaction)\n",
    "        print(f\"üíæ Stored interaction #{interaction['turn_number']}\")\n",
    "    \n",
    "    def get_recent_context(self, max_turns: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Get recent conversation history for context\n",
    "        \n",
    "        Args:\n",
    "            max_turns: Maximum number of recent turns to include\n",
    "            \n",
    "        Returns:\n",
    "            Formatted conversation context\n",
    "        \"\"\"\n",
    "        if not self.conversations:\n",
    "            return \"No previous conversation history.\"\n",
    "        \n",
    "        # Get the most recent turns\n",
    "        recent = self.conversations[-max_turns:]\n",
    "        \n",
    "        context = \"Recent conversation history:\\n\"\n",
    "        for turn in recent:\n",
    "            context += f\"Turn {turn['turn_number']} - User: {turn['user']}\\n\"\n",
    "            context += f\"Turn {turn['turn_number']} - Agent: {turn['agent']}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_conversation_summary(self) -> str:\n",
    "        \"\"\"Get a summary of the entire conversation\"\"\"\n",
    "        if not self.conversations:\n",
    "            return \"No conversations yet.\"\n",
    "        \n",
    "        total_turns = len(self.conversations)\n",
    "        topics = []\n",
    "        \n",
    "        # Extract key topics mentioned (simple keyword extraction)\n",
    "        ai_terms = ['llm', 'prompt', 'agent', 'memory', 'tool', 'embedding', 'model']\n",
    "        \n",
    "        for turn in self.conversations:\n",
    "            user_text = turn['user'].lower()\n",
    "            for term in ai_terms:\n",
    "                if term in user_text and term not in topics:\n",
    "                    topics.append(term)\n",
    "        \n",
    "        return f\"Conversation summary: {total_turns} turns, topics discussed: {', '.join(topics) if topics else 'general questions'}\"\n",
    "\n",
    "print(\"‚úÖ Episodic memory class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Enhanced Agent with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Course Assistant Agent initialized!\n",
      "   Available tools: ['search_content']\n",
      "üß† Episodic memory initialized\n",
      "üß† Memory-enabled agent initialized!\n",
      "\n",
      "‚úÖ Memory-enabled agent ready!\n"
     ]
    }
   ],
   "source": [
    "class MemoryEnabledAgent(CourseAssistantAgent):\n",
    "    \"\"\"\n",
    "    Enhanced agent with episodic memory capabilities\n",
    "    \n",
    "    This agent demonstrates how memory integration transforms\n",
    "    agent behavior from stateless to stateful.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.memory = EpisodicMemory()\n",
    "        print(\"üß† Memory-enabled agent initialized!\")\n",
    "    \n",
    "    def decide_and_act(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced decision cycle with memory integration\n",
    "        \n",
    "        Key difference from base agent: includes conversation context\n",
    "        in reasoning and stores interactions for future use.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return \"I'd be happy to help! Please ask me a question about the course content.\"\n",
    "        \n",
    "        # OBSERVE: Analyze input AND retrieve relevant memory\n",
    "        print(f\"üîç Agent observing: '{user_input}'\")\n",
    "        conversation_context = self.memory.get_recent_context(max_turns=3)\n",
    "        print(f\"üß† Agent remembering: {len(self.memory.conversations)} previous turns\")\n",
    "        \n",
    "        # PLAN: Same simple plan for now, but with memory context\n",
    "        print(\"üìã Agent planning: Will search content and provide response with conversation context\")\n",
    "        \n",
    "        # ACT: Execute plan with memory integration\n",
    "        \n",
    "        # Step 1: Search for relevant content\n",
    "        print(\"‚ö° Agent acting: Searching course content...\")\n",
    "        search_results = self.tools[\"search_content\"](user_input, max_results=3)\n",
    "        \n",
    "        # Step 2: Generate response with memory context\n",
    "        print(\"‚ö° Agent acting: Generating response with content and conversation context...\")\n",
    "        \n",
    "        response_prompt = f\"\"\"You are a helpful course assistant for an AI/ML education program.\n",
    "\n",
    "Student's current question: \"{user_input}\"\n",
    "\n",
    "Conversation context:\n",
    "{conversation_context}\n",
    "\n",
    "Relevant course content:\n",
    "{search_results}\n",
    "\n",
    "Provide a helpful answer that:\n",
    "1. Addresses the current question using the course content\n",
    "2. References previous conversation when relevant\n",
    "3. Builds on topics we've already discussed\n",
    "4. Maintains conversational continuity\n",
    "\n",
    "If the question references previous discussion, acknowledge that connection.\n",
    "Keep your response clear and educational (2-3 paragraphs maximum).\"\"\"\n",
    "        \n",
    "        main_response = self._call_llm(response_prompt, max_tokens=400)\n",
    "        \n",
    "        # Step 3: Store interaction in memory\n",
    "        self.memory.store_interaction(user_input, main_response)\n",
    "        \n",
    "        return main_response\n",
    "\n",
    "# Create the memory-enabled agent\n",
    "memory_agent = MemoryEnabledAgent()\n",
    "print(\"\\n‚úÖ Memory-enabled agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Memory Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Memory-Enabled Agent\n",
      "\n",
      "Turn 1 - User: What is prompt engineering?\n",
      "----------------------------------------\n",
      "üîç Agent observing: 'What is prompt engineering?'\n",
      "üß† Agent remembering: 0 previous turns\n",
      "üìã Agent planning: Will search content and provide response with conversation context\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with content and conversation context...\n",
      "üíæ Stored interaction #1\n",
      "Agent: Let me explain prompt engineering in a clear and accessible way.\n",
      "\n",
      "Prompt engineering is the art and science of effectively communicating with AI systems through carefully crafted inputs called prompts...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Turn 2 - User: Can you elaborate on that?\n",
      "----------------------------------------\n",
      "üîç Agent observing: 'Can you elaborate on that?'\n",
      "üß† Agent remembering: 1 previous turns\n",
      "üìã Agent planning: Will search content and provide response with conversation context\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with content and conversation context...\n",
      "üíæ Stored interaction #2\n",
      "Agent: Error calling LLM: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Turn 3 - User: What techniques did we just discuss?\n",
      "----------------------------------------\n",
      "üîç Agent observing: 'What techniques did we just discuss?'\n",
      "üß† Agent remembering: 2 previous turns\n",
      "üìã Agent planning: Will search content and provide response with conversation context\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with content and conversation context...\n",
      "üíæ Stored interaction #3\n",
      "Agent: I notice there appears to have been some technical issues in our previous exchange where we started discussing prompt engineering but encountered an error. From what I can see, we began talking about ...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Turn 4 - User: How does this relate to what we talked about earlier?\n",
      "----------------------------------------\n",
      "üîç Agent observing: 'How does this relate to what we talked about earlier?'\n",
      "üß† Agent remembering: 3 previous turns\n",
      "üìã Agent planning: Will search content and provide response with conversation context\n",
      "‚ö° Agent acting: Searching course content...\n",
      "‚ö° Agent acting: Generating response with content and conversation context...\n",
      "üíæ Stored interaction #4\n",
      "Agent: Error calling LLM: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Final Memory Summary: Conversation summary: 4 turns, topics discussed: prompt\n"
     ]
    }
   ],
   "source": [
    "print(\"üß™ Testing Memory-Enabled Agent\\n\")\n",
    "\n",
    "# Simulate a conversation sequence\n",
    "conversation = [\n",
    "    \"What is prompt engineering?\",\n",
    "    \"Can you elaborate on that?\",\n",
    "    \"What techniques did we just discuss?\",\n",
    "    \"How does this relate to what we talked about earlier?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(conversation, 1):\n",
    "    print(f\"Turn {i} - User: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    response = memory_agent.decide_and_act(question)\n",
    "    print(f\"Agent: {response[:200]}...\" if len(response) > 200 else f\"Agent: {response}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"üìä Final Memory Summary: {memory_agent.memory.get_conversation_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Section 4 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ An agent with episodic memory\n",
    "- ‚úÖ Understanding of memory integration patterns\n",
    "- ‚úÖ Experience with stateful conversation management\n",
    "- ‚úÖ Foundation ready for the next transformation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: LLM-Controlled Follow-up Questions (25 minutes)\n",
    "\n",
    "Now comes the exciting transformation! We'll add a new capability - **follow-up question generation** - but this time, we'll let the **LLM decide** when to use it.\n",
    "\n",
    "## üéØ The Key Insight\n",
    "\n",
    "**So far:** Our agent has followed **developer-defined workflows** (search ‚Üí respond)  \n",
    "**Now:** The agent will make **autonomous decisions** about when to generate follow-up questions\n",
    "\n",
    "This is the transformation from **\"Intelligent Workflow\"** to **\"Autonomous Agent\"**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Tool 2: Follow-up Question Generator\n",
    "\n",
    "This tool generates relevant follow-up questions to enhance learning. It's a **generative tool** that demonstrates the **\"LLM-as-tool\"** pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_followup_questions_tool(current_topic: str, conversation_context: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate relevant follow-up questions to enhance learning\n",
    "    \n",
    "    Args:\n",
    "        current_topic: The topic being discussed\n",
    "        conversation_context: Recent conversation for context\n",
    "        \n",
    "    Returns:\n",
    "        Formatted follow-up questions\n",
    "    \"\"\"\n",
    "    \n",
    "    if not current_topic.strip():\n",
    "        return \"Error: Topic cannot be empty for question generation\"\n",
    "    \n",
    "    try:\n",
    "        # Create prompt for follow-up question generation\n",
    "        prompt = f\"\"\"You are an educational assistant helping students explore AI and machine learning concepts more deeply.\n",
    "\n",
    "Based on the current topic '{current_topic}' and this conversation context:\n",
    "{conversation_context}\n",
    "\n",
    "Generate 3 thoughtful follow-up questions that would help a student:\n",
    "1. Deepen their understanding of this topic\n",
    "2. Connect it to other course concepts (LLMs, prompt engineering, agents)\n",
    "3. Apply it practically or think about real-world implications\n",
    "\n",
    "Make the questions specific, engaging, and educational. Format as:\n",
    "ü§î Question 1: ...\n",
    "ü§î Question 2: ...\n",
    "ü§î Question 3: ...\n",
    "\n",
    "Only return the questions, nothing else.\"\"\"\n",
    "\n",
    "        # Create the request body\n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 300,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "\n",
    "        # Call Claude for question generation\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=LLM_MODEL,\n",
    "            body=json.dumps(request_body),\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        questions = response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        return questions\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating follow-up questions: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Follow-up question generator tool implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Follow-up Question Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the follow-up question generator\n",
    "print(\"ü§î Testing Follow-up Question Generator\\n\")\n",
    "\n",
    "test_topic = \"agent memory systems\"\n",
    "test_context = \"We just discussed how agents use working memory and episodic memory\"\n",
    "\n",
    "followup_questions = generate_followup_questions_tool(test_topic, test_context)\n",
    "\n",
    "print(f\"Topic: {test_topic}\")\n",
    "print(f\"Context: {test_context}\")\n",
    "print(\"=\" * 50)\n",
    "print(followup_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† LLM-Controlled Decision Making\n",
    "\n",
    "Here's where the magic happens! Instead of hardcoded `if/else` logic, we'll let Claude decide when follow-up questions would be valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutonomousAgent(MemoryEnabledAgent):\n",
    "    \"\"\"\n",
    "    Truly autonomous agent with LLM-controlled decision making\n",
    "    \n",
    "    Key difference: The LLM decides when and how to use tools,\n",
    "    rather than following predetermined patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add the follow-up questions tool\n",
    "        self.tools[\"generate_followup_questions\"] = generate_followup_questions_tool\n",
    "        print(\"üöÄ Autonomous agent initialized!\")\n",
    "        print(f\"   Available tools: {list(self.tools.keys())}\")\n",
    "    \n",
    "    def _should_offer_followup_questions(self, user_input: str, agent_response: str, conversation_context: str) -> bool:\n",
    "        \"\"\"\n",
    "        LLM decides whether to offer follow-up questions\n",
    "        \n",
    "        This is the key transformation - decision making by LLM, not hardcoded logic!\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            decision_prompt = f\"\"\"You are helping decide when to offer educational follow-up questions.\n",
    "            \n",
    "User asked: \"{user_input}\"\n",
    "Agent responded: \"{agent_response[:200]}...\"\n",
    "Conversation context: {conversation_context}\n",
    "\n",
    "Should I offer follow-up questions? Consider:\n",
    "- Was the question clearly answered with good content?\n",
    "- Is the user exploring/learning vs seeking quick facts?\n",
    "- Would deeper questions enhance understanding?\n",
    "- Is this a natural learning moment?\n",
    "- Are we early enough in the conversation that questions would be welcome?\n",
    "\n",
    "Respond with just: YES or NO\"\"\"\n",
    "            \n",
    "            decision = self._call_llm(decision_prompt, max_tokens=10, temperature=0.1).strip().upper()\n",
    "            print(f\"ü§ñ LLM Decision: {decision}\")\n",
    "            return decision == \"YES\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Decision LLM failed: {e}, defaulting to NO\")\n",
    "            return False  # Graceful degradation\n",
    "    \n",
    "    def decide_and_act(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Autonomous decision cycle with LLM-controlled orchestration\n",
    "        \n",
    "        The LLM now participates in deciding the workflow!\n",
    "        \"\"\"\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return \"I'd be happy to help! Please ask me a question about the course content.\"\n",
    "        \n",
    "        # OBSERVE: Analyze input and retrieve memory\n",
    "        print(f\"üîç Agent observing: '{user_input}'\")\n",
    "        conversation_context = self.memory.get_recent_context(max_turns=3)\n",
    "        print(f\"üß† Agent remembering: {len(self.memory.conversations)} previous turns\")\n",
    "        \n",
    "        # Check for explicit follow-up requests first\n",
    "        followup_triggers = [\"follow up\", \"what else\", \"what next\", \"more questions\", \"dig deeper\"]\n",
    "        explicit_followup = any(trigger in user_input.lower() for trigger in followup_triggers)\n",
    "        \n",
    "        if explicit_followup:\n",
    "            print(\"üìã Agent planning: User explicitly requested follow-up questions\")\n",
    "            if self.memory.conversations:\n",
    "                last_topic = self.memory.conversations[-1]['user']\n",
    "                result = self.tools[\"generate_followup_questions\"](last_topic, conversation_context)\n",
    "                self.memory.store_interaction(user_input, result)\n",
    "                return f\"Here are some follow-up questions based on our conversation:\\n\\n{result}\"\n",
    "            else:\n",
    "                return \"I'd be happy to generate follow-up questions! What topic would you like to explore further?\"\n",
    "        \n",
    "        # PLAN: Standard workflow - search and respond\n",
    "        print(\"üìã Agent planning: Will search content, respond, then LLM will decide about follow-up questions\")\n",
    "        \n",
    "        # ACT: Execute standard workflow\n",
    "        \n",
    "        # Step 1: Search for relevant content\n",
    "        print(\"‚ö° Agent acting: Searching course content...\")\n",
    "        search_results = self.tools[\"search_content\"](user_input, max_results=3)\n",
    "        \n",
    "        # Step 2: Generate main response\n",
    "        print(\"‚ö° Agent acting: Generating main response...\")\n",
    "        response_prompt = f\"\"\"You are a helpful course assistant for an AI/ML education program.\n",
    "\n",
    "Student's current question: \"{user_input}\"\n",
    "\n",
    "Conversation context:\n",
    "{conversation_context}\n",
    "\n",
    "Relevant course content:\n",
    "{search_results}\n",
    "\n",
    "Provide a helpful answer that:\n",
    "1. Addresses the current question using the course content\n",
    "2. References previous conversation when relevant\n",
    "3. Builds on topics we've already discussed\n",
    "4. Maintains conversational continuity\n",
    "\n",
    "Keep your response clear and educational (2-3 paragraphs maximum).\"\"\"\n",
    "        \n",
    "        main_response = self._call_llm(response_prompt, max_tokens=400)\n",
    "        \n",
    "        # Step 3: LLM DECIDES whether to offer follow-up questions\n",
    "        print(\"ü§ñ Agent consulting LLM: Should I offer follow-up questions?\")\n",
    "        should_offer_followup = self._should_offer_followup_questions(user_input, main_response, conversation_context)\n",
    "        \n",
    "        final_response = main_response\n",
    "        \n",
    "        if should_offer_followup:\n",
    "            print(\"‚ö° Agent acting: Generating follow-up questions based on LLM decision...\")\n",
    "            followup_questions = self.tools[\"generate_followup_questions\"](user_input, conversation_context)\n",
    "            final_response = f\"{main_response}\\n\\n---\\nüí° **Want to explore further?**\\n{followup_questions}\"\n",
    "        \n",
    "        # Step 4: Store interaction in memory\n",
    "        self.memory.store_interaction(user_input, final_response)\n",
    "        \n",
    "        return final_response\n",
    "\n",
    "# Create the autonomous agent\n",
    "autonomous_agent = AutonomousAgent()\n",
    "print(\"\\n‚úÖ Autonomous agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Autonomous Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Testing Autonomous Agent\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the difference between agents and simple LLM applications?\",\n",
    "    \"How do vector embeddings enable semantic search?\",\n",
    "    \"What is 2+2?\",  # Simple factual question\n",
    "    \"Can you explain how agent memory works in more detail?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"Test {i} - User: {question}\")\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    response = autonomous_agent.decide_and_act(question)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"üìä Conversation Summary: {autonomous_agent.memory.get_conversation_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç What Just Happened?\n",
    "\n",
    "**Key Transformation:** The agent now uses **LLM reasoning** to decide when to offer follow-up questions!\n",
    "\n",
    "**Before:** Hardcoded logic (`if user_wants_followup:...`)\n",
    "**Now:** LLM evaluates context and makes autonomous decisions\n",
    "\n",
    "**Notice:**\n",
    "- The LLM considers conversation context, question complexity, and learning value\n",
    "- Decisions aren't predetermined - they emerge from reasoning\n",
    "- The agent adapts its behavior based on the situation\n",
    "\n",
    "**This is the essence of autonomous agency!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Demo: Experience True Agency\n",
    "\n",
    "Try conversing with the autonomous agent to see LLM-controlled decision making in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autonomous_agent_demo():\n",
    "    \"\"\"Interactive demo with the autonomous agent\"\"\"\n",
    "    print(\"üöÄ Autonomous Agent Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Ask questions and watch the agent make autonomous decisions:\")\n",
    "    print(\"  ‚Ä¢ Complex learning questions (likely to trigger follow-ups)\")\n",
    "    print(\"  ‚Ä¢ Simple factual queries (likely won't trigger follow-ups)\")\n",
    "    print(\"  ‚Ä¢ Explicitly request: 'what else should I know?'\")\n",
    "    print(\"\\nType 'memory' to see conversation history\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'stop']:\n",
    "            print(\"\\nüìä Final conversation summary:\")\n",
    "            print(autonomous_agent.memory.get_conversation_summary())\n",
    "            print(\"üëã Thanks for testing the autonomous agent!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'memory':\n",
    "            print(\"\\nüß† Current Memory State:\")\n",
    "            print(autonomous_agent.memory.get_recent_context())\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAgent: {autonomous_agent.decide_and_act(user_input)}\")\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Uncomment to run the interactive demo\n",
    "# autonomous_agent_demo()\n",
    "\n",
    "print(\"üí° Uncomment the line above to try the autonomous agent!\")\n",
    "print(\"   Watch for the LLM decision messages to see autonomous reasoning in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Section 5 Complete!**\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ Experience with LLM-controlled decision making\n",
    "- ‚úÖ A true autonomous agent that reasons about tool usage\n",
    "- ‚úÖ Understanding of the workflow ‚Üí agent transformation\n",
    "- ‚úÖ Hands-on experience with emergent vs predetermined behavior\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Workflow vs Agent Reflection (15 minutes)\n",
    "\n",
    "Let's reflect on the transformation you just experienced - from hardcoded workflow to autonomous agent.\n",
    "\n",
    "## üîç What We Built: The Evolution\n",
    "\n",
    "You've built **three different systems** in this lab, each with increasing levels of agency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä The Agency Spectrum: Your Journey\n",
    "\n",
    "| Stage | System | Decision Making | Tool Usage | Behavior |\n",
    "|-------|--------|----------------|------------|----------|\n",
    "| **1** | Basic Agent (Section 3) | Developer-defined workflow | Always search ‚Üí respond | Predictable, reliable |\n",
    "| **2** | Memory Agent (Section 4) | Developer-defined + context | Search ‚Üí respond + memory | Context-aware, predictable |\n",
    "| **3** | Autonomous Agent (Section 5) | **LLM-controlled decisions** | **Dynamic tool selection** | **Adaptive, emergent** |\n",
    "\n",
    "## üéØ The Key Transformation\n",
    "\n",
    "**The critical difference isn't the tools or memory - it's WHO makes the decisions:**\n",
    "\n",
    "### **Intelligent Workflow (Stages 1-2):**\n",
    "```python\n",
    "# Developer writes the decision logic\n",
    "if explicit_followup_request:\n",
    "    generate_followup_questions()\n",
    "else:\n",
    "    search_and_respond()\n",
    "```\n",
    "\n",
    "### **Autonomous Agent (Stage 3):**\n",
    "```python\n",
    "# LLM makes the decision based on context\n",
    "should_offer = llm_decides_based_on_context(user_input, response, history)\n",
    "if should_offer:\n",
    "    generate_followup_questions()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Insights from Your Journey\n",
    "\n",
    "### **1. Agency is a Spectrum**\n",
    "There's no binary \"agent vs not-agent\" - it's about the degree of autonomous decision-making.\n",
    "\n",
    "### **2. Both Approaches Have Value**\n",
    "- **Intelligent Workflows**: Predictable, debuggable, reliable - perfect for many production use cases\n",
    "- **Autonomous Agents**: Adaptive, context-aware, capable of handling novel situations\n",
    "\n",
    "### **3. The Foundation Matters**\n",
    "Tools, memory, and orchestration patterns are essential for BOTH workflows and agents. You learned the building blocks that power all modern AI applications.\n",
    "\n",
    "### **4. LLM Orchestration is Powerful**\n",
    "When you let the LLM participate in decision-making, you get emergent behaviors that you didn't explicitly program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Looking Forward: Production Considerations\n",
    "\n",
    "**In real-world applications, you might choose:**\n",
    "\n",
    "### **Intelligent Workflows When:**\n",
    "- Predictability is crucial (financial transactions, healthcare)\n",
    "- Debugging and auditing are essential\n",
    "- Performance and cost optimization are priorities\n",
    "- The workflow is well-defined and stable\n",
    "\n",
    "### **Autonomous Agents When:**\n",
    "- Handling diverse, unpredictable user needs\n",
    "- Personalization and adaptation are important\n",
    "- The problem space is complex and evolving\n",
    "- You want emergent capabilities\n",
    "\n",
    "### **Hybrid Approaches (Common in Production):**\n",
    "- Critical decisions: Developer-controlled\n",
    "- Creative decisions: LLM-controlled\n",
    "- Safety nets: Always developer-controlled fallbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ What You've Accomplished\n",
    "\n",
    "**Foundational Skills:**\n",
    "- ‚úÖ Tool design and integration patterns\n",
    "- ‚úÖ Memory systems and context management\n",
    "- ‚úÖ Agent orchestration and decision cycles\n",
    "- ‚úÖ Error handling and graceful degradation\n",
    "\n",
    "**Advanced Concepts:**\n",
    "- ‚úÖ LLM-controlled decision making\n",
    "- ‚úÖ Emergent vs predetermined behavior\n",
    "- ‚úÖ The spectrum of agency in AI systems\n",
    "- ‚úÖ Production considerations for different approaches\n",
    "\n",
    "**Real-World Readiness:**\n",
    "- ‚úÖ Understanding when to use workflows vs agents\n",
    "- ‚úÖ Ability to build both patterns effectively\n",
    "- ‚úÖ Foundation for advanced agent architectures\n",
    "- ‚úÖ Critical thinking about agency and autonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü The Bigger Picture\n",
    "\n",
    "You've experienced firsthand the evolution from **programmed behavior** to **emergent intelligence**. This mirrors the broader transformation happening in AI:\n",
    "\n",
    "- **Traditional Software**: Explicit programming for every scenario\n",
    "- **Intelligent Workflows**: LLMs handle language understanding, developers handle logic\n",
    "- **Autonomous Agents**: LLMs participate in reasoning about what to do next\n",
    "\n",
    "**You're now equipped to build AI applications across this entire spectrum!**\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing the AI Agents Foundations Lab!**\n",
    "\n",
    "You've not just learned to build agents - you've experienced the transformation from workflow automation to autonomous intelligence. This understanding will serve you well as AI systems continue to evolve toward greater autonomy and capability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
