{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Building AI Agents - Foundations Lab\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "1. **Understand agent orchestration** - How agents coordinate tools, memory, and decision-making\n",
    "2. **Implement practical tools** - Content search and follow-up question generation\n",
    "3. **Integrate episodic memory** - How agents remember and use conversation history\n",
    "4. **Experience the difference** between single-step LLM applications and agentic applications\n",
    "\n",
    "## ğŸ—ï¸ What We're Building\n",
    "A **Personal Assistant ChatBot Agent** for our course website that can:\n",
    "- Search through course content to answer questions\n",
    "- Generate thoughtful follow-up questions to enhance learning\n",
    "- Remember conversation history for better context\n",
    "- Make intelligent decisions about when to use which capabilities\n",
    "\n",
    "## â±ï¸ Lab Timeline (60 minutes)\n",
    "- **Section 1**: Setup & Data Loading (10 min)\n",
    "- **Section 2**: Tool Implementation (20 min) \n",
    "- **Section 3**: Agent Orchestration (15 min)\n",
    "- **Section 4**: Memory Integration (15 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Data Loading (10 minutes)\n",
    "\n",
    "First, let's set up our environment and understand how our course content was prepared for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "!pip install boto3 faiss-cpu numpy pandas beautifulsoup4 html2text --quiet\n",
    "\n",
    "print(\"âœ… Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Configuration\n",
    "\n",
    "Set up AWS Bedrock connection and file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ AWS Region: us-west-2\n",
      "ğŸ“ Embeddings file: ../embeddings/course_embeddings.json\n",
      "ğŸ§  LLM Model: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "âœ… Connected to AWS Bedrock successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "AWS_REGION = \"us-west-2\"  # Change if you prefer a different region\n",
    "EMBEDDINGS_FILE = \"../embeddings/course_embeddings.json\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v2:0\"\n",
    "LLM_MODEL = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "print(f\"ğŸŒ AWS Region: {AWS_REGION}\")\n",
    "print(f\"ğŸ“ Embeddings file: {EMBEDDINGS_FILE}\")\n",
    "print(f\"ğŸ§  LLM Model: {LLM_MODEL}\")\n",
    "\n",
    "# Initialize AWS Bedrock client\n",
    "try:\n",
    "    bedrock_client = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "    print(\"âœ… Connected to AWS Bedrock successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to connect to AWS Bedrock: {e}\")\n",
    "    print(\"Please ensure your AWS credentials are configured correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding Our Course Content Data\n",
    "\n",
    "Before we build our agent, let's understand how our course content was prepared. The embeddings were created using the script at `../course_embeddings_generator.py`.\n",
    "\n",
    "**How the embeddings were generated:**\n",
    "1. **HTML Extraction**: Converted HTML pages to clean text using `html2text`\n",
    "2. **Semantic Chunking**: Split content by `<section>` elements for logical units\n",
    "3. **Vectorization**: Created embeddings using AWS Bedrock Titan Embeddings\n",
    "4. **Storage**: Saved as structured JSON with metadata\n",
    "\n",
    "Let's load and explore this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded embeddings from ../embeddings/course_embeddings.json\n",
      "\n",
      "ğŸ“ˆ Content Statistics:\n",
      "   ğŸ“„ Files processed: 6\n",
      "   ğŸ“ Content chunks: 43\n",
      "   ğŸ“Š Total words: 18,800\n",
      "   ğŸ§  Embedding dimension: 1024\n",
      "   ğŸ• Created: 2025-05-25T07:43:34\n",
      "\n",
      "ğŸ“š Sample content chunks:\n",
      "   1. Welcome to AI Foundations (38 words) - index.html\n",
      "   2. Understanding Generative AI (358 words) - index.html\n",
      "   3. The Evolution of Artificial Intelligence (277 words) - index.html\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-generated embeddings\n",
    "def load_course_embeddings(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load embeddings and metadata from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… Loaded embeddings from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Embeddings file not found: {file_path}\")\n",
    "        print(\"Please ensure you've run the embedding generation script first\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading embeddings: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load the data\n",
    "embeddings_data = load_course_embeddings(EMBEDDINGS_FILE)\n",
    "\n",
    "if embeddings_data:\n",
    "    metadata = embeddings_data['metadata']\n",
    "    chunks = embeddings_data['chunks']\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Content Statistics:\")\n",
    "    print(f\"   ğŸ“„ Files processed: {len(metadata['processed_files'])}\")\n",
    "    print(f\"   ğŸ“ Content chunks: {metadata['chunk_count']}\")\n",
    "    print(f\"   ğŸ“Š Total words: {metadata['total_words']:,}\")\n",
    "    print(f\"   ğŸ§  Embedding dimension: {metadata['embedding_dimension']}\")\n",
    "    print(f\"   ğŸ• Created: {metadata['created_at'][:19]}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“š Sample content chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"   {i+1}. {chunk['title']} ({chunk['word_count']} words) - {chunk['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Creating the Search Index\n",
    "\n",
    "Now let's create a FAISS (Facebook AI Similarity Search) index from our embeddings. This will enable fast semantic search over our course content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created FAISS index with 43 vectors\n",
      "ğŸ“ Vector dimension: 1024\n",
      "\n",
      "ğŸ¯ Search index ready! We can now find relevant content for any query.\n"
     ]
    }
   ],
   "source": [
    "def create_search_index(embeddings_data: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Create FAISS index for fast similarity search\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (faiss_index, chunks_list)\n",
    "    \"\"\"\n",
    "    if not embeddings_data:\n",
    "        return None, []\n",
    "    \n",
    "    chunks = embeddings_data['chunks']\n",
    "    \n",
    "    # Extract embeddings as numpy array\n",
    "    embeddings_matrix = np.array([chunk['embedding'] for chunk in chunks], dtype=np.float32)\n",
    "    \n",
    "    # Create FAISS index (using Inner Product for cosine similarity)\n",
    "    dimension = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product index\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings_matrix)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    index.add(embeddings_matrix)\n",
    "    \n",
    "    print(f\"âœ… Created FAISS index with {index.ntotal} vectors\")\n",
    "    print(f\"ğŸ“ Vector dimension: {dimension}\")\n",
    "    \n",
    "    return index, chunks\n",
    "\n",
    "# Create the search index\n",
    "search_index, content_chunks = create_search_index(embeddings_data)\n",
    "\n",
    "if search_index:\n",
    "    print(f\"\\nğŸ¯ Search index ready! We can now find relevant content for any query.\")\n",
    "else:\n",
    "    print(\"âŒ Failed to create search index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Quick Search Test\n",
    "\n",
    "Let's test our search index with a simple query to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Search results for: 'What are the key characteristics of large language models?'\n",
      "--------------------------------------------------\n",
      "\n",
      "1. **Module 1: Understanding Large Language Models** (Score: 0.685)\n",
      "   Source: llm.html\n",
      "   Preview: ## Module 1: Understanding Large Language Models\n",
      "\n",
      "Large Language Models (LLMs) are sophisticated AI systems, trained on vast amounts of text data, tha...\n",
      "\n",
      "2. **6. LLM Evolution & Architectural Advances** (Score: 0.561)\n",
      "   Source: llm.html\n",
      "   Preview: ## 6\\. LLM Evolution & Architectural Advances\n",
      "\n",
      "#### Early LLM Development (2017-2022)\n",
      "\n",
      "The modern Large Language Model era began with the 2017 paper \"...\n"
     ]
    }
   ],
   "source": [
    "def quick_search_test(query: str, top_k: int = 2):\n",
    "    \"\"\"Test the search functionality with a sample query\"\"\"\n",
    "    \n",
    "    if not search_index:\n",
    "        print(\"âŒ Search index not available\")\n",
    "        return\n",
    "    \n",
    "    # Create query embedding\n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=EMBEDDING_MODEL,\n",
    "            body=json.dumps({\"inputText\": query})\n",
    "        )\n",
    "        query_embedding = json.loads(response['body'].read())['embedding']\n",
    "        \n",
    "        # Convert to numpy and normalize\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = search_index.search(query_vector, top_k)\n",
    "        \n",
    "        print(f\"ğŸ” Search results for: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            chunk = content_chunks[idx]\n",
    "            print(f\"\\n{i+1}. **{chunk['title']}** (Score: {score:.3f})\")\n",
    "            print(f\"   Source: {chunk['source']}\")\n",
    "            print(f\"   Preview: {chunk['content'][:150]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Search test failed: {e}\")\n",
    "\n",
    "# Test with a course-related query\n",
    "quick_search_test(\"What are the key characteristics of large language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ‰ Section 1 Complete!**\n",
    "\n",
    "You now have:\n",
    "- âœ… Course content loaded and indexed for search\n",
    "- âœ… Understanding of how embeddings enable semantic search  \n",
    "- âœ… A working search system ready for agent integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Tool Implementation (20 minutes)\n",
    "\n",
    "Now let's build the two tools our agent will use. Remember: **tools are how agents interact with the world beyond just generating text.**\n",
    "\n",
    "## ğŸ› ï¸ Tool Design Principles\n",
    "\n",
    "Good agent tools should:\n",
    "1. **Do one job well** - Clear, focused purpose\n",
    "2. **Have clean interfaces** - Easy for agents to understand and use\n",
    "3. **Provide useful outputs** - Structured, informative results\n",
    "4. **Handle errors gracefully** - Helpful error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Tool 1: Content Search\n",
    "\n",
    "This tool searches our course content using semantic similarity. It's a **retrieval tool** - it finds and returns existing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Content search tool implemented!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Clean data structure for search results\"\"\"\n",
    "    content: str\n",
    "    title: str\n",
    "    source: str\n",
    "    relevance_score: float\n",
    "\n",
    "def search_content_tool(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Search course content using semantic similarity\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    if not search_index or not content_chunks:\n",
    "        return \"Error: Search index not available\"\n",
    "    \n",
    "    if not query.strip():\n",
    "        return \"Error: Search query cannot be empty\"\n",
    "    \n",
    "    try:\n",
    "        # Create query embedding\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=EMBEDDING_MODEL,\n",
    "            body=json.dumps({\"inputText\": query})\n",
    "        )\n",
    "        query_embedding = json.loads(response['body'].read())['embedding']\n",
    "        \n",
    "        # Convert to numpy and normalize for cosine similarity\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search for similar content\n",
    "        scores, indices = search_index.search(query_vector, max_results)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if score > 0.3:  # Only include reasonably relevant results\n",
    "                chunk = content_chunks[idx]\n",
    "                results.append(SearchResult(\n",
    "                    content=chunk['content'][:500] + \"...\" if len(chunk['content']) > 500 else chunk['content'],\n",
    "                    title=chunk['title'],\n",
    "                    source=chunk['source'],\n",
    "                    relevance_score=float(score)\n",
    "                ))\n",
    "        \n",
    "        if not results:\n",
    "            return f\"No relevant content found for query: '{query}'\"\n",
    "        \n",
    "        # Format output for the agent\n",
    "        output = f\"Found {len(results)} relevant content sections for '{query}':\\n\\n\"\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            output += f\"{i}. **{result.title}** (Relevance: {result.relevance_score:.3f})\\n\"\n",
    "            output += f\"   Source: {result.source}\\n\"\n",
    "            output += f\"   Content: {result.content}\\n\\n\"\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching content: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Content search tool implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test Tool 1: Content Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Testing Content Search Tool\n",
      "\n",
      "Query: What is prompt engineering?\n",
      "==================================================\n",
      "Found 2 relevant content sections for 'What is prompt engineering?':\n",
      "\n",
      "1. **1. Prompt Engineering Overview** (Relevance: 0.620)\n",
      "   Source: prompts.html\n",
      "   Content: ## 1\\. Prompt Engineering Overview\n",
      "\n",
      "Input (Prompt) AI Model (Processing) Output (Response)\n",
      "\n",
      "### 1.1 What are Prompts?\n",
      "\n",
      "A **prompt** is the input you provide to an AI system to elicit a specific output. Think of it as the interface between human intent and AI capabilityâ€”they're how we communicate what we want the model to do.\n",
      "\n",
      "In technical terms, a **prompt is a sequence of tokens (words, characters, or subwords) that provides context and instructions** to a language model.\n",
      "\n",
      "**Simple Prompt:** \"W...\n",
      "\n",
      "2. **3. Prompt Engineering Techniques** (Relevance: 0.549)\n",
      "   Source: prompts.html\n",
      "   Content: ## 3\\. Prompt Engineering Techniques\n",
      "\n",
      "Beyond fundamental principles, prompt engineering includes specialized techniques that can significantly enhance model performance for specific tasks and scenarios. This toolkit of advanced approaches allows you to progressively refine your prompts when facing complex challenges, moving from simpler techniques to more sophisticated methods, only as needed, to achieve your desired outcomes.\n",
      "\n",
      "### 3.1 Intermediate Techniques\n",
      "\n",
      "#### 3.1.1 Role Assignment\n",
      "\n",
      "**What ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the content search tool\n",
    "print(\"ğŸ” Testing Content Search Tool\\n\")\n",
    "\n",
    "test_query = \"What is prompt engineering?\"\n",
    "search_result = search_content_tool(test_query, max_results=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"=\" * 50)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” Tool 2: Follow-up Question Generator\n",
    "\n",
    "This tool generates relevant follow-up questions to enhance learning. It's a **generative tool** - it creates new content using an LLM. This demonstrates the **\"LLM-as-tool\"** pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Follow-up question generator tool implemented!\n"
     ]
    }
   ],
   "source": [
    "def generate_followup_questions_tool(current_topic: str, conversation_context: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate relevant follow-up questions to enhance learning\n",
    "    \n",
    "    Args:\n",
    "        current_topic: The topic being discussed\n",
    "        conversation_context: Recent conversation for context\n",
    "        \n",
    "    Returns:\n",
    "        Formatted follow-up questions\n",
    "    \"\"\"\n",
    "    \n",
    "    if not current_topic.strip():\n",
    "        return \"Error: Topic cannot be empty for question generation\"\n",
    "    \n",
    "    try:\n",
    "        # Create prompt for follow-up question generation\n",
    "        prompt = f\"\"\"You are an educational assistant helping students explore Generative AI concepts more deeply.\n",
    "\n",
    "Based on the current topic '{current_topic}' and this conversation context:\n",
    "{conversation_context}\n",
    "\n",
    "Generate 3 thoughtful follow-up questions that would help a student:\n",
    "1. Deepen their understanding of this topic\n",
    "2. Connect it to other course concepts (LLMs, prompt engineering, agents)\n",
    "3. Apply it practically or think about real-world implications\n",
    "\n",
    "Make the questions specific, engaging, and educational. Format as:\n",
    "ğŸ¤” Question 1: ...\n",
    "ğŸ¤” Question 2: ...\n",
    "ğŸ¤” Question 3: ...\n",
    "\n",
    "Only return the questions, nothing else.\"\"\"\n",
    "\n",
    "        # Create the request body\n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 300,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "\n",
    "        # Call Claude for question generation\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=LLM_MODEL,\n",
    "            body=json.dumps(request_body),\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        questions = response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        return questions\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating follow-up questions: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Follow-up question generator tool implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test Tool 2: Follow-up Question Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤” Testing Follow-up Question Generator\n",
      "\n",
      "Topic: agent memory systems\n",
      "Context: We just discussed how agents use working memory and episodic memory\n",
      "==================================================\n",
      "ğŸ¤” Question 1: How does the interplay between working memory and episodic memory in AI agents differ from traditional computer memory systems, and what unique challenges arise when trying to implement human-like memory mechanisms in artificial systems?\n",
      "\n",
      "ğŸ¤” Question 2: In what ways could advanced prompt engineering techniques be used to better simulate episodic memory in LLMs, and how might this improve an agent's ability to maintain context consistency across long conversations?\n",
      "\n",
      "ğŸ¤” Question 3: Consider a real-world AI assistant helping multiple users throughout a day - what are the ethical and practical implications of implementing different types of memory systems, especially regarding privacy and information retention?\n"
     ]
    }
   ],
   "source": [
    "# Test the follow-up question generator\n",
    "print(\"ğŸ¤” Testing Follow-up Question Generator\\n\")\n",
    "\n",
    "test_topic = \"agent memory systems\"\n",
    "test_context = \"We just discussed how agents use working memory and episodic memory\"\n",
    "\n",
    "followup_questions = generate_followup_questions_tool(test_topic, test_context)\n",
    "\n",
    "print(f\"Topic: {test_topic}\")\n",
    "print(f\"Context: {test_context}\")\n",
    "print(\"=\" * 50)\n",
    "print(followup_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Tool Comparison Exercise\n",
    "\n",
    "Let's compare our two tools to understand different tool patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Tool Pattern Analysis\n",
      "\n",
      "       Aspect                  Content Search               Follow-up Generator\n",
      "      Purpose       Find existing information       Create new learning content\n",
      "      Pattern                  Retrieval tool     Generative tool (LLM-as-tool)\n",
      "        Input                    Query string                   Topic + context\n",
      "   Processing        Vector similarity search        LLM reasoning + generation\n",
      "       Output      Existing content + sources   New questions tailored to topic\n",
      "Deterministic Yes (same query = same results) No (creative variation each time)\n",
      "\n",
      "ğŸ’¡ Key Insight: Agents become powerful by combining different types of tools!\n",
      "   - Retrieval tools provide accurate, factual information\n",
      "   - Generative tools provide creative, contextual assistance\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”¬ Tool Pattern Analysis\\n\")\n",
    "\n",
    "comparison_data = {\n",
    "    \"Aspect\": [\"Purpose\", \"Pattern\", \"Input\", \"Processing\", \"Output\", \"Deterministic\"],\n",
    "    \"Content Search\": [\n",
    "        \"Find existing information\",\n",
    "        \"Retrieval tool\", \n",
    "        \"Query string\",\n",
    "        \"Vector similarity search\",\n",
    "        \"Existing content + sources\",\n",
    "        \"Yes (same query = same results)\"\n",
    "    ],\n",
    "    \"Follow-up Generator\": [\n",
    "        \"Create new learning content\",\n",
    "        \"Generative tool (LLM-as-tool)\",\n",
    "        \"Topic + context\", \n",
    "        \"LLM reasoning + generation\",\n",
    "        \"New questions tailored to topic\",\n",
    "        \"No (creative variation each time)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insight: Agents become powerful by combining different types of tools!\")\n",
    "print(\"   - Retrieval tools provide accurate, factual information\")\n",
    "print(\"   - Generative tools provide creative, contextual assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ‰ Section 2 Complete!**\n",
    "\n",
    "You now have:\n",
    "- âœ… Two complementary tools with different patterns (retrieval + generative)\n",
    "- âœ… Understanding of clean tool interface design\n",
    "- âœ… Experience with LLM-as-tool patterns\n",
    "- âœ… Working tools ready for agent integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Agent Orchestration (15 minutes)\n",
    "\n",
    "Now for the exciting part: building an **agent** that intelligently orchestrates our tools! \n",
    "\n",
    "## ğŸ§  What Makes This an Agent?\n",
    "\n",
    "Unlike a simple LLM application, our agent will:\n",
    "1. **Decide** which tools to use based on the user's request\n",
    "2. **Coordinate** multiple tools in sequence when needed\n",
    "3. **Reason** about when to offer follow-up questions\n",
    "4. **Adapt** its behavior based on context\n",
    "\n",
    "This is **orchestration** - the agent acts as the conductor of an orchestra of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Course Assistant Agent initialized!\n",
      "   Available tools: ['search_content', 'generate_followup_questions']\n",
      "\n",
      "âœ… Agent ready for testing!\n"
     ]
    }
   ],
   "source": [
    "class CourseAssistantAgent:\n",
    "    \"\"\"\n",
    "    A personal assistant agent for course content\n",
    "    \n",
    "    This agent demonstrates the core pattern of agentic applications:\n",
    "    - Observe: Analyze user input and current context\n",
    "    - Plan: Decide which tools to use and in what order\n",
    "    - Act: Execute the plan and generate a response\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Tool registry - the agent's available capabilities\n",
    "        self.tools = {\n",
    "            \"search_content\": search_content_tool,\n",
    "            \"generate_followup_questions\": generate_followup_questions_tool\n",
    "        }\n",
    "        \n",
    "        # Initialize without memory for now (we'll add this in Section 4)\n",
    "        self.memory = None\n",
    "        \n",
    "        print(\"ğŸ¤– Course Assistant Agent initialized!\")\n",
    "        print(f\"   Available tools: {list(self.tools.keys())}\")\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Helper method to call Claude via Bedrock\"\"\"\n",
    "        try:\n",
    "            # Create the request body\n",
    "            request_body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "\n",
    "            # Call Claude for question generation\n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=LLM_MODEL,\n",
    "                body=json.dumps(request_body),\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            return response_body['content'][0]['text'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error calling LLM: {str(e)}\"\n",
    "    \n",
    "    def _should_offer_followup_questions(self, user_input: str, response: str) -> bool:\n",
    "        \"\"\"Decide if we should offer follow-up questions\"\"\"\n",
    "        \n",
    "        # Check if user explicitly wants follow-up questions\n",
    "        followup_triggers = [\n",
    "            \"follow up\", \"what else\", \"what next\", \"more questions\", \n",
    "            \"dig deeper\", \"learn more\", \"what should i know\"\n",
    "        ]\n",
    "        \n",
    "        if any(trigger in user_input.lower() for trigger in followup_triggers):\n",
    "            return True\n",
    "        \n",
    "        # For now, keep it simple - don't auto-offer \n",
    "        # (In a more sophisticated agent, we could use LLM reasoning here)\n",
    "        return False\n",
    "    \n",
    "    def decide_and_act(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        The main agent decision cycle: Observe â†’ Plan â†’ Act\n",
    "        \n",
    "        Args:\n",
    "            user_input: The user's question or request\n",
    "            \n",
    "        Returns:\n",
    "            Agent's response\n",
    "        \"\"\"\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return \"I'd be happy to help! Please ask me a question about the course content.\"\n",
    "        \n",
    "        # OBSERVE: Analyze the user's input\n",
    "        print(f\"ğŸ” Agent observing: '{user_input}'\")\n",
    "        \n",
    "        # PLAN: Decide what to do\n",
    "        followup_triggers = [\"follow up\", \"what else\", \"what next\", \"more questions\"]\n",
    "        wants_followup = any(trigger in user_input.lower() for trigger in followup_triggers)\n",
    "        \n",
    "        if wants_followup:\n",
    "            print(\"ğŸ“‹ Agent planning: User wants follow-up questions, will generate them\")\n",
    "            # Extract topic from recent context or ask for clarification\n",
    "            topic = \"the current topic we've been discussing\"\n",
    "            result = self.tools[\"generate_followup_questions\"](topic, user_input)\n",
    "            return f\"Here are some follow-up questions to explore further:\\n\\n{result}\"\n",
    "        \n",
    "        # Default plan: Search for content first, then respond\n",
    "        print(\"ğŸ“‹ Agent planning: Will search content and provide comprehensive response\")\n",
    "        \n",
    "        # ACT: Execute the plan\n",
    "        \n",
    "        # Step 1: Search for relevant content\n",
    "        print(\"âš¡ Agent acting: Searching course content...\")\n",
    "        search_results = self.tools[\"search_content\"](user_input, max_results=3)\n",
    "        \n",
    "        # Step 2: Generate response using search results\n",
    "        print(\"âš¡ Agent acting: Generating response with found content...\")\n",
    "        response_prompt = f\"\"\"You are a helpful course assistant for an AI/ML education program.\n",
    "\n",
    "A student asked: \"{user_input}\"\n",
    "\n",
    "Here's relevant content from the course materials:\n",
    "{search_results}\n",
    "\n",
    "Based on this content, provide a clear, helpful answer to the student's question. \n",
    "Focus on the key concepts and make it educational. If the search results don't \n",
    "contain relevant information, say so politely and offer to help with other topics.\n",
    "\n",
    "Keep your response concise but complete (2-3 paragraphs maximum).\"\"\"\n",
    "        \n",
    "        main_response = self._call_llm(response_prompt, max_tokens=400)\n",
    "        \n",
    "        # Step 3: Decide if we should offer follow-up questions\n",
    "        if self._should_offer_followup_questions(user_input, main_response):\n",
    "            print(\"âš¡ Agent acting: Adding follow-up questions...\")\n",
    "            followup_questions = self.tools[\"generate_followup_questions\"](user_input)\n",
    "            return f\"{main_response}\\n\\n---\\nğŸ’¡ **Want to explore further?**\\n{followup_questions}\"\n",
    "        else:\n",
    "            # Subtly offer follow-up option without being pushy\n",
    "            return f\"{main_response}\\n\\nğŸ’¡ *Want me to suggest follow-up questions? Just ask!*\"\n",
    "\n",
    "# Create our agent\n",
    "agent = CourseAssistantAgent()\n",
    "print(\"\\nâœ… Agent ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Test the Agent: Basic Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– Testing Agent - Basic Question\\n\")\n",
    "\n",
    "user_question = \"What are the main differences between LLMs and AI agents?\"\n",
    "print(f\"User: {user_question}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "response = agent.decide_and_act(user_question)\n",
    "print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Test the Agent: Follow-up Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– Testing Agent - Follow-up Questions\\n\")\n",
    "\n",
    "followup_request = \"What else should I know about prompt engineering?\"\n",
    "print(f\"User: {followup_request}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "response = agent.decide_and_act(followup_request)\n",
    "print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Interactive Demo: Try Your Own Questions!\n",
    "\n",
    "Now you can interact with the agent directly. Try different types of questions to see how it chooses tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_agent_demo():\n",
    "    \"\"\"Interactive demo for testing the agent\"\"\"\n",
    "    print(\"ğŸ® Interactive Agent Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Ask questions about:\")\n",
    "    print(\"  â€¢ LLM fundamentals\")\n",
    "    print(\"  â€¢ Prompt engineering techniques\")\n",
    "    print(\"  â€¢ Agent concepts and memory\")\n",
    "    print(\"  â€¢ Or request 'follow up questions' or 'what else should I know?'\")\n",
    "    print(\"\\nType 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'stop']:\n",
    "            print(\"ğŸ‘‹ Thanks for testing the agent!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAgent: {agent.decide_and_act(user_input)}\")\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Uncomment the line below to run the interactive demo\n",
    "# interactive_agent_demo()\n",
    "\n",
    "print(\"ğŸ’¡ Uncomment the line above to try the interactive demo!\")\n",
    "print(\"   Or run this cell and test specific questions in the cells below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ” Understanding Agent Decision-Making**\n",
    "\n",
    "Notice how the agent:\n",
    "1. **Analyzes** the user's input (Observe)\n",
    "2. **Decides** which tools to use (Plan)\n",
    "3. **Coordinates** tool execution (Act)\n",
    "4. **Adapts** behavior based on context\n",
    "\n",
    "This is the core difference between agents and simple LLM applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ‰ Section 3 Complete!**\n",
    "\n",
    "You now have:\n",
    "- âœ… A working agent that orchestrates multiple tools\n",
    "- âœ… Understanding of the agent decision cycle (Observe â†’ Plan â†’ Act)\n",
    "- âœ… Experience with tool coordination and context-aware responses\n",
    "- âœ… A foundation ready for memory integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Memory Integration (15 minutes)\n",
    "\n",
    "The final piece: giving our agent **memory**! This transforms it from a stateless system into one that can maintain context across multiple interactions.\n",
    "\n",
    "## ğŸ§  Why Memory Matters\n",
    "\n",
    "Without memory, each interaction is isolated:\n",
    "- âŒ \"What did we just discuss?\"\n",
    "- âŒ \"Can you elaborate on that previous point?\"\n",
    "- âŒ Learning from past mistakes or preferences\n",
    "\n",
    "With memory, agents become context-aware:\n",
    "- âœ… Remembers conversation history\n",
    "- âœ… Builds on previous discussions\n",
    "- âœ… Provides continuity across interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Implementing Episodic Memory\n",
    "\n",
    "We'll implement simple **episodic memory** - storing and retrieving conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicMemory:\n",
    "    \"\"\"\n",
    "    Simple episodic memory for storing conversation history\n",
    "    \n",
    "    In production, this might use a database, but for learning\n",
    "    purposes, we'll use in-memory storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversations = []  # List of conversation turns\n",
    "        print(\"ğŸ§  Episodic memory initialized\")\n",
    "    \n",
    "    def store_interaction(self, user_input: str, agent_response: str):\n",
    "        \"\"\"\n",
    "        Store a conversation turn\n",
    "        \n",
    "        Args:\n",
    "            user_input: What the user said\n",
    "            agent_response: How the agent responded\n",
    "        \"\"\"\n",
    "        interaction = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"user\": user_input,\n",
    "            \"agent\": agent_response,\n",
    "            \"turn_number\": len(self.conversations) + 1\n",
    "        }\n",
    "        \n",
    "        self.conversations.append(interaction)\n",
    "        print(f\"ğŸ’¾ Stored interaction #{interaction['turn_number']}\")\n",
    "    \n",
    "    def get_recent_context(self, max_turns: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Get recent conversation history for context\n",
    "        \n",
    "        Args:\n",
    "            max_turns: Maximum number of recent turns to include\n",
    "            \n",
    "        Returns:\n",
    "            Formatted conversation context\n",
    "        \"\"\"\n",
    "        if not self.conversations:\n",
    "            return \"No previous conversation history.\"\n",
    "        \n",
    "        # Get the most recent turns\n",
    "        recent = self.conversations[-max_turns:]\n",
    "        \n",
    "        context = \"Recent conversation history:\\n\"\n",
    "        for turn in recent:\n",
    "            # Truncate long responses for context\n",
    "            agent_preview = turn['agent'][:100] + \"...\" if len(turn['agent']) > 100 else turn['agent']\n",
    "            context += f\"Turn {turn['turn_number']} - User: {turn['user']}\\n\"\n",
    "            context += f\"Turn {turn['turn_number']} - Agent: {agent_preview}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_conversation_summary(self) -> str:\n",
    "        \"\"\"Get a summary of the entire conversation\"\"\"\n",
    "        if not self.conversations:\n",
    "            return \"No conversations yet.\"\n",
    "        \n",
    "        total_turns = len(self.conversations)\n",
    "        topics = []\n",
    "        \n",
    "        # Extract key topics mentioned (simple keyword extraction)\n",
    "        ai_terms = ['llm', 'prompt', 'agent', 'memory', 'tool', 'embedding', 'model']\n",
    "        \n",
    "        for turn in self.conversations:\n",
    "            user_text = turn['user'].lower()\n",
    "            for term in ai_terms:\n",
    "                if term in user_text and term not in topics:\n",
    "                    topics.append(term)\n",
    "        \n",
    "        return f\"Conversation summary: {total_turns} turns, topics discussed: {', '.join(topics) if topics else 'general questions'}\"\n",
    "\n",
    "print(\"âœ… Episodic memory class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Updating the Agent with Memory\n",
    "\n",
    "Now let's enhance our agent to use memory in its decision-making:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEnabledAgent(CourseAssistantAgent):\n",
    "    \"\"\"\n",
    "    Enhanced agent with episodic memory capabilities\n",
    "    \n",
    "    This agent demonstrates how memory integration transforms\n",
    "    agent behavior from stateless to stateful.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.memory = EpisodicMemory()\n",
    "        print(\"ğŸ§  Memory-enabled agent initialized!\")\n",
    "    \n",
    "    def decide_and_act(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced decision cycle with memory integration\n",
    "        \n",
    "        Key difference from base agent: includes conversation context\n",
    "        in reasoning and stores interactions for future use.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return \"I'd be happy to help! Please ask me a question about the course content.\"\n",
    "        \n",
    "        # OBSERVE: Analyze input AND retrieve relevant memory\n",
    "        print(f\"ğŸ” Agent observing: '{user_input}'\")\n",
    "        conversation_context = self.memory.get_recent_context(max_turns=3)\n",
    "        print(f\"ğŸ§  Agent remembering: {len(self.memory.conversations)} previous turns\")\n",
    "        \n",
    "        # PLAN: Enhanced planning with memory context\n",
    "        followup_triggers = [\"follow up\", \"what else\", \"what next\", \"more questions\", \"elaborate\", \"tell me more\"]\n",
    "        context_triggers = [\"we discussed\", \"you mentioned\", \"earlier\", \"before\", \"that previous\"]\n",
    "        \n",
    "        wants_followup = any(trigger in user_input.lower() for trigger in followup_triggers)\n",
    "        references_context = any(trigger in user_input.lower() for trigger in context_triggers)\n",
    "        \n",
    "        if wants_followup:\n",
    "            print(\"ğŸ“‹ Agent planning: User wants follow-up questions, using conversation context\")\n",
    "            # Use memory to provide better context for follow-up questions\n",
    "            if self.memory.conversations:\n",
    "                last_topic = self.memory.conversations[-1]['user']\n",
    "                result = self.tools[\"generate_followup_questions\"](last_topic, conversation_context)\n",
    "            else:\n",
    "                result = \"I'd be happy to generate follow-up questions! What topic would you like to explore further?\"\n",
    "            \n",
    "            # Store this interaction\n",
    "            self.memory.store_interaction(user_input, result)\n",
    "            return f\"Here are some follow-up questions based on our conversation:\\n\\n{result}\"\n",
    "        \n",
    "        # Default plan: Search + respond with memory context\n",
    "        print(\"ğŸ“‹ Agent planning: Will search content and provide response with conversation context\")\n",
    "        \n",
    "        # ACT: Execute plan with memory integration\n",
    "        \n",
    "        # Step 1: Search for relevant content\n",
    "        print(\"âš¡ Agent acting: Searching course content...\")\n",
    "        search_results = self.tools[\"search_content\"](user_input, max_results=3)\n",
    "        \n",
    "        # Step 2: Generate response with memory context\n",
    "        print(\"âš¡ Agent acting: Generating response with content and conversation context...\")\n",
    "        \n",
    "        response_prompt = f\"\"\"You are a helpful course assistant for an AI/ML education program.\n",
    "\n",
    "Student's current question: \"{user_input}\"\n",
    "\n",
    "Conversation context:\n",
    "{conversation_context}\n",
    "\n",
    "Relevant course content:\n",
    "{search_results}\n",
    "\n",
    "Provide a helpful answer that:\n",
    "1. Addresses the current question using the course content\n",
    "2. References previous conversation when relevant\n",
    "3. Builds on topics we've already discussed\n",
    "4. Maintains conversational continuity\n",
    "\n",
    "If the question references previous discussion, acknowledge that connection.\n",
    "Keep your response clear and educational (2-3 paragraphs maximum).\"\"\"\n",
    "        \n",
    "        main_response = self._call_llm(response_prompt, max_tokens=400)\n",
    "        \n",
    "        # Step 3: Store interaction in memory\n",
    "        self.memory.store_interaction(user_input, main_response)\n",
    "        \n",
    "        # Step 4: Decide on follow-up offer (enhanced with memory)\n",
    "        if references_context or len(self.memory.conversations) > 2:\n",
    "            # For ongoing conversations, be more proactive about follow-ups\n",
    "            return f\"{main_response}\\n\\nğŸ’¡ *Want me to suggest follow-up questions based on our discussion? Just ask!*\"\n",
    "        else:\n",
    "            return f\"{main_response}\\n\\nğŸ’¡ *Want me to suggest follow-up questions? Just ask!*\"\n",
    "\n",
    "# Create the memory-enabled agent\n",
    "memory_agent = MemoryEnabledAgent()\n",
    "print(\"\\nâœ… Memory-enabled agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Compare: Agent With vs Without Memory\n",
    "\n",
    "Let's see the difference memory makes in a multi-turn conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ Memory Comparison Demo\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate a conversation sequence\n",
    "conversation = [\n",
    "    \"What is prompt engineering?\",\n",
    "    \"Can you elaborate on that?\",\n",
    "    \"What techniques did we just discuss?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– WITHOUT MEMORY (Original Agent):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, question in enumerate(conversation, 1):\n",
    "    print(f\"\\nTurn {i} - User: {question}\")\n",
    "    response = agent.decide_and_act(question)\n",
    "    # Show truncated response for comparison\n",
    "    short_response = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    print(f\"Turn {i} - Agent: {short_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ§  WITH MEMORY (Memory-Enabled Agent):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, question in enumerate(conversation, 1):\n",
    "    print(f\"\\nTurn {i} - User: {question}\")\n",
    "    response = memory_agent.decide_and_act(question)\n",
    "    # Show truncated response for comparison\n",
    "    short_response = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    print(f\"Turn {i} - Agent: {short_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“Š Memory Summary: {memory_agent.memory.get_conversation_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Test Memory Functionality\n",
    "\n",
    "Let's test specific memory features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Testing Memory Features\\n\")\n",
    "\n",
    "# Test 1: Context reference\n",
    "print(\"Test 1: Referencing previous conversation\")\n",
    "test_question = \"You mentioned techniques earlier - can you give me more details?\"\n",
    "response = memory_agent.decide_and_act(test_question)\n",
    "print(f\"Response: {response[:200]}...\\n\")\n",
    "\n",
    "# Test 2: Follow-up request\n",
    "print(\"Test 2: Requesting follow-up questions\")\n",
    "followup_request = \"What else should I know about this topic?\"\n",
    "response = memory_agent.decide_and_act(followup_request)\n",
    "print(f\"Response: {response[:200]}...\\n\")\n",
    "\n",
    "# Test 3: Memory inspection\n",
    "print(\"Test 3: Current memory state\")\n",
    "print(f\"Total conversations stored: {len(memory_agent.memory.conversations)}\")\n",
    "print(f\"Memory summary: {memory_agent.memory.get_conversation_summary()}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Recent context being used:\")\n",
    "print(memory_agent.memory.get_recent_context(max_turns=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Final Interactive Demo: Memory-Enabled Agent\n",
    "\n",
    "Try a conversation with the memory-enabled agent to see how it maintains context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_agent_demo():\n",
    "    \"\"\"Interactive demo with the memory-enabled agent\"\"\"\n",
    "    print(\"ğŸ§  Memory-Enabled Agent Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Try these to see memory in action:\")\n",
    "    print(\"  â€¢ Ask a question about AI concepts\")\n",
    "    print(\"  â€¢ Follow up with 'tell me more about that'\")\n",
    "    print(\"  â€¢ Reference previous discussion: 'you mentioned...'\")\n",
    "    print(\"  â€¢ Request: 'what else should I know?'\")\n",
    "    print(\"\\nType 'memory' to see conversation history\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'stop']:\n",
    "            print(\"\\nğŸ“Š Final conversation summary:\")\n",
    "            print(memory_agent.memory.get_conversation_summary())\n",
    "            print(\"ğŸ‘‹ Thanks for testing the memory-enabled agent!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'memory':\n",
    "            print(\"\\nğŸ§  Current Memory State:\")\n",
    "            print(memory_agent.memory.get_recent_context())\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAgent: {memory_agent.decide_and_act(user_input)}\")\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Uncomment to run the interactive demo\n",
    "# memory_agent_demo()\n",
    "\n",
    "print(\"ğŸ’¡ Uncomment the line above to try the memory-enabled agent!\")\n",
    "print(\"   Notice how it references previous conversations and builds context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Understanding Memory Integration\n",
    "\n",
    "**Key Memory Design Decisions:**\n",
    "\n",
    "1. **Developer-Controlled Retrieval**: We always include recent context rather than letting the LLM decide when to retrieve memory. This ensures reliability while keeping the implementation simple.\n",
    "\n",
    "2. **Automatic Storage**: Every interaction is stored automatically, ensuring no conversation history is lost.\n",
    "\n",
    "3. **Context Window Management**: We limit context to recent turns to stay within LLM token limits while maintaining relevance.\n",
    "\n",
    "**Production Considerations:**\n",
    "- Real agents might use databases for persistent memory\n",
    "- Advanced systems could use LLM-controlled memory retrieval\n",
    "- Semantic memory (facts/preferences) could be added alongside episodic memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ‰ Section 4 Complete!**\n",
    "\n",
    "You now have:\n",
    "- âœ… A complete agent with episodic memory\n",
    "- âœ… Understanding of memory integration patterns\n",
    "- âœ… Experience with stateful vs stateless AI applications\n",
    "- âœ… Hands-on knowledge of agent orchestration, tools, and memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Lab Summary & Key Takeaways\n",
    "\n",
    "Congratulations! You've built a complete AI agent from scratch. Let's review what you've accomplished:\n",
    "\n",
    "## ğŸ—ï¸ What You Built\n",
    "\n",
    "1. **Content Search Tool** - Semantic search using embeddings and FAISS\n",
    "2. **Follow-up Question Generator** - LLM-as-tool for educational enhancement\n",
    "3. **Agent Orchestration** - Decision logic for tool coordination\n",
    "4. **Episodic Memory** - Conversation history for context continuity\n",
    "\n",
    "## ğŸ§  Key Concepts Learned\n",
    "\n",
    "### Agent vs LLM Application\n",
    "- **LLM Application**: Single-step input â†’ output processing\n",
    "- **Agent**: Multi-step reasoning with tools, memory, and adaptive behavior\n",
    "\n",
    "### Tool Patterns\n",
    "- **Retrieval Tools**: Access external information (search, databases)\n",
    "- **Generative Tools**: Create new content using LLMs\n",
    "- **Clean Interfaces**: Well-structured inputs/outputs for reliability\n",
    "\n",
    "### Memory Types\n",
    "- **Working Memory**: Current context and reasoning\n",
    "- **Episodic Memory**: Conversation history and past interactions\n",
    "- **Integration Strategy**: Developer vs LLM-controlled retrieval\n",
    "\n",
    "### Orchestration Patterns\n",
    "- **Observe**: Analyze input and retrieve relevant context\n",
    "- **Plan**: Decide which tools to use and in what order\n",
    "- **Act**: Execute the plan and generate responses\n",
    "\n",
    "## ğŸš€ Looking Forward\n",
    "\n",
    "Your agent demonstrates the fundamental patterns of modern AI applications. In production systems, you might see:\n",
    "\n",
    "- **More sophisticated memory systems** (vector databases, semantic memory)\n",
    "- **Advanced orchestration frameworks** (LangChain, CrewAI)\n",
    "- **Multi-agent collaboration** (specialized agents working together)\n",
    "- **Human-in-the-loop systems** (approval workflows, escalation)\n",
    "\n",
    "## ğŸ› ï¸ Extension Ideas\n",
    "\n",
    "Want to enhance your agent further? Try:\n",
    "\n",
    "1. **Add more tools** (calculator, web search, code executor)\n",
    "2. **Implement user preferences** (learning style, preferred topics)\n",
    "3. **Add error recovery** (retry logic, fallback strategies)\n",
    "4. **Create specialized agents** (different personas for different subjects)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations on completing the AI Agents Foundations Lab!**\n",
    "\n",
    "You now understand the core principles of building production-grade AI agents and have hands-on experience with the patterns that power modern agentic applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
