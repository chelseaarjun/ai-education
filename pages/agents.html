<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agents</title>
    <script data-goatcounter="https://ai-foundation-course.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
    <link rel="stylesheet" href="../assets/css/course-nav.css">
    <link rel="stylesheet" href="../assets/css/module-nav-bar.css">
    <link rel="stylesheet" href="../assets/css/module-sidebar.css">
    <link rel="stylesheet" href="../assets/css/code-examples.css">
    <link rel="stylesheet" href="../assets/css/tables.css">
    <link rel="stylesheet" href="../assets/css/course-index.css">
    <link rel="stylesheet" href="../assets/css/mobile-fixes.css">
    <link rel="stylesheet" href="../assets/css/introduction-fix.css">
    <link rel="stylesheet" href="../assets/css/quiz.css">
    <style>
        /* Removed .course-description styles, now in shared CSS */
        .diagram {
            display: block;
            margin: 25px auto;
            max-width: 100%;
            text-align: center;
            border: 1px solid var(--border);
            border-radius: 4px;
            padding: 10px;
            overflow: hidden;
        }

        .subtitle {
            font-size: 1.2rem;
            margin-top: 0.5rem;
            font-weight: 400;
            color: var(--secondary);
            line-height: 1.5;
        }

        /* Section content spacing */
        section > *:first-child {
            margin-top: 0;
        }

        section > p:first-of-type {
            margin-top: 0.5rem;
        }

        /* Lists */
        ul, ol {
            margin-top: 0.5rem;
            margin-bottom: 1rem;
            padding-left: 1.5rem;
        }

        li {
            margin-bottom: 0.5rem;
            line-height: var(--line-height-body);
        }

        /* Examples and components spacing */
        .example:first-child,
        .prompt-example:first-child,
        .response-example:first-child,
        .crisp-component:first-child,
        .interactive-demo:first-child {
            margin-top: 0.5rem;
        }

        /* Examples */
        .example {
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .example:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);
        }

        /* Add responsive adjustments */
        @media (max-width: 768px) {
            .example, .crisp-component, .interactive-demo {
                padding: 1rem;
                margin: 0.75rem 0;
            }

            .demo-button {
                width: 100%;
            }
        }

        /* Introduction section specific styles */
        #fundamentals {
            padding-top: 0.75rem;
        }

        #fundamentals h2 {
            margin-top: 0;
            margin-bottom: 1rem;
        }

        #fundamentals h3 {
            margin-top: 1rem;
            margin-bottom: 0.5rem;
        }

        #fundamentals p {
            margin-top: 0.5rem;
            margin-bottom: 1rem;
        }

        #fundamentals ul {
            margin-top: 0.5rem;
            margin-bottom: 1rem;
        }

        #fundamentals .section-nav-btns {
            margin-top: 1rem;
        }
        .code-summary {
            background: #f8fafc;
            border-left: 4px solid #4a6fa5;
            border-radius: 6px;
            padding: 0.75em 1em;
            margin-bottom: 0.7em;
            font-size: 1em;
            color: #2d3a4a;
            line-height: 1.6;
        }
        .dimension {
            font-weight: bold;
        }
    </style>
</head>
<body>
    <nav class="module-nav">
        <button class="module-nav-btn" data-section="introduction">Introduction</button>   
        <button class="module-nav-btn" data-section="agent-intro">History</button>       
        <button class="module-nav-btn" data-section="fundamentals">Agent</button>
        <button class="module-nav-btn" data-section="when-to-use-agents">Choice</button>
        <button class="module-nav-btn" data-section="memory">Memory</button>
        <button class="module-nav-btn" data-section="tools">Tools</button>
        <button class="module-nav-btn" data-section="decision-cycle">Decision Cycle</button>
        <button class="module-nav-btn" data-section="agent-patterns">Patterns</button>
        <button class="module-nav-btn" data-section="resources">Resources</button>
        <button class="module-nav-btn" data-section="quiz-section">Quiz</button>
    </nav>
    <nav class="module-sidebar"></nav>
    <div class="content-inner">
        <!-- Section 0: Introduction -->
        <section id="introduction">
            <div class="course-description">
                <h2 class="module-title">Module 3: Building Agentic LLM Applications</h2>
                <p>In the previous modules, you learned the foundational building blocks of modern AI applications:</p>
                <ul>
                    <li><strong>Module 1</strong> introduced you to Large Language Models (LLMs)—powerful AI systems capable of understanding and generating human language. LLMs excel at reasoning, summarizing, answering questions, and more, but they operate within certain boundaries: they have no persistent memory, cannot access external tools or data, and do not act autonomously.</li>
                    <li><strong>Module 2</strong> explored the art and science of prompt engineering—the practice of crafting clear, effective instructions to get the best results from LLMs. With strong prompt engineering, you can build surprisingly capable applications using just an LLM, without any additional complexity.</li>
                </ul>
                <p>This module builds on your understanding of LLMs and prompt engineering, showing you how to design and build agents that can remember, reason, use tools, and act autonomously—unlocking a new level of capability for your AI applications.</p>
                <h3 class="section-title">What You'll Learn</h3>
                <p>In this course, we focus specifically on agentic LLM applications that leverage Large Language Models as their core reasoning engine. This represents a modern approach to agent design, with unique capabilities and limitations.
                </p>
                <ul class="enhanced-list">
                    <li><b>Agent Fundamentals:</b> The key characteristics that define agents</li>
                    <li><b>When to use Agents:</b> What makes agents different from Single-Step or Workflow-Based LLM applications.</li>
                    <li><b>Memory:</b> Types of memory and how agents use them</li>
                    <li><b>Tools:</b> How agents use external tools to extend their capabilities</li>
                    <li><b>Decision Cycle:</b> How agents observe, plan, and act in iterative loops</li>
                    <li><b>Agent Patterns:</b> Different agent patterns and production considerations</li>
                </ul>
            </div>  
            <span style='font-size:1.2em; margin-right:0.3em;'>💡</span><strong>Tip:</strong> No matter if you're building a simple LLM-powered app or a sophisticated agent system, every successful AI solution depends on clear, well-crafted prompts - even the most advanced agents rely on clear, well-structured prompts to guide the LLM's reasoning and actions.         
            <div class="section-nav-btns">
                <button id="back-introduction" disabled>Back</button>
                <button id="next-introduction">Next</button>
            </div>
        </section>
        <section id="agent-intro" class="module-section">
            <h2>Historical Context</h2>
            <p>The concept of agents in AI dates back to the 1950s with early work in cybernetics (the study of control systems and information processing in both machines and living organisms) and the development of the first AI programs. The Turing Test, proposed by Alan Turing, established a framework for evaluating if machines could exhibit human-like intelligence—though not specifically defining "agents" as we understand them today.
            In the 1980s and 1990s, the agent paradigm became more formalized in AI research, with researchers developing various types of agents from simple reactive systems to more complex deliberative architectures.</p>
            
            <p>Traditional AI literature identifies several key characteristics of agents:
                <ul>
                    <li><b>Autonomy</b>: The ability to operate without direct human intervention</li>
                    <li><b>Reactivity</b>: The ability to perceive and respond to changes in the environment</li>
                    <li><b>Pro-activeness</b>: The ability to take initiative and pursue goals</li>
                    <li><b>Social Ability</b>: The ability to interact with other agents or humans</li>
                </ul>
                However, it's crucial to understand that these characteristics exist on a spectrum rather than as binary attributes.
            </p>
            <div class="callout-info" style="background:#f8fafd;border-left:4px solid #4a6fa5;margin:1.2em 0 0.5em 0;padding:1em 1.2em;">Rather than thinking of agency as binary (either something is an agent or it's not), it's more helpful to consider a spectrum of agency, varying from highly supervised to fully autonomous.</div>
            <h3>What is an Agent?</h3>
            <p>An <strong>agent is a system that perceives its environment through sensors, processes this information, and acts upon the environment through actuators to achieve specific goals</strong>. Today, this same principle applies to LLM-powered agents, but with digital sensors and actuators:
            </p>
            <ul>
                <li><b>Sensors</b> → Text inputs, API responses, database queries, and file contents</li>
                <li><b>Processing</b> → LLM reasoning combined with memory and planning systems</li>
                <li><b>Actuators</b> → Tool calls, API requests, text generation, and system commands</li>
            </ul>
            <div class="section-nav-btns">
                <button id="back-agent-intro" disabled>Back</button>
                <button id="next-agent-intro">Next</button>
            </div>
        </section>
        <section id="fundamentals" class="module-section">
            <h2>From LLMs to Agents: Why Go Further?</h2>
            <p>While LLMs are incredibly versatile, many real-world applications require more than just language understanding. This is where LLM-powered agents come in.</p>
            <div class="callout-info" style="background:#f8fafd;border-left:4px solid #4a6fa5;margin:1.2em 0 0.5em 0;padding:1em 1.2em;"><span class="callout-icon">🤖</span>
                <strong>Agentic LLM application</strong> is a software system that wraps around the LLM, operating in a loop—observing its environment, using the LLM's reasoning to decide what to do next, and taking actions to achieve its goals.
            </div>
            <p>LLM-powered agents build upon the foundation of Large Language Models by extending them with critical capabilities:            </p>
            <ul class="enhanced-list">
                <li><strong>Tool Use:</strong> While base LLMs can only process and generate text, LLM-powered agents can interact with the world by using external tools, APIs, and services to retrieve information or perform actions.</li>
                <li><strong>Persistent Memory:</strong> Unlike base LLMs limited to their context window, agents remember past actions, user preferences, or important facts (short-term and long-term). They can also use it to improve future actions.</li>
                <li><strong>Orchestration Logic:</strong> Coordinates when and how to use the LLM, tools, and memory within each decision cycle, enabling adaptive, multi-step workflows.</li>
            </ul>
            <div class="diagram">
                <img src="../assets/images/agent_component.png" alt="Diagram showing an agent at the center, connected to tools, memory, and a decision cycle, all triggered by a user request or task." style="max-width: 420px; width: 100%; height: auto; display: block; margin: 0 auto;" />
                <div style="font-size:0.98em; color:#444; margin-top:0.5em;">Figure: Core components of an LLM-powered agent. The agent orchestrates tool use, memory, and a decision cycle in response to user requests or tasks.</div>
            </div>
            These three capabilities transform LLMs from reactive language models into semi-autonomous systems that can reason, remember, and act to accomplish complex real-world tasks.            
            <div class="callout-info" style="background:#fffbe6;border-left:4px solid #ffd700;margin:1em 0 1.2em 0;padding:1em 1.2em;">
              <span style="font-size:1.2em;margin-right:0.3em;">⚡</span>
              For the remainder of this module, <b>"agents"</b> refers to <b>Agentic LLM applications</b>. The key differentiator from <b>Single-Step or Workflow Based LLM Applications</b> is the use of LLM for orchestration, and optionally, persistent memory for feedback loop and learning. These require careful engineering and enable agents to adapt to handle complex tasks flexibly at the cost of increased implementation complexity.
            </div>
            <h3>The Engineering Challenge: From Prompts to Orchestration</h3>
            <p>While prompt engineering remains important, building LLM-powered agents shifts the primary engineering focus to orchestration design:</p>
            <ul>
                <li><b>Decision Logic:</b> When should the agent call external tools versus generate responses using the LLM's training knowledge? How does it choose between multiple available tools for the same task?</li>
                <li><b>Error Handling:</b> What happens when a tool call fails, returns incomplete data, or produces unexpected results? How should the agent recover and continue?</li>
                <li><b>Memory Operations:</b> What information should be stored after each interaction? When should past information be retrieved and used? How should conflicting information be handled?</li>
                <li><b>Loop Termination:</b> How does the agent determine when a task is complete versus when to continue the decision cycle? What prevents infinite loops?</li>
            </ul>  
            <p>This orchestration logic varies dramatically by use case - a research agent needs different decision patterns than a customer service agent or a data analysis agent. The engineering complexity lies in designing these control flows rather than just crafting prompts.</p>
            <div class='section-nav-btns'>
                <button id='back-fundamentals' disabled>Back</button>
                <button id='next-fundamentals'>Next</button>
            </div>
        </section>
    <section id="when-to-use-agents" class="module-section">
            <h2>Agentic LLM Applications: When Are They Needed?</h2>
            <p>Not every application needs the complexity of an agent. Many tasks—like summarization, classification, or Q&A—can be solved with just prompt engineering and an LLM.</p>
            <p>However, agents become essential when achieving your goals requires handling multi-step complex tasks and the workflow cannot be fully specified in advance—demanding adaptive, dynamic decision-making.</p>
            <div class="callout" style="background: #fffbe6; border-left: 4px solid #ffd700; margin: 1em 0 1.2em 0; padding: 0.5em 1em;">
              <strong>Guiding Principles for Using Agents:</strong>
              <ul style="margin-top: 0.5em;">
                <li><b>Don't Build Agents for Everything:</b> Use agents only for complex, ambiguous, high-value tasks; prefer Single-Step or Workflow-Based LLM Applications for simple cases.</li>
                <li><b>Keep It Simple:</b> Start with a minimal architecture (environment, tools, prompt); iterate before adding complexity.</li>
              </ul>
            </div>
            <p>The following table compares traditional workflows, single-step or workflow-based LLM applications, and agentic LLM applications to help clarify when each approach is most appropriate.</p>
            <table class="enhanced-table">
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th>Traditional Workflows</th>
                        <th>Single-Step or Workflow Based LLM Applications</th>
                        <th>Agentic LLM Applications</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="dimension">Visual</td>
                        <td style="text-align:center;vertical-align:middle;">
                            <img src="../assets/images/traditional_workflow.png" alt="Traditional Workflow" style="width:100%;max-width:400px;height:auto;display:block;margin:0 auto;" />
                        </td>
                        <td style="text-align:center;vertical-align:middle;">
                            <div style="display:flex;flex-direction:column;align-items:center;gap:18px;">
                                <img src="../assets/images/llm-basic.png" alt="LLM Basic" style="width:100%;max-width:340px;height:auto;" />
                                <img src="../assets/images/llm-knowledge.png" alt="LLM Knowledge" style="width:100%;max-width:340px;height:auto;" />
                            </div>
                        </td>
                        <td style="text-align:center;vertical-align:middle;">
                            <img src="../assets/images/agent-cycle.png" alt="Agent Cycle" style="width:100%;max-width:400px;height:auto;display:block;margin:0 auto;" />
                        </td>
                    </tr>
                    <tr>
                        <td class="dimension">Description</td>
                        <td>Software systems with predefined logic and workflows</td>
                        <td>Applications that use LLMs in one or more fixed, code-defined steps—each step may involve an LLM call or tool, but the workflow is predetermined and not dynamically chosen by the LLM.</td>
                        <td>Systems operating in a loop—observing its environment, using the LLM's reasoning to decide what to do next, and taking actions to achieve its goals</td>
                    </tr>
                    <tr>
                        <td class="dimension">Implementation Complexity</td>
                        <td>Medium-High (requires specific logic for each task)</td>
                        <td>Low-Medium (prompt engineering plus predefined integrations)</td>
                        <td>High (requires orchestration, tool integration, memory systems)</td>
                    </tr>
                    <tr>
                        <td class="dimension">Applications & Examples</td>
                        <td>Well-defined processes: order processing, data validation, reporting</td>
                        <td>Tasks solved by running the LLM in one or more fixed, code-defined steps—such as content creation, simple chatbots, text-to-SQL, or multi-step data processing—where the workflow and tool use are predetermined and not dynamically chosen by the LLM.</td>
                        <td>Complex tasks requiring multiple steps reasoning, external data, or persistent context. Customer service agents, research assistants, automated analysts</td>
                    </tr>
                    <tr>
                        <td class="dimension">Autonomy</td>
                        <td>Limited within predefined rules and predetermined logic</td>
                        <td>Limited within LLM reasoning and predefined tool usage patterns</td>
                        <td>Semi-autonomous with LLM orchestrating tool selection and execution based on goals and context</td>
                    </tr>
                    <tr>
                        <td class="dimension">Reactivity</td>
                        <td>Responds to specific triggers and data changes</td>
                        <td>Responds to user prompts with enhanced context</td>
                        <td>Responds to environmental changes and adapts strategy accordingly</td>
                    </tr>
                    <tr>
                        <td class="dimension">Pro-activeness</td>
                        <td>Follows predetermined paths without initiative</td>
                        <td>Reactive within single interactions, no cross-session initiative</td>
                        <td>Takes initiative to pursue goals across multiple steps and sessions</td>
                    </tr>
                    <tr>
                        <td class="dimension">Social Ability</td>
                        <td>Structured interactions with predefined interfaces</td>
                        <td>Natural language conversation with enhanced responses</td>
                        <td>Multi-turn dialogue with context awareness and goal persistence</td>
                    </tr>
                    <tr>
                        <td class="dimension">Tool Integration</td>
                        <td>Pre-programmed connections to specific systems</td>
                        <td>Predefined tool usage (RAG integrations, LLM output as tool input)</td>
                        <td>LLM decides which tools to use; orchestrated tool selection with feedback loops</td>
                    </tr>
                    <tr>
                        <td class="dimension">Memory Management</td>
                        <td>Database-driven with explicit schema design</td>
                        <td>Context window concatenation (limited to context window size)</td>
                        <td>Persistent across sessions with both short and long-term storage</td>
                    </tr>
                    <tr>
                        <td class="dimension">Reasoning Process</td>
                        <td>Linear, rule-based or algorithmic</td>
                        <td>Single-step or multi-step reasoning per interaction (may use CoT, ToT, ReAct within prompts)</td>
                        <td>Multi-step reasoning across interactions with planning and feedback loops</td>
                    </tr>
                </tbody>
            </table>
            <h3>Example: Document Extraction</h3>
            <ul>
                <li><strong>Traditional Workflow:</strong> Extracts fixed fields from one type of document that always follows the same structure (e.g., always pulls "Name" and "Date" from a standard lease form).</li>
                <li><strong>Single-Step or Workflow-Based LLM Applications:</strong> Can flexibly extract different fields based on the prompt, but still processes one document at a time and does not adapt its process or use external tools.</li>
                <li><strong>Agentic Application:</strong> Can interact with tools to translate documents, convert between different document types, and extract relevant fields—even adapting its approach based on the document's structure or missing information.</li>
            </ul>
            <div class="code-examples-row">
                <div class="code-example-col">
                    <div class="code-example-header">Single-Step LLM Application</div>
                    <div class="code-summary">
                        This code sends a prompt to a language model to extract specific fields from a lease document in a single step. It highlights how prompt engineering alone enables flexible information extraction without any additional logic or memory.
                    </div>
                    <pre><code class="language-python">import boto3
import json

# Set up Bedrock client
bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
<span class="code-highlight">
def extract_fields(document):
    prompt = (
        "Extract the following fields from this lease document: Tenant Name, Lease Start Date, Rent Amount.\n\n"
        f"Document:\n{document}\n\nFields:"
    )
    body = json.dumps({
        "prompt": prompt,
        "max_tokens_to_sample": 200,
        "temperature": 0
    })
    response = bedrock.invoke_model(
        modelId="anthropic.claude-3-sonnet-20240229-v1",
        body=body
    )
    result = json.loads(response["body"].read())
    return result["completion"].strip()
</span>
# Example document
doc = "This lease is made between John Doe and ACME Corp. Lease starts on 2024-07-01. Monthly rent is $2,500."

# Run the extraction
result = extract_fields(doc)
print(result)
</code></pre>
                </div>
                <div class="code-example-col">
                    <div class="code-example-header">Agentic Application</div>
                    <div class="code-summary">
                        This code first checks if a lease document is in English or Spanish, then uses a single prompt to instruct the language model to translate to English if needed and extract key fields. It illustrates how an agent can handle multilingual input and autonomously solve a multi-step task by leveraging LLM reasoning and prompt design.
                    </div>
                    <pre><code class="language-python">import boto3
import json

# Define your tools
def translate_to_english(text):
# Dummy translation for demo; in real use, call an API or LLM
if "Este contrato" in text:
    return "This lease is made between John Doe and ACME Corp. Lease starts on 2024-07-01. Monthly rent is $2,500."
return text

def extract_fields(text):
# Dummy extraction for demo; in real use, call an LLM
if "John Doe" in text:
    return "Tenant: John Doe, Start Date: 2024-07-01, Rent: $2,500"
return "Fields not found"

<span class="code-highlight"># Build prompt for Claude
# Tool registry
TOOLS = {
"translate_to_english": translate_to_english,
"extract_fields": extract_fields,
}</span>

# Bedrock client
bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")

def call_claude(prompt):
body = json.dumps({
    "prompt": prompt,
    "max_tokens_to_sample": 200,
    "temperature": 0
})
response = bedrock.invoke_model(
    modelId="anthropic.claude-3-sonnet-20240229-v1",
    body=body
)
result = json.loads(response["body"].read())
return result['content'][0]['text'].strip()

<span class="code-highlight">
def agent_decision_loop(document):
    history = []
    while True:
        #Build prompt for Claude
        prompt = (
            "Your goal: Extract the tenant name, lease start date, and rent amount from the provided lease document. "
            "If the document is not in English, translate it to English first.\n\n"
            "You are an agent that can use the following tools:\n"
            "- translate_to_english(text): Translates text to English if needed.\n"
            "- extract_fields(text): Extracts tenant name, lease start date, and rent amount from an English lease document.\n\n"
            f"Document: {document}\n"
            f"History: {history}\n"
            "What should you do next? Reply with:\n"
            "Action: '<'tool_name'>'\n"
            "Action Input: '<'input'>'\n"
            "or\n"
            "Final Answer: <your answer>\n"
        )
        output = call_claude(prompt)
        print("Claude Output:", output)
        if output.startswith("Final Answer:"):
            return output[len("Final Answer:"):].strip()
        elif output.startswith("Action:"):
            lines = output.splitlines()
            action = lines[0].split(":", 1)[1].strip()
            action_input = lines[1].split(":", 1)[1].strip()
            result = TOOLS[action](action_input)
            history.append({"action": action, "input": action_input, "result": result})
            document = result  # For this simple example, update document for next step
        else:
            return "Agent did not understand what to do."
</span>

# Example usage
spanish_doc = "Este contrato de arrendamiento es entre John Doe y ACME Corp. Comienza el 1 de julio de 2024. La renta mensual es de $2,500."
print(agent_decision_loop(spanish_doc))
</code></pre>
                </div>
            </div>
            <div class='section-nav-btns'>
                <button id='back-when-to-use-agents' disabled>Back</button>
                <button id='next-when-to-use-agents'>Next</button>
            </div>
        </section>
        <section id="memory" class="module-section" style="display:none">
            <h2>Memory: Retaining and Utilizing Information</h2>
            <h3>What is Memory in AI Agents?</h3>
            <p>Memory enables an agent to remember, reason, and act based on past interactions, knowledge, and goals. For chatbots and digital agents, memory is essential for holding context, learning from conversations, and improving over time.</p>
            <p><strong>Analogy:</strong><br>Just as people remember recent conversations, facts, and how to perform tasks, agents use different types of memory to be helpful and context-aware.</p>
            <hr>
            <h3>Memory Types in Language Agents</h3>
            <h4>1. Working Memory:  What the agent is thinking about right now</h4>
            <p><strong>Definition:</strong><br>Working memory is the agent's "active desk"—it holds all the information the agent needs right now to make decisions and respond. This includes:</p>
            <ul>
                <li>The current user message and recent conversation history</li>
                <li>Any goals or tasks the agent is working on</li>
                <li>Facts or context retrieved from long-term memory for the current turn</li>
            </ul>
            <p><strong>Key Points:</strong></p>
            <ul>
                <li>Working memory is refreshed every decision cycle (e.g., each time the agent responds)</li>
                <li>It is the main input to the LLM for generating a response</li>
                <li>After the LLM responds, new information (actions, decisions, updated goals) is stored back in working memory for the next cycle</li>
            </ul>
            <p><strong>Analogy:</strong><br>Like having all the notes and materials you need on your desk while working on a homework assignment—everything you need right now is in front of you and easy to use.</p>
            <hr>
            <h4>2. Long-Term Memory: What the agent has experienced before and knows as facts</h4>
            <p>Long-term memory is where the agent stores information it may need in the future, even after the current conversation or task is over. It has two main types:</p>
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>What it Stores</th>
                        <th>Example in Chatbots/Agents</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td><strong>Episodic</strong></td><td>Recall what happened in previous chats or tasks</td><td>Specific experiences and events</td><td>Past conversations, user preferences, previous actions taken</td></tr>
                    <tr><td><strong>Semantic</strong></td><td>Lookup facts or knowledge to answer questions or make decisions</td><td>General knowledge and facts</td><td>Company policies, product info, FAQs, world knowledge</td></tr>
                </tbody>
            </table>
            <div class="callout-info" style="background:#f8fafd;border-left:4px solid #4a6fa5;margin:1em 0;padding:1em 1.2em;">
                <b>Tip:</b> Vector databases—such as <b>Pinecone</b>, <b>FAISS</b>, <b>Amazon Kendra</b> and <b>PostgreSQL with pgvector</b>—are commonly used to implement long-term or semantic memory in modern AI agents, enabling fast retrieval of relevant information based on meaning. For more on choosing a vector database for AI use cases, see the <a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/choosing-an-aws-vector-database-for-rag-use-cases/introduction.html" target="_blank">AWS Prescriptive Guidance on vector databases</a>.
            </div>
            <p><strong>Analogy:</strong><br>Episodic memory is like your chat history or diary; semantic memory is like your personal wiki or knowledge base.</p>
            <hr>
            <h4>3. Procedural Memory: How the agent knows what to do and how to do it</h4>
            <p>Procedural memory is how the agent knows what to do and how to do it.</p>
            <ul>
                <li><strong>Implicit procedural memory:</strong> The skills and reasoning built into the LLM itself, encoded in the model's weights.</li>
                <li><strong>Explicit procedural memory:</strong> The agent's code, prompt templates, and programmed workflows (e.g., how to escalate a support ticket, how to call an API).</li>
            </ul>
            <p><strong>Key Points:</strong></p>
            <ul>
                <li>Procedural memory is set up by the agent designer (the developer).</li>
                <li>It can be updated, but changes must be made carefully to avoid bugs or unintended behavior.</li>
            </ul>
            <p><strong>Analogy:</strong><br>Implicit is like knowing how to ride a bike; explicit is like following a recipe or checklist.</p>
            <hr>
            <h3>How These Memories Work Together</h3>
            <ul>
                <li><strong>Working memory</strong> is the "hub" for each decision: it brings in the current message, retrieves relevant info from long-term memory, and uses procedural memory to decide what to do.</li>
                <li><strong>Episodic and semantic memory</strong> are "archives" the agent can search for relevant past events or facts.</li>
                <li><strong>Procedural memory</strong> is the "how-to manual" and skillset the agent uses to act.</li>
            </ul>
            <hr>
            <h4>Memory Architecture Visualization</h4>
            <img src="../assets/images/agent-architecture.png" alt="Cognitive Agent Architecture" style="max-width:100%;margin:20px 0;">
            <p><em>This diagram shows how working memory, long-term memory (episodic and semantic), and procedural memory interact in a language agent. Working memory is the central workspace, connecting the agent's reasoning, actions, and memory systems.</em></p>
            <p><em>Adapted from the CoALA framework. For more, see <a href="https://arxiv.org/pdf/2309.02427" target="_blank">Cognitive Architectures for Language Agents</a>.</em></p>
            <hr>
            <h3>Practical Example (Chatbot Context)</h3>
            <p><strong>User:</strong> "Last time I chatted, you gave me a troubleshooting tip. What was it?"</p>
            <ul>
                <li><strong>Agent's working memory:</strong> Holds the current question and user ID.</li>
                <li><strong>Agent's episodic memory:</strong> Retrieves the specific advice or troubleshooting tip given in the previous conversation with this user.</li>
                <li><strong>Agent's semantic memory:</strong> Knows general troubleshooting procedures and device information.</li>
                <li><strong>Agent's procedural memory:</strong> Uses a programmed workflow to guide the user through troubleshooting steps.</li>
            </ul>
            <p><strong>Memory Type Breakdown:</strong></p>
            <ul>
                <li><strong>Episodic memory:</strong> "In your last chat, I suggested you restart your router."</li>
                <li><strong>Semantic memory:</strong> "Restarting the router is a common fix for connectivity issues."</li>
                <li><strong>Procedural memory:</strong> The step-by-step process the agent uses to walk the user through restarting the router.</li>
            </ul>
            <hr>
            <div class='section-nav-btns'>
                <button id='back-memory'>Back</button>
                <button id='next-memory'>Next</button>
            </div>
        </section>
        <section id="tools" class="module-section" style="display:none">
            <h2>Tools: Extending the Agent's Capabilities</h2>
            <h3>2.1 Tools: Extending the Agent's Capabilities</h3>
            <h4>What Are Tools in the Context of AI Agents?</h4>
            <p>Tools are specialized functions that enable AI agents to perform specific tasks beyond text generation, connecting them to external systems and capabilities. They serve as the interface between an agent's decision-making capabilities and the real world.</p>
            <p><strong>Key Analogy:</strong><br>An LLM is like a brain, and tools are its limbs and senses - they allow the agent to interact with and perceive the world around it.</p>
            <h4>Why Tools Are Essential for Agent Capabilities</h4>
            <p>LLMs have four key limitations that tools help overcome:</p>
            <ol>
                <li><strong>Knowledge Cutoff:</strong> LLMs only know information they were trained on</li>
                <li><strong>Data Manipulation:</strong> LLMs struggle with complex calculations</li>
                <li><strong>External Interaction:</strong> LLMs can't access current information or systems</li>
                <li><strong>Verification:</strong> LLMs can't verify outputs against real-world data</li>
            </ol>
            <p>Tools transform a passive text generator into an active agent by providing:</p>
            <ul>
                <li>Real-time information access</li>
                <li>Computational capabilities</li>
                <li>External system integration</li>
                <li>Output verification mechanisms</li>
            </ul>
            <h3>2.2 Types of External Environment Interactions</h3>
            <table>
                <thead>
                    <tr>
                        <th>Interaction Pattern</th>
                        <th>Description</th>
                        <th>When to Use</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Direct Function</td><td>Agent executes local functions</td><td>Simple operations with no external dependencies</td><td>Calculator, text formatting, local data processing</td></tr>
                    <tr><td>External</td><td>Agent connects to APIs or triggers workflows</td><td>Real-time data, integrations, or external actions</td><td>MCP Servers, Weather API, Slack Webhooks</td></tr>
                    <tr><td>Database Retrieval</td><td>Agent queries databases for information</td><td>Working with persistent structured data</td><td>Customer records, product catalogs, transaction history</td></tr>
                    <tr><td>Code Execution</td><td>Agent generates and runs code</td><td>Complex computational tasks requiring flexibility</td><td>Data analysis, visualization generation, algorithm implementation</td></tr>
                    <tr><td>Human Interaction</td><td>Agent collaborates or escalates to a human</td><td>Tasks requiring judgment, approval, or clarification</td><td>Escalating support tickets, requesting user input, human-in-the-loop review</td></tr>
                </tbody>
            </table>
            <h3>2.3 Key Principles for Building Agent Tools</h3>
            <p>Building effective tools for AI agents requires careful consideration of how agents interact with and understand tools. Here are five key principles:</p>
            <h4>1. Speak the Agent's Language</h4>
            <p>Design your tool description in clear natural language that helps the agent understand exactly when and how to use it.</p>
            <ul>
                <li>❌ "API for meteorological data retrieval"</li>
                <li>✅ "Get current weather conditions for any location by city name or zip code"</li>
            </ul>
            <h4>2. Right-Size Your Tools</h4>
            <p>Create tools that do one job well, not too granular (requiring too many calls) or too broad (causing confusion about purpose).</p>
            <ul>
                <li>❌ Generic "DatabaseTool"</li>
                <li>✅ Specific tools like "CustomerLookup" and "OrderHistory" with clear, distinct purposes</li>
            </ul>
            <h4>3. Structure for Success</h4>
            <p>Design inputs and outputs to make the agent's job easier, with intuitive parameter names and results formatted for easy reasoning.</p>
            <ul>
                <li>❌ Generic parameters like "input1" and "input2"</li>
                <li>✅ Descriptive parameters like "sourceText" and "targetLanguage"</li>
            </ul>
            <h4>4. Fail Informatively</h4>
            <p>Return helpful error messages that guide the agent toward correction rather than confusion.</p>
            <ul>
                <li>❌ "Error 404"</li>
                <li>✅ "Location 'Atlantis' not found. Please provide a valid city name or zip code"</li>
            </ul>
            <h4>5. Prevent Hallucinations</h4>
            <p>Provide factual, verifiable outputs that reduce the likelihood of the agent making things up.</p>
            <ul>
                <li>❌ Empty results that might lead to invented details</li>
                <li>✅ "No information available about product XYZ-123"</li>
            </ul>
            <div class='section-nav-btns'>
                <button id='back-tools'>Back</button>
                <button id='next-tools'>Next</button>
            </div>
        </section>
        <section id="decision-cycle" class="module-section" style="display:none">
            <h2>Decision Cycle: Observe, Plan, and Act</h2>
            <p>In the context of AI agents for digital applications (like chatbots, virtual assistants, or workflow automation), the <strong>decision cycle</strong> is the repeating process an agent uses to understand, reason, and act—much like how a human knowledge worker would handle a task.</p>
            <h3>What is the Decision Cycle?</h3>
            <div style="display: flex; justify-content: center;">
            <svg width="500" height="320" viewBox="0 0 500 320" xmlns="http://www.w3.org/2000/svg">
              <!-- Arrows -->
              <defs>
                <marker id="arrow" markerWidth="10" markerHeight="10" refX="7" refY="3" orient="auto" markerUnits="strokeWidth">
                  <path d="M0,0 L0,6 L9,3 z" fill="#888"/>
                </marker>
              </defs>
              <!-- Circles -->
              <circle cx="250" cy="60" r="45" fill="#e9f0f9" stroke="#4a6fa5" stroke-width="2"/>
              <circle cx="410" cy="220" r="45" fill="#f9e9f0" stroke="#a54c6b" stroke-width="2"/>
              <circle cx="90" cy="220" r="45" fill="#e9f9f0" stroke="#4ca56b" stroke-width="2"/>
              <!-- Labels -->
              <text x="250" y="65" text-anchor="middle" fill="#333" font-size="18" font-weight="bold">Observe</text>
              <text x="410" y="225" text-anchor="middle" fill="#333" font-size="18" font-weight="bold">Plan</text>
              <text x="90" y="225" text-anchor="middle" fill="#333" font-size="18" font-weight="bold">Act</text>
              <!-- Arrows -->
              <path d="M290,80 Q375,120 400,175" fill="none" stroke="#888" stroke-width="2" marker-end="url(#arrow)"/>
              <path d="M380,255 Q250,320 135,255" fill="none" stroke="#888" stroke-width="2" marker-end="url(#arrow)"/>
              <path d="M90,175 Q115,120 200,80" fill="none" stroke="#888" stroke-width="2" marker-end="url(#arrow)"/>
              <!-- Descriptions (moved for clarity) -->
              <text x="390" y="120" fill="#666" font-size="13" text-anchor="start">Interpret &amp; Plan</text>
              <text x="250" y="305" fill="#666" font-size="13" text-anchor="middle">Execute</text>
              <text x="40" y="120" fill="#666" font-size="13" text-anchor="start">Assess Results</text>
            </svg>
            </div>
            <p><em>This diagram illustrates how the agent's memory, tools, and decision logic interact in a continuous decision cycle, enabling the agent to observe, plan, and act in digital environments.</em></p>
            <p>The decision cycle is a loop where the agent:</p>
            <ol>
                <li><strong>Observes</strong> the current situation (e.g., receives a user message or new data).</li>
                <li><strong>Plans</strong> what to do next by combining what it knows (memory), what it can do (tools), and the current goal.</li>
                <li><strong>Acts</strong> by generating a response, calling a tool, retrieving information, or escalating to a human if needed.</li>
            </ol>
            <p>After acting, the agent updates its memory and starts the cycle again for the next input or task. The agent's "brain" (the LLM and its code) brings together memory and tools to decide the best next step in each cycle.</p>
            <hr>
            <h3>Orchestration in the Agent Decision Cycle</h3>
            <p>
              In agentic LLM applications, orchestration is key: the agent coordinates memory, tool use, and LLM reasoning within each decision cycle. 
              The agent actively manages when to retrieve context, when to call tools, and when to leverage the LLM for reasoning or generation. 
              This orchestration enables adaptive, multi-step workflows and robust integration with external systems.
            </p>
            <hr>
            <h3>Building Agents: Do You Need a Library?</h3>
            <p>You don't strictly need a library to build an agent—at its core, an agent is a software system that manages memory, tool use, and decision logic around an LLM. However, building a robust agent from scratch can be complex and time-consuming.</p>
            <p><strong>Popular open-source agent frameworks include:</strong></p>
            <ul>
                <li><strong>LangChain</strong> (Python, JS): Modular framework for building agentic LLM applications with memory, tools, and workflows.</li>
                <li><strong>CrewAI</strong>: Focuses on multi-agent collaboration and workflow orchestration.</li>
                <li><strong>Autogen</strong> (Microsoft): For building multi-agent and tool-using systems.</li>
            </ul>
             <!-- Frameworks Note -->
             <div class="callout-info" style="background:#f8fafd;border-left:4px solid #4a6fa5;margin:1.2em 0 0.5em 0;padding:1em 1.2em;">
                <span style="font-size:1.1em;margin-right:0.3em;">🛠️</span>
                <b>Note:</b> These frameworks provide reusable components, integrations, and best practices that can greatly simplify the development effort needed to build safe production-grade agents. However, the decision to incorporate such frameworks should be carefully evaluated based on your specific use case, complexity, and production requirements.
            </div>
            <hr>
            <h3>Example (Customer Support Chatbot)</h3>
            <ol>
                <li><strong>Observe:</strong> The user asks, "What's my order status?"</li>
                <li><strong>Plan:</strong> The agent checks its memory for recent orders, decides it needs up-to-date info, and chooses to use an external tool (API) to fetch the order status.</li>
                <li><strong>Act:</strong> The agent retrieves the status and replies, "Your order is out for delivery and should arrive today."</li>
            </ol>
            <p>The agent then updates its memory with this interaction, ready for the next question.</p>
           <div class='section-nav-btns'>
                <button id='back-decision-cycle'>Back</button>
                <button id='next-decision-cycle'>Next</button>
            </div>    
        </section>
        
        <section id="agent-patterns" class="module-section">
            <h2>Agent Patterns</h2>
            <p>Agentic LLM applications can be implemented in various ways depending on the application needs. Here are some of the patterns you'll encounter:</p>
            <table class="enhanced-table" style="margin-bottom:2em;">
                <thead>
                    <tr>
                        <th>Pattern</th>
                        <th>Description</th>
                        <th>Best For</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>Conversational Agents</b></td>
                        <td>One agent handles multi-turn conversations with users</td>
                        <td>Customer service, personal assistants, Q&A systems</td>
                        <td>ChatGPT-style interfaces, support chatbots, coding assistants etc.</td>
                    </tr>
                    <tr>
                        <td><b>Task-Oriented Agents</b></td>
                        <td>Designed to complete specific workflows or objectives, including those requiring interaction with browsers, desktop applications, or system interfaces </td>
                        <td>Automated analysis, report generation, document handling, web automation, and more</td>
                        <td>Market research agent,  inventory analysis agent, web scraping agent, automated testing agent</td>
                    </tr>
                    <tr>
                        <td><b>Multi-Agent Systems</b></td>
                        <td>Multiple specialized agents collaborate on complex tasks</td>
                        <td>Complex workflows requiring different expertise areas</td>
                        <td>Research team (data gathering, analysis, reporting)</td>
                    </tr>
                    <tr>
                        <td><b>Human-in-the-Loop Systems</b></td>
                        <td>Require human approval for key decisions or actions</td>
                        <td>High-stakes decisions, regulated environments, building trust</td>
                        <td>Investment recommendations needing manager approval</td>
                    </tr>
                </tbody>
            </table>
            <h3>Production Considerations</h3>
            <p>Building agents for production environments requires careful attention to several critical areas:</p>
            <table class="enhanced-table">
                <thead>
                    <tr>
                        <th>Area</th>
                        <th>Key Practices/Considerations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>Reliability & Error Handling</b></td>
                        <td>Retry logic, graceful degradation, clear error messaging, fallback mechanisms</td>
                    </tr>
                    <tr>
                        <td><b>Output Consistency</b></td>
                        <td>Structured output (JSON/templates), temperature=0, human review, pin model versions, comprehensive testing</td>
                    </tr>
                    <tr>
                        <td><b>Cost & Performance</b></td>
                        <td>Monitor token usage, cost guardrails, optimize loops, caching, balance thoroughness and latency</td>
                    </tr>
                    <tr>
                        <td><b>Security & Access Control</b></td>
                        <td>Access controls, authentication, audit logging, guardrails, input validation/sanitization</td>
                    </tr>
                    <tr>
                        <td><b>Monitoring & Observability</b></td>
                        <td>Track decision paths, tool usage, failure rates, monitor for anomalies, maintain logs, collect metrics, alerts</td>
                    </tr>
                </tbody>
            </table>
            <hr/>
            <h3>Module 3 Summary</h3>
            <p>In this module, you learned how modern AI agents are designed to go beyond simple text generation. You explored:</p>
            <ul>
                <li>The fundamentals of what makes an AI agent, including the importance of memory, tools, and the decision cycle</li>
                <li>How agents use different types of memory (working, episodic, semantic, procedural) to remember, reason, and act</li>
                <li>The various ways agents interact with external environments using tools and integration patterns</li>
                <li>The decision cycle as the core loop that enables agents to observe, plan, act, and learn—mirroring the way human knowledge workers handle tasks</li>
                <li>The importance of separating the agent's orchestration logic from the LLM's language and reasoning capabilities, and how frameworks like LangChain, CrewAI, and others can help you build robust, production-ready agents</li>
            </ul>
            <p>By understanding these concepts, you're now equipped to design and build AI agents that can autonomously assist, augment, or automate knowledge work in digital applications.</p>
        
            <div class='section-nav-btns'>
                <button id='back-agent-patterns'>Back</button>
                <button id='next-agent-patterns'>Next</button>
            </div>
        </section>
        <!-- Resources Section -->
        <section id="resources" class="module-section">
        <h2>Resources</h2>
        <ul>
            <li><b>CoALA: Cognitive Architectures for Language Agents</b><br>
                <a href="https://arxiv.org/pdf/2309.02427" target="_blank">arXiv PDF</a> – A comprehensive survey of cognitive architectures for language agents, including memory, planning, and tool use.
            </li>
            <li><b>Amazon Bedrock Agents Documentation</b><br>
                <a href="https://aws.amazon.com/bedrock/agents/" target="_blank">AWS Bedrock Agents</a> – Official AWS documentation for building, orchestrating, and deploying agents with memory, tool use, and multi-agent collaboration.
            </li>
            <li><b>LangChain Documentation</b><br>
                <a href="https://python.langchain.com/docs/" target="_blank">LangChain Docs</a> – The most popular open-source framework for building agents with memory, tools, and workflows.
            </li>
            <li><b>CrewAI</b><br>
                <a href="https://github.com/joaomdmoura/crewAI" target="_blank">CrewAI GitHub</a> – Open-source framework for multi-agent collaboration and workflow orchestration.
            </li>
            <li><b>Microsoft AutoGen</b><br>
                <a href="https://github.com/microsoft/autogen" target="_blank">AutoGen GitHub</a> – Framework for building multi-agent and tool-using systems.
            </li>
            <li><b>LLM Orchestration: Strategies, Frameworks, and Best Practices</b><br>
                <a href="https://labelyourdata.com/articles/llm-orchestration" target="_blank">Label Your Data</a> – Overview of orchestration concepts, frameworks, and best practices for agentic systems.
            </li>
            <li><b>LLM Orchestration in the Real World: Best Practices</b><br>
                <a href="https://www.crossml.com/llm-orchestration-in-the-real-world/" target="_blank">CrossML Blog</a> – Practical strategies and production insights for orchestrating agents.
            </li>
            <li><b>IBM: LLM Agent Orchestration Guide</b><br>
                <a href="https://www.ibm.com/think/tutorials/llm-agent-orchestration-with-langchain-and-granite" target="_blank">IBM Think Tutorial</a> – Step-by-step guide to agent orchestration with modern frameworks.
            </li>
            <li><b>How We Build Effective Agents: Barry Zhang, Anthropic</b><br>
                <a href="https://www.youtube.com/watch?v=D7_ipDqhtwk&ab_channel=AIEngineer" target="_blank">YouTube Video</a> – Practical insights and strategies for building effective agentic LLM applications from an Anthropic engineer.
            </li>
        </ul>
        <div class='section-nav-btns'>
            <button id='back-resources'>Back</button>
            <button id='next-resources'>Next</button>
        </div>
        </section>
        <section id="quiz-section" class="module-section" style="display:none">
            <h2 class="quiz-title">Concept Check Questions</h2>
            <div class="quiz-container">
                <div class="quiz-question">Which memory type is responsible for remembering the user's last support ticket?</div>
                <ul class="quiz-options">
                    <li class="quiz-option" data-correct="true" onclick="selectQuizOption(this)">A) Episodic memory</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">B) Semantic memory</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">C) Procedural memory</li>
                </ul>
                <div class="quiz-feedback">
                    <strong>Explanation:</strong> Episodic memory stores specific experiences and events, such as previous support tickets.
                </div>
            </div>
            <div class="quiz-container">
                <div class="quiz-question">What is the primary difference between an LLM and an AI agent?</div>
                <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">a) LLMs are less advanced than agents</li>
                    <li class="quiz-option" data-correct="true" onclick="selectQuizOption(this)">b) Agents actively take actions and use tools to achieve goals</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">c) LLMs cannot understand human language</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">d) Agents do not use language models</li>
                </ul>
                <div class="quiz-feedback">
                    <strong>Explanation:</strong> b) Agents actively take actions and use tools to achieve goals.
                </div>
            </div>
            <div class="quiz-container">
                <div class="quiz-question">True or False: AI agents are always fully autonomous and require no human intervention.</div>
                <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">a) True</li>
                    <li class="quiz-option" data-correct="true" onclick="selectQuizOption(this)">b) False</li>
                </ul>
                <div class="quiz-feedback">
                    <strong>Explanation:</strong> b) False. Many agents operate with varying degrees of autonomy and may require human oversight or intervention at different points in their operation.
                </div>
            </div>
            <div class="quiz-container">
                <div class="quiz-question">Which of the following is the core capability that distinguishes agentic LLM applications from single-step or workflow-based LLM applications?</div>
                <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">A) Ability to use external tools</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">B) Persistent memory across interactions</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">C) Multi-step adaptable orchestration logic</li>
                    <li class="quiz-option" data-correct="true" onclick="selectQuizOption(this)">D) Generating images from text prompts</li>
                </ul>
                <div class="quiz-feedback">
                    <strong>Explanation:</strong> C) Multi-step adaptable orchestration logic is the core capability that distinguishes agentic LLM applications from single-step or workflow-based LLM applications.
                </div>
            </div>
            <div class="quiz-container">
                <div class="quiz-question">Which of the following best describes the agent decision cycle in a digital personal assistant?</div>
                <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">A) The agent only responds to user input without using memory or tools</li>
                    <li class="quiz-option" data-correct="true" onclick="selectQuizOption(this)">B) The agent observes, plans, acts, and updates its memory in a repeating loop</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">C) The agent always escalates to a human for every task</li>
                    <li class="quiz-option" data-correct="false" onclick="selectQuizOption(this)">D) The agent only uses pre-programmed responses</li>
                </ul>
                <div class="quiz-feedback">
                    <strong>Explanation:</strong> B) The agent observes, plans, acts, and updates its memory in a repeating loop.
                </div>
            </div>
            <div class='section-nav-btns'>
                <button id='back-quiz-section'>Back</button>
                <button id='next-quiz-section'>Next</button>
            </div>
        </section>
    </div>
    
    <script src="../assets/js/course-nav.js"></script>
    <script src="../assets/js/module-sidebar.js"></script>
    <script src="../assets/js/quiz.js"></script>
    <script src="../assets/js/module-nav-bar.js"></script>
</body>
</html> 