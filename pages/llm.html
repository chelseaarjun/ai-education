<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Concepts</title>
    <link rel="stylesheet" href="../assets/css/course-nav.css">
    <link rel="stylesheet" href="../assets/css/module-nav-bar.css">
    <link rel="stylesheet" href="../assets/css/module-sidebar.css">
    <link rel="stylesheet" href="../assets/css/code-examples.css">
    <link rel="stylesheet" href="../assets/css/tables.css">
    <link rel="stylesheet" href="../assets/css/course-index.css">
    <link rel="stylesheet" href="../assets/css/mobile-fixes.css">
    <link rel="stylesheet" href="../assets/css/introduction-fix.css">
    <link rel="stylesheet" href="../assets/css/quiz.css">
    <style>
        .diagram {
            display: block;
            margin: 25px auto;
            max-width: 100%;
            text-align: center;
            border: 1px solid var(--border);
            border-radius: 4px;
            padding: 10px;
            overflow: hidden;
        }

        .subtitle {
            font-size: 1.2rem;
            margin-top: 0.5rem;
            font-weight: 400;
            color: var(--secondary);
            line-height: 1.5;
        }

        /* Section content spacing */
        section > *:first-child {
            margin-top: 0;
        }

        section > p:first-of-type {
            margin-top: 0.5rem;
        }

        /* Lists */
        ul, ol {
            margin-top: 0.5rem;
            margin-bottom: 1rem;
            padding-left: 1.5rem;
        }

        li {
            margin-bottom: 0.5rem;
            line-height: var(--line-height-body);
        }

        /* Examples and components spacing */
        .example:first-child,
        .prompt-example:first-child,
        .response-example:first-child,
        .crisp-component:first-child,
        .interactive-demo:first-child {
            margin-top: 0.5rem;
        }

        /* Code Blocks */
        .code-block {
            background-color: var(--code-bg);
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
            font-family: var(--font-mono);
            font-size: 0.9rem;
            margin: 1rem 0;
            border-left: 4px solid var(--accent);
            white-space: pre-wrap;
            line-height: 1.5;
            box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.05);
        }
        
        /* Examples */
        .example {
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .example:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);
        }

        /* LLM-specific visualizations */
        .context-window {
            width: 100%;
            height: 300px;
            border: 3px solid var(--primary);
            border-radius: 10px;
            position: relative;
            background-color: #eef7ff;
            margin: 1.25rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }

        .prompt-box {
            width: 40%;
            height: 100px;
            background-color: var(--accent);
            border-radius: 8px;
            position: absolute;
            top: 30px;
            left: 30px;
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-weight: bold;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        .response-box {
            width: 70%;
            height: 150px;
            background-color: var(--success);
            border-radius: 8px;
            position: absolute;
            bottom: 30px;
            right: 30px;
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-weight: bold;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        .token-container {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin: 1.25rem 0;
            padding: 1rem;
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .token, .token-alt {
            padding: 0.5rem 0.75rem;
            color: white;
            border-radius: 4px;
            font-family: var(--font-mono);
            font-size: 0.9rem;
            box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }

        .token {
            background-color: var(--primary);
        }

        .token-alt {
            background-color: var(--secondary);
        }

        .token-estimation {
            width: 100%;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.25rem;
            margin: 1.25rem 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .estimation-row {
            display: flex;
            margin: 0.75rem 0;
            border-bottom: 1px solid var(--border);
            padding-bottom: 0.75rem;
        }

        .estimation-label {
            flex: 1;
            font-weight: 500;
            color: var(--primary);
        }

        .estimation-value {
            flex: 2;
        }

        .embedding-space {
            width: 100%;
            height: 300px;
            border: 1px solid var(--border);
            border-radius: 8px;
            position: relative;
            background-color: white;
            margin: 1.25rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .word-node {
            position: absolute;
            width: 80px;
            height: 30px;
            display: flex;
            justify-content: center;
            align-items: center;
            border-radius: 15px;
            background-color: var(--primary);
            color: white;
            font-weight: 500;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .word-connection {
            position: absolute;
            background-color: var(--border);
            transform-origin: 0 0;
            height: 2px;
        }

        .logit-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.25rem 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .logit-table th, .logit-table td {
            border: 1px solid var(--border);
            padding: 0.75rem;
            text-align: left;
        }

        .logit-table th {
            background-color: var(--bg-alt);
            font-weight: 500;
            color: var(--primary);
        }

        .logit-score {
            display: flex;
            align-items: center;
        }

        .score-bar {
            height: 20px;
            background-color: var(--primary);
            margin-left: 0.75rem;
            border-radius: 3px;
        }

        .output-comparison {
            display: flex;
            gap: 1.25rem;
            margin: 1.25rem 0;
        }

        .output-box {
            flex: 1;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.25rem;
            background: white;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .output-comparison {
                flex-direction: column;
            }

            .token-container {
                padding: 0.75rem;
            }

            .token, .token-alt {
                font-size: 0.8rem;
                padding: 0.4rem 0.6rem;
            }
        }
    </style>
</head>
<body>
    <nav class="module-nav">
        <button class="module-nav-btn" data-section="introduction">Introduction</button>
        <button class="module-nav-btn" data-section="context-window">Context Window</button>
        <button class="module-nav-btn" data-section="tokenization">Tokenization</button>
        <button class="module-nav-btn" data-section="embeddings">Embeddings</button>
        <button class="module-nav-btn" data-section="temperature">Logits</button>
        <button class="module-nav-btn" data-section="response-format">Response Format</button>
        <button class="module-nav-btn" data-section="model-selection">LLM Evolution</button>
        <button class="module-nav-btn" data-section="quiz-section">Quiz</button>
    </nav>
    <nav class="module-sidebar"></nav>
    <div class="content-inner">
        <section id="introduction">
            <div class="course-description">
                <h2 class="module-title">Module 1: Understanding Large Language Models</h2>
                <p>Large Language Models (LLMs) are sophisticated AI systems trained on vast amounts of text data that can understand, generate, and manipulate human language. These powerful tools form the foundation of modern AI applications like chatbots, content generators, and virtual assistants.</p>
                <h3 class="section-title">What You'll Learn</h3>
                <p>In this module, you'll master the following key areas:</p>
                <ul>
                    <li><b>Context Window:</b> How LLMs process and limit information</li>
                    <li><b>Tokenization:</b> How text is broken down for model processing</li>
                    <li><b>Embeddings:</b> How LLMs represent meaning and relationships</li>
                    <li><b>Logits & Temperature:</b> How LLMs make predictions and control creativity</li>
                    <li><b>Response Format:</b> How to structure and interpret model outputs</li>
                    <li><b>Model Evolution:</b> Advances in LLM architectures and capabilities</li>
                </ul>
            </div>
            <div class="section-nav-btns">
                <button id="back-introduction" disabled>Back</button>
                <button id="next-introduction">Next</button>
            </div>
        </section>
        <section id="context-window" class="module-section">
            <div class="concept-section">
                <h2>1. Context Window: The Model's Working Memory</h2>
                
                <p>
                    <span class="highlight">Concept:</span>
                    The <strong>context window</strong> is the model's "working memory"—the total number of <strong>tokens</strong> (chunks of text) it can consider at once. This includes both your input and the model's output. 
                    <br>
                    Modern AI models, called <strong>transformers</strong> (invented by Google), use an <strong>attention mechanism</strong> to focus on all tokens in this window at the same time—but nothing outside it.
                </p>

                <p>
                    <span class="highlight">Everyday Example:</span>
                    Imagine a whiteboard with limited space. You write your question (input tokens) and leave room for the model's answer (output tokens). If your question fills the board, there's less space for the answer. If you run out of space, the model stops writing—even mid-sentence.
                </p>

                <div class="visual-container">
                    <div class="context-window">
                        <div class="prompt-box">Your Prompt<br>(e.g., 3,000 tokens)</div>
                        <div class="response-box">Model's Response<br>(up to 5,000 tokens)</div>
                    </div>
                    <p style="text-align: center; margin-top: 10px;">
                        <strong>Context Window: 8,000 tokens total (input + output)</strong>
                    </p>
                </div>

                <p>
                    <span class="highlight">Why it matters:</span>
                    <ul>
                        <li><strong>Hard limit:</strong> <b>input tokens + output tokens ≤ max context window</b></li>
                        <li>If your input is large, you have less room for the model's answer.</li>
                        <li>If you hit the limit, the model will stop—sometimes in the middle of a sentence.</li>
                        <li>This applies to all transformer models: OpenAI's GPT-4o/o1, Anthropic's Claude 3.7, and Amazon's Nova Premier.</li>
                        <li><strong>Cost control:</strong> Although foundational models have a maximum context window, most APIs let you set a smaller limit (using parameters like <code>max_tokens</code>) if you want to control costs or keep responses shorter.</li>
                    </ul>
                </p>

                <div class="callout" style="background: #f8f9fa; border-left: 4px solid #3498db; padding: 12px 18px; margin: 18px 0;">
                    <strong>Note on Reasoning Models & Token Budgets:</strong>
                    Newer models (like OpenAI's o1, Anthropic's Claude 3.7, and Amazon's Nova Premier) support <strong>very large context windows</strong>—sometimes up to 1 million tokens! These models can "think" for longer and do multi-step reasoning, but every step and intermediate thought also uses up tokens. Many APIs let you control this with a <strong>budget_tokens</strong> or <strong>reasoning budget</strong> parameter, so you can balance depth of reasoning with cost and performance.
                </div>

                <span style='font-size:1.2em; margin-right:0.3em;'>💡</span><strong>Tip:</strong> Keep your prompts concise and leave enough space for the model's answer—especially for complex tasks that need extended reasoning.
            </div>
            <div class="section-nav-btns">
                <button id="back-context-window">Back</button>
                <button id="next-context-window">Next</button>
            </div>
        </section>
        <section id="tokenization" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>2. Tokenization: Breaking Down Text</h2>
                
                <p><span class="highlight">Concept:</span> <strong>Tokenization</strong> splits text into small pieces called <strong>tokens</strong> that the model can process within its context window.</p>
                
                <p><span class="highlight">Everyday Example:</span> Think of tokenization like cutting a pizza. The whole pizza is your full text, and the slices are your tokens.</p>
                
                <div class="visual-container">
                    <p>Input Text: "Machine learning is fascinating"</p>
                    
                    <p><strong>Model A tokenization:</strong></p>
                    <div class="token-container">
                        <div class="token">"Machine"</div>
                        <div class="token">" learning"</div>
                        <div class="token">" is"</div>
                        <div class="token">" fascinating"</div>
                    </div>
                    
                    <p><strong>Model B tokenization:</strong></p>
                    <div class="token-container">
                        <div class="token-alt">"Machine"</div>
                        <div class="token-alt">" learn"</div>
                        <div class="token-alt">"ing"</div>
                        <div class="token-alt">" is"</div>
                        <div class="token-alt">" fascin"</div>
                        <div class="token-alt">"ating"</div>
                    </div>
                </div>
                
                <p><span class="highlight">Practical Application:</span> 
                <strong>Quick Token & Cost Estimation for Developers:</strong><br>
                For English, <b>1 token ≈ 4 characters</b> (including spaces/punctuation) or <b>¾ of a word</b>.<br>
                <span style='font-size:0.95em;color:#555;'>Reference: <a href='https://platform.openai.com/tokenizer' target='_blank' rel='noopener'>OpenAI Tokenizer</a></span><br><br>
                <b>How to use:</b>
                <ul>
                  <li>Count words or characters in your input and expected output.</li>
                  <li>Estimate tokens (words × 1.33 or characters ÷ 4).</li>
                  <li>Add input and output tokens for total usage.</li>
                  <li>Check your provider's pricing—most charge <b>less for input tokens</b> and <b>more for output tokens</b>.</li>
                  <li>Multiply by the respective rates to estimate total cost.</li>
                </ul>
                <b>Example:</b> 375 words input ~ 500 tokens, 75 words output ~ 100 tokens ≈ 600 tokens in context window. If input is $0.01/1K tokens and output is $0.02/1K tokens, cost ≈ $0.005 (input) + $0.002 (output) = $0.007 per request.
                </p>
            </div>
            <div class="section-nav-btns">
                <button id="back-tokenization">Back</button>
                <button id="next-tokenization">Next</button>
            </div>
        </section>
        <section id="embeddings" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>3. Embeddings: Understanding Meaning</h2>
                
                <p><span class="highlight">Concept:</span> <strong>Embeddings</strong> are numerical representations of tokens that capture their <strong>meaning</strong> in a mathematical space.</p>
                
                <p><span class="highlight">Everyday Example:</span> Imagine a map where similar words are clustered together. "Happy" and "joyful" would be neighbors, while "happy" and "sad" would be far apart.</p>
                
                <div class="visual-container">
                    <div class="embedding-space" style="height: 350px; overflow: hidden; position: relative;">
                        <!-- Title labels for clusters -->
                        <div style="position: absolute; top: 10px; left: 200px; font-weight: bold; color: #3498db;">Positive Emotions</div>
                        <div style="position: absolute; top: 190px; left: 160px; font-weight: bold; color: #e74c3c;">Negative Emotions</div>
                        <div style="position: absolute; top: 10px; left: 530px; font-weight: bold; color: #2ecc71;">Animals</div>
                    
                        <!-- Positive emotion cluster -->
                        <div class="word-node" style="top: 50px; left: 150px; background-color: #3498db;">happy</div>
                        <div class="word-node" style="top: 80px; left: 220px; background-color: #3498db;">joyful</div>
                        <div class="word-node" style="top: 60px; left: 280px; background-color: #3498db;">pleased</div>
                        <div class="word-node" style="top: 110px; left: 190px; background-color: #3498db;">delighted</div>
                        
                        <!-- Negative emotion cluster -->
                        <div class="word-node" style="top: 230px; left: 150px; background-color: #e74c3c;">sad</div>
                        <div class="word-node" style="top: 250px; left: 220px; background-color: #e74c3c;">unhappy</div>
                        <div class="word-node" style="top: 270px; left: 170px; background-color: #e74c3c;">gloomy</div>
                        
                        <!-- Animal cluster -->
                        <div class="word-node" style="top: 60px; left: 500px; background-color: #2ecc71;">dog</div>
                        <div class="word-node" style="top: 90px; left: 540px; background-color: #2ecc71;">cat</div>
                        <div class="word-node" style="top: 70px; left: 580px; background-color: #2ecc71;">puppy</div>
                    </div>
                    <p style="text-align: center; font-style: italic;">Words with similar meanings cluster together in embedding space, while different concept groups remain separate</p>
                </div>
                
                <p><span class="highlight">Practical Application:</span> Embeddings allow models to understand <strong>semantic relationships</strong> and make <strong>connections between concepts</strong> that weren't explicitly mentioned.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-embeddings">Back</button>
                <button id="next-embeddings">Next</button>
            </div>
        </section>
        <section id="temperature" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>4. Logits: Making Predictions</h2>
                
                <p><span class="highlight">Concept:</span> <strong>Logits</strong> are raw numerical scores the model assigns to each possible next token before making its final selection. The text generation process has two phases: First, the model processes your prompt to calculate these logits, which are then converted to probabilities for each potential next word using the <strong>softmax function</strong> (<a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">Wikipedia</a>). This calculation phase is deterministic—identical inputs always produce the same probability distribution. Second, the model selects tokens from this distribution, either predictably (by always choosing the highest-probability token) or with controlled randomness to balance accuracy with creativity.</p>
                
                <p><span class="highlight">Everyday Example:</span> When completing "The capital of France is ____," a model assigns high scores to relevant answers like "Paris" and low scores to irrelevant options like "banana."</p>
                
                <div class="visual-container">
                    <p><strong>Input:</strong> "The capital of France is"</p>
                    
                    <div style="padding: 20px; background-color: #f8f9fa; border-radius: 8px; border: 1px solid #ddd; margin-bottom: 20px;">
                        <h3 style="margin-top: 0;">How Token Selection Works</h3>
                        
                        <table class="logit-table">
                            <tr>
                                <th>Rank (k)</th>
                                <th>Token</th>
                                <th>Raw Logit</th>
                                <th>Base Probability</th>
                            </tr>
                            <tr>
                                <td><strong>1</strong></td>
                                <td>"Paris"</td>
                                <td>8.2</td>
                                <td>80%</td>
                            </tr>
                            <tr>
                                <td><strong>2</strong></td>
                                <td>"Lyon"</td>
                                <td>4.6</td>
                                <td>10%</td>
                            </tr>
                            <tr>
                                <td><strong>3</strong></td>
                                <td>"Nice"</td>
                                <td>3.9</td>
                                <td>5%</td>
                            </tr>
                            <tr>
                                <td><strong>4</strong></td>
                                <td>"Marseille"</td>
                                <td>3.2</td>
                                <td>3%</td>
                            </tr>
                            <tr>
                                <td><strong>5</strong></td>
                                <td>"banana"</td>
                                <td>-5.0</td>
                                <td>0.1%</td>
                            </tr>
                            <tr>
                                <td><strong>6+</strong></td>
                                <td>Other tokens</td>
                                <td>varies</td>
                                <td>1.9%</td>
                            </tr>
                        </table>
                        
                        <div style="margin-top: 30px; display: flex; gap: 20px;">
                            <div style="flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 8px; background-color: #f1f9fe;">
                                <h4 style="margin-top: 0;">Temperature</h4>
                                <p><strong>Modifies the probability distribution itself. Lower temperatures make the model more deterministic, leading to predictable outputs, while higher temperatures introduce more randomness and creativity. The allowed range depends on the provider and model—check your API documentation.</strong></p>
                                <ul>
                                    <li><strong>Low (0.2):</strong> Makes likely tokens even more likely</li>
                                    <li><strong>High (1.0):</strong> Makes distribution more uniform</li>
                                </ul>
                                <div style="font-size: 0.9em; font-style: italic;">With temperature 0.2, "Paris" might be 95% likely</div>
                            </div>
                            
                            <div style="flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 8px; background-color: #f1f9fe;">
                                <h4 style="margin-top: 0;">topP (also called Nucleus Sampling)</h4>
                                <p><strong>Uses a cumulative probability distribution. Sorts all possible next tokens by their probability (from highest to lowest). Then selects the smallest set of tokens whose cumulative probability adds up to the value of topP (e.g., 0.9 means the top tokens that together make up 90% of the probability)</strong></p>
                                <ul>
                                    <li><strong>topP = 0.9:</strong> Only "Paris", "Lyon", "Nice" considered (95% cumulative)</li>
                                    <li><strong>topP = 0.8:</strong> Only "Paris" considered (80% cumulative)</li>
                                </ul>
                                <div style="font-size: 0.9em; font-style: italic;">It's more flexible than top-K because it dynamically adjusts the number of tokens based on their probabilities</div>
                            </div>
                            
                            <div style="flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 8px; background-color: #f1f9fe;">
                                <h4 style="margin-top: 0;">topK</h4>
                                <p><strong>Considers only K most likely tokens</strong></p>
                                <ul>
                                    <li><strong>topK = 3:</strong> Only "Paris", "Lyon", "Nice" considered</li>
                                    <li><strong>topK = 1:</strong> Only "Paris" considered</li>
                                </ul>
                                <div style="font-size: 0.9em; font-style: italic;">Fixed number regardless of probabilities</div>
                            </div>
                        </div>
                    </div>
                    
                    <div style="margin-top: 15px; background: #fffbe6; border-left: 4px solid #f1c40f; padding: 10px 16px; border-radius: 6px; font-size: 1em;">
                        <span style='font-size:1.2em; margin-right:0.3em;'>💡</span><strong>Tip:</strong> For most use cases, set <b>either</b> <code>temperature</code> <b>or</b> <code>topP</code>—not both. Controlling both can lead to unpredictable or unstable results, as both parameters affect randomness in different ways.
                    </div>
                    
                    <p><strong>Combined Effect:</strong> These parameters work together to control selection. Temperature modifies the distribution, then topP and topK filter which tokens can be selected from the modified distribution.</p>
                </div>
                
                <p><span class="highlight">Practical Application:</span> The <strong>temperature</strong>, <strong>topP</strong>, and <strong>topK</strong> parameters control creativity vs. predictability in responses. These parameters let you balance deterministic, factual outputs with more creative, varied responses.</p>

                <!-- Interactive Probability Distribution Graph -->
                <div id="temperature-graph-container" style="margin-top: 32px; padding: 20px; background: #f8f9fa; border-radius: 8px; border: 1px solid #ddd;">
                    <h4 style="margin-top: 0;">Interactive: See How Temperature Changes Probabilities</h4>
                    <p style="margin-bottom: 8px;">Adjust the temperature to see how the probability distribution changes for a generic set of logits.</p>
                    <label for="temperature-slider"><b>Temperature:</b> <span id="temperature-value">1.0</span></label>
                    <input type="range" id="temperature-slider" min="0.1" max="2.0" step="0.01" value="1.0" style="width: 220px; vertical-align: middle; margin-left: 10px;">
                    <canvas id="temperature-prob-chart" width="500" height="260" style="display: block; margin-top: 18px;"></canvas>
                    <div style="font-size: 0.95em; color: #555; margin-top: 8px;">This chart uses a generic set of logits: [2.0, 1.0, 0.5, 0.0, -1.0]. Probabilities are calculated using the softmax function after scaling by temperature.</div>
                </div>
                <!-- End Interactive Probability Distribution Graph -->
            </div>
            <div class="section-nav-btns">
                <button id="back-temperature">Back</button>
                <button id="next-temperature">Next</button>
            </div>
        </section>
        <section id="response-format" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>5. Response and Structured Output</h2>
                
                <p><span class="highlight">Concept:</span> Models can format outputs as either <strong>free-form text</strong> or <strong>structured data</strong> (JSON, XML, etc.).</p>
                
                <p><span class="highlight">Everyday Example:</span> Compare asking for weather information as a casual description versus a formatted weather report with specific fields.</p>
                
                <div class="visual-container">
                    <div class="output-comparison">
                        <div class="output-box">
                            <div class="output-header">Free-form Response</div>
                            <p>"It's sunny and 72°F with light winds from the west."</p>
                            <ul style="font-size: 0.9em; color: #555;">
                                <li>Easy for humans to read</li>
                                <li>Natural conversational style</li>
                                <li>Less predictable structure</li>
                            </ul>
                        </div>
                        
                        <div class="output-box">
                            <div class="output-header">Structured Output (JSON)</div>
                            <div class="json-box">
                                {<br>
                                &nbsp;&nbsp;"weather": {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;"temperature": "72°F",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;"condition": "sunny",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;"wind": {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"speed": "light",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"direction": "west"<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;}<br>
                                &nbsp;&nbsp;}<br>
                                }
                            </div>
                            <ul style="font-size: 0.9em; color: #555;">
                                <li>Machine-readable format</li>
                                <li>Consistent, predictable structure</li>
                                <li>Easy to process programmatically</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <p><span class="highlight">Practical Application:</span> <strong>Structured outputs</strong> are essential when the AI's response needs to be <strong>processed by other systems</strong> rather than read by humans.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-response-format">Back</button>
                <button id="next-response-format">Next</button>
            </div>
        </section>
        <section id="model-selection" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>6. LLM Evolution & Architectural Advances</h2>
                <h4>Early LLM Development (2017-2022)</h4>
                <p>The modern Large Language Model era began with the 2017 paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">"Attention Is All You Need,"</a> which introduced the Transformer architecture. This revolutionary approach replaced recurrent neural networks with three key innovations:</p>
                <ul>
                  <li><b>Self-attention mechanism</b>: Allowing models to connect related words regardless of distance</li>
                  <li><b>Parallel processing</b>: Enabling simultaneous rather than sequential computation</li>
                  <li><b>Flexible architecture</b>: Supporting various NLP tasks through encoder-decoder components</li>
                </ul>
                <p>Following this breakthrough, researchers discovered the "scaling law" phenomenon: model capabilities improve predictably when increasing parameters, training data, and computing power. This insight led to a rapid expansion in model size:</p>
                <table>
                  <thead><tr><th>Year</th><th>Model</th><th>Parameters</th><th>Key Advancement</th></tr></thead>
                  <tbody>
                    <tr><td>2018</td><td>BERT</td><td>340M</td><td>Bidirectional understanding</td></tr>
                    <tr><td>2020</td><td>GPT-3</td><td>175B</td><td>Few-shot learning capabilities</td></tr>
                    <tr><td>2022</td><td>PaLM</td><td>540B</td><td>Improved reasoning abilities</td></tr>
                  </tbody>
                </table>
                <p>The scaling era culminated with the release of ChatGPT on November 30, 2022, which brought LLMs into mainstream use through its user-friendly interface and impressive capabilities.</p>
                <h4>The Rise of Reasoning Models (2023-Present)</h4>
                <p>Around 2023, a new generation of models emerged with enhanced reasoning abilities, representing a significant leap beyond simple pattern recognition. To build these reasoning models, training approaches evolved from basic transformer architectures to include explicit reasoning demonstrations, self-critique methods, and human feedback on multi-step solutions.</p>
                <table>
                  <thead><tr><th>Aspect</th><th>Description</th></tr></thead>
                  <tbody>
                    <tr><td><b>Key Capabilities</b></td><td>&bull; <b>Structured problem-solving</b>: Breaking down complex tasks into clear, logical steps<br>&bull; <b>Self-consistency checking</b>: Detecting and correcting contradictions in their own reasoning<br>&bull; <b>Extended reasoning chains</b>: Following longer, more complex logical arguments</td></tr>
                    <tr><td><b>Current Limitations</b></td><td>&bull; <b>Complex multi-step reasoning</b>: Still struggle with novel mathematical proofs and multi-constraint optimization<br>&bull; <b>Specialized domain knowledge</b>: Difficulty with advanced legal reasoning or medical diagnosis<br>&bull; <b>Spatial reasoning</b>: Inconsistent performance on complex physical systems or 3D visualization problems</td></tr>
                    <tr><td><b>Notable Examples</b></td><td>&bull; <b>ChatGPT o1</b>: OpenAI's model with improved mathematical and logical reasoning<br>&bull; <b>Claude 3.7 Sonnet</b>: Anthropic's model with structured problem-solving capabilities<br>&bull; <b>DeepSeek-R1</b>: Notable for performance on academic reasoning benchmarks</td></tr>
                    <tr><td><b>Real-World Impact</b></td><td>Higher accuracy on complex tasks, fewer hallucinations, and more reliable performance&mdash;making reasoning models the foundation for building autonomous AI agents</td></tr>
                  </tbody>
                </table>
                <blockquote><b>Note:</b> When a model shows its reasoning, all reasoning steps count toward the context window limit and output token costs. Parameters like <b>max_tokens</b> and <b>budget_tokens</b> can control total output length and costs.</blockquote>
                <h4>Current Limitations of LLMs</h4>
                <p>Despite impressive advances, even today's most sophisticated models face significant challenges:</p>
                <table>
                  <thead><tr><th>Limitation Type</th><th>Description</th></tr></thead>
                  <tbody>
                    <tr><td><b>Hallucinations</b></td><td>Generate plausible but factually incorrect information; invent citations; blend facts with fiction</td></tr>
                    <tr><td><b>Knowledge Boundaries</b></td><td>Fixed knowledge cutoffs; limited context windows (8K-200K tokens); inability to verify information</td></tr>
                    <tr><td><b>Reasoning Limitations</b></td><td>Struggle with complex multi-step reasoning; limited mathematical capabilities; domain knowledge gaps</td></tr>
                  </tbody>
                </table>
                <ul>
                  <li><b>Struggle with complex multi-step reasoning</b> (e.g., solving novel mathematical proofs or multi-constraint optimization problems)</li>
                  <li><b>Difficulty with tasks requiring specialized domain knowledge</b> (e.g., advanced legal reasoning or medical diagnosis)</li>
                  <li><b>Inconsistent performance on spatial reasoning tasks</b> (e.g., complex physical systems or 3D visualization problems)</li>
                </ul>
                <h4>Future Research Directions</h4>
                <p>The field is rapidly evolving beyond current LLM limitations, with several promising research directions that could transform how we build AI applications:</p>
                <table>
                  <thead><tr><th>Research Area</th><th>Description</th><th>Potential Real-World Impact</th><th>Reference</th></tr></thead>
                  <tbody>
                    <tr><td><b>Neuro-Symbolic Integration</b></td><td>Combining traditional symbolic systems with neural networks</td><td>Could enhance reasoning capabilities while maintaining interpretability</td><td><a href="https://arxiv.org/abs/2501.05435" target="_blank">Neuro-Symbolic AI in 2024: A Systematic Review</a></td></tr>
                    <tr><td><b>JEPA (Joint Embedding Predictive Architecture)</b></td><td>Yann LeCun's approach focusing on predicting abstract representations rather than raw outputs</td><td>May enable more efficient learning with less data and better understanding of causality</td><td><a href="https://arxiv.org/abs/2403.00504" target="_blank">Learning and Leveraging World Models in Visual Representation Learning (2024)</a></td></tr>
                    <tr><td><b>World Models</b></td><td>Systems that build internal representations of physical environments to predict outcomes and plan actions</td><td>Could enable AI to better understand physical reality and spatial relationships for robotics and embodied AI</td><td><a href="https://www.constellationr.com/blog-news/insights/physical-ai-world-foundation-models-will-move-forefront" target="_blank">Nvidia's Cosmos World Foundation Models (2025)</a></td></tr>
                  </tbody>
                </table>
                <p>These research areas could address some of the current limitations of autonomous agents and reduce the engineering overhead for building systems that can work on complex tasks while interacting with both physical and digital worlds.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-model-selection">Back</button>
                <button id="next-model-selection">Next</button>
            </div>
        </section>
        <section id="quiz-section" class="module-section" style="display:none">
            <div class="quiz-section">
                <h2 class="quiz-title">Concept Check Questions</h2>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q1">
                        <p class="question-title">1. Context Window: If a model has a context window of 16,000 tokens, and your prompt uses 7,500 tokens, how many tokens remain available for the response?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) 7,500 tokens</li>
                            <li class="quiz-option" data-correct="true">B) 8,500 tokens</li>
                            <li class="quiz-option" data-correct="false">C) 16,000 tokens</li>
                            <li class="quiz-option" data-correct="false">D) 24,500 tokens</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) 8,500 tokens. The remaining space is calculated by subtracting the prompt size (7,500) from the total context window size (16,000).
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q2">
                        <p class="question-title">2. Tokenization: Which would likely use more tokens?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) Common English words in a short sentence</li>
                            <li class="quiz-option" data-correct="true">B) Technical jargon and rare terminology</li>
                            <li class="quiz-option" data-correct="false">C) Simple numbers (1, 2, 3)</li>
                            <li class="quiz-option" data-correct="false">D) All options use exactly the same number of tokens</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) Technical jargon and rare terminology. Uncommon words are often broken into multiple tokens, whereas common words are typically represented as single tokens.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q3">
                        <p class="question-title">3. Token Costs: A model charges $0.01 per 1K input tokens and $0.02 per 1K output tokens. What's the approximate cost of processing 10 documents (1,000 words each) with 200-word summaries?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) $0.10</li>
                            <li class="quiz-option" data-correct="false">B) $0.30</li>
                            <li class="quiz-option" data-correct="true">C) $1.65</li>
                            <li class="quiz-option" data-correct="false">D) $3.00</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> C) $1.65. Each 1,000-word document is approximately 1,300 tokens (input) and each 200-word summary is approximately 260 tokens (output). Total: 10 × (1,300 × $0.01/1K + 260 × $0.02/1K) = $0.13 + $0.052 = $0.182 per document × 10 documents ≈ $1.65.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q4">
                        <p class="question-title">4. Embeddings: What makes embeddings powerful for understanding language?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) They contain the dictionary definition of each word</li>
                            <li class="quiz-option" data-correct="true">B) They represent words as points in space where similar words are closer together</li>
                            <li class="quiz-option" data-correct="false">C) They store grammar rules for proper sentence construction</li>
                            <li class="quiz-option" data-correct="false">D) They directly translate between different languages</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) They represent words as points in space where similar words are closer together. This allows the model to understand relationships between concepts and generalize to new situations.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q5">
                        <p class="question-title">5. Logits: When would you use a high temperature setting?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="true">A) When generating creative stories or poetry</li>
                            <li class="quiz-option" data-correct="false">B) When performing factual question answering</li>
                            <li class="quiz-option" data-correct="false">C) When extracting structured data from text</li>
                            <li class="quiz-option" data-correct="false">D) When performing mathematical calculations</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> A) When generating creative stories or poetry. Higher temperature settings introduce more randomness, allowing for more creative and varied outputs.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q6">
                        <p class="question-title">6. Response and Structured Output: Which scenario would benefit most from a structured output format?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) A bedtime story for children</li>
                            <li class="quiz-option" data-correct="false">B) A personalized email response</li>
                            <li class="quiz-option" data-correct="true">C) Data extraction for a financial dashboard</li>
                            <li class="quiz-option" data-correct="false">D) A creative description of a landscape</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> C) Data extraction for a financial dashboard. Structured output formats like JSON allow other systems to easily process and display the information without needing to parse natural language.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q7">
                        <p class="question-title">7. Reasoning in Foundational Models: Which approach would likely yield the most accurate answer to a multi-step math problem?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) Asking for just the final answer</li>
                            <li class="quiz-option" data-correct="true">B) Requesting step-by-step reasoning</li>
                            <li class="quiz-option" data-correct="false">C) Using the highest temperature setting</li>
                            <li class="quiz-option" data-correct="true">D) Using the lowest temperature setting</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) Requesting step-by-step reasoning and D) Using the lowest temperature setting. Step-by-step reasoning allows the model to work through the problem methodically, catching errors in its reasoning process. A low temperature setting increases determinism and reduces creativity, which is beneficial for mathematical accuracy.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q8">
                        <p class="question-title">8. Claude 3.7 Sonnet Specifications: What task would specifically benefit from Claude 3.7 Sonnet's large context window?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="true">A) Analyzing an entire legal contract at once</li>
                            <li class="quiz-option" data-correct="false">B) Generating a single paragraph response</li>
                            <li class="quiz-option" data-correct="false">C) Converting a short text to JSON</li>
                            <li class="quiz-option" data-correct="false">D) Translating a single sentence</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> A) Analyzing an entire legal contract at once. Claude 3.7 Sonnet's 200,000 token context window allows it to process lengthy documents entirely, maintaining understanding of references and relationships throughout the text.
                        </div>
                    </div>
                </div>
                <div class="quiz-container">
                    <div class="quiz-question">
                        <p class="question-title">9. Why do LLMs sometimes hallucinate information, and what approaches can developers take to mitigate this problem? (Select all that apply)</p>
                        <ul class="quiz-options">
                        <li class="quiz-option" data-correct="false">A) LLMs have perfect knowledge but choose to be creative</li>
                        <li class="quiz-option" data-correct="true">B) The statistical nature of prediction sometimes generates plausible but incorrect information</li>
                        <li class="quiz-option" data-correct="true">C) Using retrieval augmentation to ground model responses in verified sources</li>
                        <li class="quiz-option" data-correct="false">D) Training models on larger datasets always eliminates hallucinations</li>
                        <li class="quiz-option" data-correct="true">E) Implementing fact-checking components that verify model outputs</li>
                        </ul>
                        <div class="quiz-feedback">
                        <strong>Answer:</strong> B, C, and E. Hallucinations occur due to the statistical nature of LLMs. Retrieval augmentation and fact-checking can help mitigate this issue.
                        </div>
                    </div>
                </div>
            </div>
            <div class="quiz-container">
                <div class="quiz-question">
                    <p class="question-title">10. Which training approach below is specifically designed to enhance an LLM's reasoning capabilities?</p>
                    <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false">A) Next-token prediction</li>
                    <li class="quiz-option" data-correct="true">B) Chain-of-thought training</li>
                    <li class="quiz-option" data-correct="false">C) Masked language modeling</li>
                    <li class="quiz-option" data-correct="false">D) Decoder-only architecture</li>
                    </ul>
                    <div class="quiz-feedback">
                    <strong>Answer:</strong> B) Chain-of-thought training. This approach teaches models to break down problems into logical steps, improving their reasoning capabilities.
                    </div>
                </div>
            </div>
            <div class="section-nav-btns">
                <button id="back-quiz-section">Back</button>
                <button id="next-quiz-section" disabled>Next</button>
            </div>
        </section>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="../assets/js/temperature-graph.js"></script>
    <script src="../assets/js/course-nav.js"></script>
    <script src="../assets/js/module-sidebar.js"></script>
    <script src="../assets/js/quiz.js"></script>
    <script src="../assets/js/module-nav-bar.js"></script>
</body>
</html>
