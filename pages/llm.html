<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Concepts</title>
    <link rel="stylesheet" href="../assets/css/course-nav.css">
    <link rel="stylesheet" href="../assets/css/module-nav-bar.css">
    <link rel="stylesheet" href="../assets/css/module-sidebar.css">
    <link rel="stylesheet" href="../assets/css/code-examples.css">
    <link rel="stylesheet" href="../assets/css/tables.css">
    <link rel="stylesheet" href="../assets/css/course-index.css">
    <link rel="stylesheet" href="../assets/css/mobile-fixes.css">
    <link rel="stylesheet" href="../assets/css/introduction-fix.css">
    <link rel="stylesheet" href="../assets/css/quiz.css">
    <style>
        .diagram {
            display: block;
            margin: 25px auto;
            max-width: 100%;
            text-align: center;
            border: 1px solid var(--border);
            border-radius: 4px;
            padding: 10px;
            overflow: hidden;
        }

        .course-description {
            background: linear-gradient(135deg, #eaf2f8 0%, #f0f7ff 100%);
            padding: 1.25rem;
            padding-top: 1rem;
            border-radius: 12px;
            margin-bottom: 1.5rem;
            margin-top: 0;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            border: 1px solid rgba(74, 111, 165, 0.1);
        }

        .subtitle {
            font-size: 1.2rem;
            margin-top: 0.5rem;
            font-weight: 400;
            color: var(--secondary);
            line-height: 1.5;
        }

        /* Section content spacing */
        section > *:first-child {
            margin-top: 0;
        }

        section > p:first-of-type {
            margin-top: 0.5rem;
        }

        /* Lists */
        ul, ol {
            margin-top: 0.5rem;
            margin-bottom: 1rem;
            padding-left: 1.5rem;
        }

        li {
            margin-bottom: 0.5rem;
            line-height: var(--line-height-body);
        }

        /* Examples and components spacing */
        .example:first-child,
        .prompt-example:first-child,
        .response-example:first-child,
        .crisp-component:first-child,
        .interactive-demo:first-child {
            margin-top: 0.5rem;
        }

        /* Code Blocks */
        .code-block {
            background-color: var(--code-bg);
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
            font-family: var(--font-mono);
            font-size: 0.9rem;
            margin: 1rem 0;
            border-left: 4px solid var(--accent);
            white-space: pre-wrap;
            line-height: 1.5;
            box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.05);
        }
        
        /* Examples */
        .example {
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .example:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);
        }

        /* LLM-specific visualizations */
        .context-window {
            width: 100%;
            height: 300px;
            border: 3px solid var(--primary);
            border-radius: 10px;
            position: relative;
            background-color: #eef7ff;
            margin: 1.25rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }

        .prompt-box {
            width: 40%;
            height: 100px;
            background-color: var(--accent);
            border-radius: 8px;
            position: absolute;
            top: 30px;
            left: 30px;
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-weight: bold;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        .response-box {
            width: 70%;
            height: 150px;
            background-color: var(--success);
            border-radius: 8px;
            position: absolute;
            bottom: 30px;
            right: 30px;
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-weight: bold;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        .token-container {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin: 1.25rem 0;
            padding: 1rem;
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .token, .token-alt {
            padding: 0.5rem 0.75rem;
            color: white;
            border-radius: 4px;
            font-family: var(--font-mono);
            font-size: 0.9rem;
            box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }

        .token {
            background-color: var(--primary);
        }

        .token-alt {
            background-color: var(--secondary);
        }

        .token-estimation {
            width: 100%;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.25rem;
            margin: 1.25rem 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .estimation-row {
            display: flex;
            margin: 0.75rem 0;
            border-bottom: 1px solid var(--border);
            padding-bottom: 0.75rem;
        }

        .estimation-label {
            flex: 1;
            font-weight: 500;
            color: var(--primary);
        }

        .estimation-value {
            flex: 2;
        }

        .embedding-space {
            width: 100%;
            height: 300px;
            border: 1px solid var(--border);
            border-radius: 8px;
            position: relative;
            background-color: white;
            margin: 1.25rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .word-node {
            position: absolute;
            width: 80px;
            height: 30px;
            display: flex;
            justify-content: center;
            align-items: center;
            border-radius: 15px;
            background-color: var(--primary);
            color: white;
            font-weight: 500;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .word-connection {
            position: absolute;
            background-color: var(--border);
            transform-origin: 0 0;
            height: 2px;
        }

        .logit-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.25rem 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        .logit-table th, .logit-table td {
            border: 1px solid var(--border);
            padding: 0.75rem;
            text-align: left;
        }

        .logit-table th {
            background-color: var(--bg-alt);
            font-weight: 500;
            color: var(--primary);
        }

        .logit-score {
            display: flex;
            align-items: center;
        }

        .score-bar {
            height: 20px;
            background-color: var(--primary);
            margin-left: 0.75rem;
            border-radius: 3px;
        }

        .output-comparison {
            display: flex;
            gap: 1.25rem;
            margin: 1.25rem 0;
        }

        .output-box {
            flex: 1;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.25rem;
            background: white;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .output-comparison {
                flex-direction: column;
            }

            .token-container {
                padding: 0.75rem;
            }

            .token, .token-alt {
                font-size: 0.8rem;
                padding: 0.4rem 0.6rem;
            }
        }
    </style>
</head>
<body>
    <nav class="module-nav">
        <button class="module-nav-btn" data-section="introduction">Introduction</button>
        <button class="module-nav-btn" data-section="context-window">Context Window</button>
        <button class="module-nav-btn" data-section="tokenization">Tokenization</button>
        <button class="module-nav-btn" data-section="embeddings">Embeddings</button>
        <button class="module-nav-btn" data-section="temperature">Logits & Temperature</button>
        <button class="module-nav-btn" data-section="response-format">Response Format</button>
        <button class="module-nav-btn" data-section="model-selection">LLM Evolution</button>
        <button class="module-nav-btn" data-section="quiz-section">Quiz</button>
    </nav>
    <div class="content-inner">
        <section id="introduction">
            <div class="course-description">
                <h2 class="module-title">Module 1: Understanding Large Language Models</h2>
                <p>Large Language Models (LLMs) are sophisticated AI systems trained on vast amounts of text data that can understand, generate, and manipulate human language. These powerful tools form the foundation of modern AI applications like chatbots, content generators, and virtual assistants.</p>
                <h3 class="section-title">What You'll Learn</h3>
                <p>In this module, you'll master the following key areas:</p>
                <ul>
                    <li><b>Context Window:</b> How LLMs process and limit information</li>
                    <li><b>Tokenization:</b> How text is broken down for model processing</li>
                    <li><b>Embeddings:</b> How LLMs represent meaning and relationships</li>
                    <li><b>Logits & Temperature:</b> How LLMs make predictions and control creativity</li>
                    <li><b>Response Format:</b> How to structure and interpret model outputs</li>
                    <li><b>Model Evolution:</b> Advances in LLM architectures and capabilities</li>
                </ul>
            </div>
            <div class="section-nav-btns">
                <button id="back-introduction" disabled>Back</button>
                <button id="next-introduction">Next</button>
            </div>
        </section>
        <section id="context-window" class="module-section">
            <div class="concept-section">
                <h2>1. Context Window: The Container of Information</h2>
                
                <p><span class="highlight">Concept:</span> The <strong>context window</strong> is the <strong>maximum number of tokens</strong> that can be processed in a single API call, including both the input prompt and the generated output.</p>
                
                <p><span class="highlight">Everyday Example:</span> Think of the context window like a container with a fixed size. Your prompt takes up part of this container, and the remaining space is available for the model's response.</p>
                
                <div class="visual-container">
                    <div class="context-window">
                        <div class="prompt-box">Your Prompt<br>(3,000 tokens)</div>
                        <div class="response-box">Maximum Response Length<br>(5,000 tokens)</div>
                    </div>
                    <p style="text-align: center; margin-top: 10px;">Context Window: 8,000 tokens total</p>
                </div>
                
                <p><span class="highlight">Practical Application:</span> Your <strong>prompt size + maximum desired response length</strong> must fit within the context window. If you request a longer response than available in the context window, the model will <strong>stop generating at the limit</strong>.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-context-window">Back</button>
                <button id="next-context-window">Next</button>
            </div>
        </section>
        <section id="tokenization" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>2. Tokenization: Breaking Down Text</h2>
                
                <p><span class="highlight">Concept:</span> <strong>Tokenization</strong> splits text into small pieces called <strong>tokens</strong> that the model can process within its context window.</p>
                
                <p><span class="highlight">Everyday Example:</span> Think of tokenization like cutting a pizza. The whole pizza is your full text, and the slices are your tokens.</p>
                
                <div class="visual-container">
                    <p>Input Text: "Machine learning is fascinating"</p>
                    
                    <p><strong>Model A tokenization:</strong></p>
                    <div class="token-container">
                        <div class="token">"Machine"</div>
                        <div class="token">" learning"</div>
                        <div class="token">" is"</div>
                        <div class="token">" fascinating"</div>
                    </div>
                    
                    <p><strong>Model B tokenization:</strong></p>
                    <div class="token-container">
                        <div class="token-alt">"Machine"</div>
                        <div class="token-alt">" learn"</div>
                        <div class="token-alt">"ing"</div>
                        <div class="token-alt">" is"</div>
                        <div class="token-alt">" fascin"</div>
                        <div class="token-alt">"ating"</div>
                    </div>
                </div>
                
                <p><span class="highlight">Practical Application:</span> Understanding tokenization helps you <strong>optimize prompts</strong> to fit within context window limits and <strong>manage costs</strong> effectively. For English text: <strong>1 token ≈ 4 characters</strong> or <strong>¾ of a word</strong> on average. Code typically uses <strong>more tokens</strong> than natural language.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-tokenization">Back</button>
                <button id="next-tokenization">Next</button>
            </div>
        </section>
        <section id="embeddings" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>3. Embeddings: Understanding Meaning</h2>
                
                <p><span class="highlight">Concept:</span> <strong>Embeddings</strong> are numerical representations of tokens that capture their <strong>meaning</strong> in a mathematical space.</p>
                
                <p><span class="highlight">Everyday Example:</span> Imagine a map where similar words are clustered together. "Happy" and "joyful" would be neighbors, while "happy" and "sad" would be far apart.</p>
                
                <div class="visual-container">
                    <div class="embedding-space" style="height: 350px; overflow: hidden; position: relative;">
                        <!-- Title labels for clusters -->
                        <div style="position: absolute; top: 10px; left: 200px; font-weight: bold; color: #3498db;">Positive Emotions</div>
                        <div style="position: absolute; top: 190px; left: 160px; font-weight: bold; color: #e74c3c;">Negative Emotions</div>
                        <div style="position: absolute; top: 10px; left: 530px; font-weight: bold; color: #2ecc71;">Animals</div>
                    
                        <!-- Positive emotion cluster -->
                        <div class="word-node" style="top: 50px; left: 150px; background-color: #3498db;">happy</div>
                        <div class="word-node" style="top: 80px; left: 220px; background-color: #3498db;">joyful</div>
                        <div class="word-node" style="top: 60px; left: 280px; background-color: #3498db;">pleased</div>
                        <div class="word-node" style="top: 110px; left: 190px; background-color: #3498db;">delighted</div>
                        
                        <!-- Negative emotion cluster -->
                        <div class="word-node" style="top: 230px; left: 150px; background-color: #e74c3c;">sad</div>
                        <div class="word-node" style="top: 250px; left: 220px; background-color: #e74c3c;">unhappy</div>
                        <div class="word-node" style="top: 270px; left: 170px; background-color: #e74c3c;">gloomy</div>
                        
                        <!-- Animal cluster -->
                        <div class="word-node" style="top: 60px; left: 500px; background-color: #2ecc71;">dog</div>
                        <div class="word-node" style="top: 90px; left: 540px; background-color: #2ecc71;">cat</div>
                        <div class="word-node" style="top: 70px; left: 580px; background-color: #2ecc71;">puppy</div>
                    </div>
                    <p style="text-align: center; font-style: italic;">Words with similar meanings cluster together in embedding space, while different concept groups remain separate</p>
                </div>
                
                <p><span class="highlight">Practical Application:</span> Embeddings allow models to understand <strong>semantic relationships</strong> and make <strong>connections between concepts</strong> that weren't explicitly mentioned.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-embeddings">Back</button>
                <button id="next-embeddings">Next</button>
            </div>
        </section>
        <section id="temperature" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>4. Logits: Making Predictions</h2>
                
                <p><span class="highlight">Concept:</span> <strong>Logits</strong> are raw numerical scores the model assigns to each possible next token before making its final selection. The text generation process has two phases: First, the model processes your prompt to calculate these logits, which are then converted to probabilities for each potential next word. This calculation phase is deterministic - identical inputs always produce the same probability distribution. Second, the model selects tokens from this distribution, either predictably (by always choosing the highest-probability token) or with controlled randomness to balance accuracy with creativity.</p>
                
                <p><span class="highlight">Everyday Example:</span> When completing "The capital of France is ____," a model assigns high scores to relevant answers like "Paris" and low scores to irrelevant options like "banana."</p>
                
                <div class="visual-container">
                    <p><strong>Input:</strong> "The capital of France is"</p>
                    
                    <div style="padding: 20px; background-color: #f8f9fa; border-radius: 8px; border: 1px solid #ddd; margin-bottom: 20px;">
                        <h3 style="margin-top: 0;">How Token Selection Works</h3>
                        
                        <table class="logit-table">
                            <tr>
                                <th>Rank (k)</th>
                                <th>Token</th>
                                <th>Raw Logit</th>
                                <th>Base Probability</th>
                            </tr>
                            <tr>
                                <td><strong>1</strong></td>
                                <td>"Paris"</td>
                                <td>8.2</td>
                                <td>80%</td>
                            </tr>
                            <tr>
                                <td><strong>2</strong></td>
                                <td>"Lyon"</td>
                                <td>4.6</td>
                                <td>10%</td>
                            </tr>
                            <tr>
                                <td><strong>3</strong></td>
                                <td>"Nice"</td>
                                <td>3.9</td>
                                <td>5%</td>
                            </tr>
                            <tr>
                                <td><strong>4</strong></td>
                                <td>"Marseille"</td>
                                <td>3.2</td>
                                <td>3%</td>
                            </tr>
                            <tr>
                                <td><strong>5</strong></td>
                                <td>"banana"</td>
                                <td>-5.0</td>
                                <td>0.1%</td>
                            </tr>
                            <tr>
                                <td><strong>6+</strong></td>
                                <td>Other tokens</td>
                                <td>varies</td>
                                <td>1.9%</td>
                            </tr>
                        </table>
                        
                        <div style="margin-top: 30px; display: flex; gap: 20px;">
                            <div style="flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 8px; background-color: #f1f9fe;">
                                <h4 style="margin-top: 0;">Temperature</h4>
                                <p><strong>Modifies the probability distribution itself. Lower temperatures make the model more deterministic, leading to predictable outputs, while higher temperatures introduce more randomness and creativity</strong></p>
                                <ul>
                                    <li><strong>Low (0.2):</strong> Makes likely tokens even more likely</li>
                                    <li><strong>High (1.0):</strong> Makes distribution more uniform</li>
                                </ul>
                                <div style="font-size: 0.9em; font-style: italic;">With temperature 0.2, "Paris" might be 95% likely</div>
                            </div>
                            
                            <div style="flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 8px; background-color: #f1f9fe;">
                                <h4 style="margin-top: 0;">topP (Nucleus Sampling)</h4>
                                <p><strong>Considers only top tokens that sum to P probability</strong></p>
                                <ul>
                                    <li><strong>topP = 0.9:</strong> Only "Paris", "Lyon", "Nice" considered (95% cumulative)</li>
                                    <li><strong>topP = 0.8:</strong> Only "Paris" considered (80% cumulative)</li>
                                </ul>
                                <div style="font-size: 0.9em; font-style: italic;">It's more flexible than top-K because it dynamically adjusts the number of tokens based on their probabilities</div>
                            </div>
                            
                            <div style="flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 8px; background-color: #f1f9fe;">
                                <h4 style="margin-top: 0;">topK</h4>
                                <p><strong>Considers only K most likely tokens</strong></p>
                                <ul>
                                    <li><strong>topK = 3:</strong> Only "Paris", "Lyon", "Nice" considered</li>
                                    <li><strong>topK = 1:</strong> Only "Paris" considered</li>
                                </ul>
                                <div style="font-size: 0.9em; font-style: italic;">Fixed number regardless of probabilities</div>
                            </div>
                        </div>
                    </div>
                    
                    <p><strong>Combined Effect:</strong> These parameters work together to control selection. Temperature modifies the distribution, then topP and topK filter which tokens can be selected from the modified distribution.</p>
                </div>
                
                <p><span class="highlight">Practical Application:</span> The <strong>temperature</strong>, <strong>topP</strong>, and <strong>topK</strong> parameters control creativity vs. predictability in responses. These parameters let you balance deterministic, factual outputs with more creative, varied responses.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-temperature">Back</button>
                <button id="next-temperature">Next</button>
            </div>
        </section>
        <section id="response-format" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>5. Response and Structured Output</h2>
                
                <p><span class="highlight">Concept:</span> Models can format outputs as either <strong>free-form text</strong> or <strong>structured data</strong> (JSON, XML, etc.).</p>
                
                <p><span class="highlight">Everyday Example:</span> Compare asking for weather information as a casual description versus a formatted weather report with specific fields.</p>
                
                <div class="visual-container">
                    <div class="output-comparison">
                        <div class="output-box">
                            <div class="output-header">Free-form Response</div>
                            <p>"It's sunny and 72°F with light winds from the west."</p>
                            <ul style="font-size: 0.9em; color: #555;">
                                <li>Easy for humans to read</li>
                                <li>Natural conversational style</li>
                                <li>Less predictable structure</li>
                            </ul>
                        </div>
                        
                        <div class="output-box">
                            <div class="output-header">Structured Output (JSON)</div>
                            <div class="json-box">
                                {<br>
                                &nbsp;&nbsp;"weather": {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;"temperature": "72°F",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;"condition": "sunny",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;"wind": {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"speed": "light",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"direction": "west"<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;}<br>
                                &nbsp;&nbsp;}<br>
                                }
                            </div>
                            <ul style="font-size: 0.9em; color: #555;">
                                <li>Machine-readable format</li>
                                <li>Consistent, predictable structure</li>
                                <li>Easy to process programmatically</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <p><span class="highlight">Practical Application:</span> <strong>Structured outputs</strong> are essential when the AI's response needs to be <strong>processed by other systems</strong> rather than read by humans.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-response-format">Back</button>
                <button id="next-response-format">Next</button>
            </div>
        </section>
        <section id="model-selection" class="module-section" style="display:none">
            <div class="concept-section">
                <h2>6. LLM Evolution & Architectural Advances</h2>
                <h4>Early LLM Development (2017-2022)</h4>
                <p>The modern Large Language Model era began with the 2017 paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">"Attention Is All You Need,"</a> which introduced the Transformer architecture. This revolutionary approach replaced recurrent neural networks with three key innovations:</p>
                <ul>
                  <li><b>Self-attention mechanism</b>: Allowing models to connect related words regardless of distance</li>
                  <li><b>Parallel processing</b>: Enabling simultaneous rather than sequential computation</li>
                  <li><b>Flexible architecture</b>: Supporting various NLP tasks through encoder-decoder components</li>
                </ul>
                <p>Following this breakthrough, researchers discovered the "scaling law" phenomenon: model capabilities improve predictably when increasing parameters, training data, and computing power. This insight led to a rapid expansion in model size:</p>
                <table>
                  <thead><tr><th>Year</th><th>Model</th><th>Parameters</th><th>Key Advancement</th></tr></thead>
                  <tbody>
                    <tr><td>2018</td><td>BERT</td><td>340M</td><td>Bidirectional understanding</td></tr>
                    <tr><td>2020</td><td>GPT-3</td><td>175B</td><td>Few-shot learning capabilities</td></tr>
                    <tr><td>2022</td><td>PaLM</td><td>540B</td><td>Improved reasoning abilities</td></tr>
                  </tbody>
                </table>
                <p>The scaling era culminated with the release of ChatGPT on November 30, 2022, which brought LLMs into mainstream use through its user-friendly interface and impressive capabilities.</p>
                <h4>The Rise of Reasoning Models (2023-Present)</h4>
                <p>Around 2023, a new generation of models emerged with enhanced reasoning abilities, representing a significant leap beyond simple pattern recognition. To build these reasoning models, training approaches evolved from basic transformer architectures to include explicit reasoning demonstrations, self-critique methods, and human feedback on multi-step solutions.</p>
                <table>
                  <thead><tr><th>Aspect</th><th>Description</th></tr></thead>
                  <tbody>
                    <tr><td><b>Key Capabilities</b></td><td>&bull; <b>Structured problem-solving</b>: Breaking down complex tasks into clear, logical steps<br>&bull; <b>Self-consistency checking</b>: Detecting and correcting contradictions in their own reasoning<br>&bull; <b>Extended reasoning chains</b>: Following longer, more complex logical arguments<br>&bull; <b>Understanding cause and effect</b>: Recognizing how events relate and influence outcomes</td></tr>
                    <tr><td><b>Current Limitations</b></td><td>&bull; <b>Complex multi-step reasoning</b>: Still struggle with novel mathematical proofs and multi-constraint optimization<br>&bull; <b>Specialized domain knowledge</b>: Difficulty with advanced legal reasoning or medical diagnosis<br>&bull; <b>Spatial reasoning</b>: Inconsistent performance on complex physical systems or 3D visualization problems</td></tr>
                    <tr><td><b>Notable Examples</b></td><td>&bull; <b>ChatGPT o1</b>: OpenAI's model with improved mathematical and logical reasoning<br>&bull; <b>Claude 3.7 Sonnet</b>: Anthropic's model with structured problem-solving capabilities<br>&bull; <b>DeepSeek-R1</b>: Notable for performance on academic reasoning benchmarks</td></tr>
                    <tr><td><b>Real-World Impact</b></td><td>Higher accuracy on complex tasks, fewer hallucinations, and more reliable performance&mdash;making reasoning models the foundation for building autonomous AI agents</td></tr>
                  </tbody>
                </table>
                <blockquote><b>Note:</b> When a model shows its reasoning, all reasoning steps count toward the context window limit and output token costs. Parameters like <b>max_tokens</b> and <b>budget_tokens</b> can control total output length and costs.</blockquote>
                <div class="quiz-question">
                  <p class="question-title">Quiz: Which training approach below is specifically designed to enhance an LLM's reasoning capabilities?</p>
                  <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false">A) Next-token prediction</li>
                    <li class="quiz-option" data-correct="true">B) Chain-of-thought training</li>
                    <li class="quiz-option" data-correct="false">C) Masked language modeling</li>
                    <li class="quiz-option" data-correct="false">D) Decoder-only architecture</li>
                  </ul>
                  <div class="quiz-feedback">
                    <strong>Answer:</strong> B) Chain-of-thought training. This approach teaches models to break down problems into logical steps, improving their reasoning capabilities.
                  </div>
                </div>
                <h4>Current Limitations of LLMs</h4>
                <p>Despite impressive advances, even today's most sophisticated models face significant challenges:</p>
                <table>
                  <thead><tr><th>Limitation Type</th><th>Description</th></tr></thead>
                  <tbody>
                    <tr><td><b>Hallucinations</b></td><td>Generate plausible but factually incorrect information; invent citations; blend facts with fiction</td></tr>
                    <tr><td><b>Knowledge Boundaries</b></td><td>Fixed knowledge cutoffs; limited context windows (8K-200K tokens); inability to verify information</td></tr>
                    <tr><td><b>Reasoning Limitations</b></td><td>Struggle with complex multi-step reasoning; limited mathematical capabilities; domain knowledge gaps</td></tr>
                  </tbody>
                </table>
                <ul>
                  <li><b>Struggle with complex multi-step reasoning</b> (e.g., solving novel mathematical proofs or multi-constraint optimization problems)</li>
                  <li><b>Difficulty with tasks requiring specialized domain knowledge</b> (e.g., advanced legal reasoning or medical diagnosis)</li>
                  <li><b>Inconsistent performance on spatial reasoning tasks</b> (e.g., complex physical systems or 3D visualization problems)</li>
                </ul>
                <div class="quiz-question">
                  <p class="question-title">Assessment: Why do LLMs sometimes hallucinate information, and what approaches can developers take to mitigate this problem? (Select all that apply)</p>
                  <ul class="quiz-options">
                    <li class="quiz-option" data-correct="false">A) LLMs have perfect knowledge but choose to be creative</li>
                    <li class="quiz-option" data-correct="true">B) The statistical nature of prediction sometimes generates plausible but incorrect information</li>
                    <li class="quiz-option" data-correct="true">C) Using retrieval augmentation to ground model responses in verified sources</li>
                    <li class="quiz-option" data-correct="false">D) Training models on larger datasets always eliminates hallucinations</li>
                    <li class="quiz-option" data-correct="true">E) Implementing fact-checking components that verify model outputs</li>
                  </ul>
                  <div class="quiz-feedback">
                    <strong>Answer:</strong> B, C, and E. Hallucinations occur due to the statistical nature of LLMs. Retrieval augmentation and fact-checking can help mitigate this issue.
                  </div>
                </div>
                <h4>Building Applications with LLMs</h4>
                <p>There are three primary approaches to building LLM applications, with increasing levels of sophistication:</p>
                <table style="width:100%; margin-bottom: 24px; text-align:center;">
                  <thead>
                    <tr>
                      <th style="width: 16%;">Attribute</th>
                      <th style="width: 28%;">LLMs + Prompt Engineering</th>
                      <th style="width: 28%;">LLMs + Prompt Engineering + External Knowledge</th>
                      <th style="width: 28%;">Autonomous Agents</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td><b>Visual</b></td>
                      <td><img src="../assets/images/llm-basic.png" alt="Basic LLM interaction with input and output" style="max-width:90%;margin:6px 0;"/></td>
                      <td><img src="../assets/images/llm-knowledge.png" alt="LLM with external knowledge sources" style="max-width:90%;margin:6px 0;"/></td>
                      <td><img src="../assets/images/agent-cycle.png" alt="Agent architecture with observation-action cycle" style="max-width:90%;margin:6px 0;"/></td>
                    </tr>
                    <tr>
                      <td><b>Description</b></td>
                      <td>Directly interact with the LLM using carefully crafted prompts.</td>
                      <td>Enhance LLMs with real-time or domain-specific information from external sources (databases, APIs, documents).</td>
                      <td>Combine LLMs with tools, memory, and decision-making to create systems that can plan, act, and adapt autonomously.</td>
                    </tr>
                    <tr>
                      <td><b>Best For</b></td>
                      <td>Simple, self-contained tasks that rely on general knowledge.</td>
                      <td>Information-rich tasks requiring up-to-date, specialized, or proprietary data.</td>
                      <td>Complex, multi-step workflows that require reasoning, tool use, and adaptive planning.</td>
                    </tr>
                    <tr>
                      <td><b>Use Cases</b></td>
                      <td>Text summarization, content generation, classification, translation.</td>
                      <td>Company data Q&amp;A, research over large document sets, market analysis.</td>
                      <td>Personal assistants, automated research agents, workflow automation, multi-tool orchestration.</td>
                    </tr>
                    <tr>
                      <td><b>Complexity</b></td>
                      <td>Low</td>
                      <td>Medium</td>
                      <td>High</td>
                    </tr>
                    <tr>
                      <td><b>Limitations</b></td>
                      <td>Limited by model's context window and training data cutoff; cannot access new or proprietary information.</td>
                      <td>Dependent on quality and freshness of external data; requires robust retrieval and integration.</td>
                      <td>Increased system complexity; harder to debug; may have higher latency and operational overhead.</td>
                    </tr>
                  </tbody>
                </table>
                <p>These three approaches represent a continuum of complexity and capability that we'll explore in greater detail in subsequent modules.</p>
                <h4>Future Research Directions</h4>
                <p>The field is rapidly evolving beyond current LLM limitations, with several promising research directions that could transform how we build AI applications:</p>
                <table>
                  <thead><tr><th>Research Area</th><th>Description</th><th>Potential Real-World Impact</th><th>Reference</th></tr></thead>
                  <tbody>
                    <tr><td><b>Neuro-Symbolic Integration</b></td><td>Combining traditional symbolic systems with neural networks</td><td>Could enhance reasoning capabilities while maintaining interpretability</td><td><a href="https://arxiv.org/abs/2501.05435" target="_blank">Neuro-Symbolic AI in 2024: A Systematic Review</a></td></tr>
                    <tr><td><b>JEPA (Joint Embedding Predictive Architecture)</b></td><td>Yann LeCun's approach focusing on predicting abstract representations rather than raw outputs</td><td>May enable more efficient learning with less data and better understanding of causality</td><td><a href="https://arxiv.org/abs/2403.00504" target="_blank">Learning and Leveraging World Models in Visual Representation Learning (2024)</a></td></tr>
                    <tr><td><b>World Models</b></td><td>Systems that build internal representations of physical environments to predict outcomes and plan actions</td><td>Could enable AI to better understand physical reality and spatial relationships for robotics and embodied AI</td><td><a href="https://www.constellationr.com/blog-news/insights/physical-ai-world-foundation-models-will-move-forefront" target="_blank">Nvidia's Cosmos World Foundation Models (2025)</a></td></tr>
                  </tbody>
                </table>
                <p>These research areas could address some of the current limitations of autonomous agents and reduce the engineering overhead for building systems that can work on complex tasks while interacting with both physical and digital worlds.</p>
            </div>
            <div class="section-nav-btns">
                <button id="back-model-selection">Back</button>
                <button id="next-model-selection">Next</button>
            </div>
        </section>
        <section id="quiz-section" class="module-section" style="display:none">
            <div class="quiz-section">
                <h2 class="quiz-title">Concept Check Questions</h2>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q1">
                        <p class="question-title">1. Context Window: If a model has a context window of 16,000 tokens, and your prompt uses 7,500 tokens, how many tokens remain available for the response?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) 7,500 tokens</li>
                            <li class="quiz-option" data-correct="true">B) 8,500 tokens</li>
                            <li class="quiz-option" data-correct="false">C) 16,000 tokens</li>
                            <li class="quiz-option" data-correct="false">D) 24,500 tokens</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) 8,500 tokens. The remaining space is calculated by subtracting the prompt size (7,500) from the total context window size (16,000).
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q2">
                        <p class="question-title">2. Tokenization: Which would likely use more tokens?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) Common English words in a short sentence</li>
                            <li class="quiz-option" data-correct="true">B) Technical jargon and rare terminology</li>
                            <li class="quiz-option" data-correct="false">C) Simple numbers (1, 2, 3)</li>
                            <li class="quiz-option" data-correct="false">D) All options use exactly the same number of tokens</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) Technical jargon and rare terminology. Uncommon words are often broken into multiple tokens, whereas common words are typically represented as single tokens.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q3">
                        <p class="question-title">3. Token Costs: A model charges $0.01 per 1K input tokens and $0.02 per 1K output tokens. What's the approximate cost of processing 10 documents (1,000 words each) with 200-word summaries?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) $0.10</li>
                            <li class="quiz-option" data-correct="false">B) $0.30</li>
                            <li class="quiz-option" data-correct="true">C) $1.65</li>
                            <li class="quiz-option" data-correct="false">D) $3.00</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> C) $1.65. Each 1,000-word document is approximately 1,300 tokens (input) and each 200-word summary is approximately 260 tokens (output). Total: 10 × (1,300 × $0.01/1K + 260 × $0.02/1K) = $0.13 + $0.052 = $0.182 per document × 10 documents ≈ $1.65.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q4">
                        <p class="question-title">4. Embeddings: What makes embeddings powerful for understanding language?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) They contain the dictionary definition of each word</li>
                            <li class="quiz-option" data-correct="true">B) They represent words as points in space where similar words are closer together</li>
                            <li class="quiz-option" data-correct="false">C) They store grammar rules for proper sentence construction</li>
                            <li class="quiz-option" data-correct="false">D) They directly translate between different languages</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) They represent words as points in space where similar words are closer together. This allows the model to understand relationships between concepts and generalize to new situations.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q5">
                        <p class="question-title">5. Logits: When would you use a high temperature setting?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="true">A) When generating creative stories or poetry</li>
                            <li class="quiz-option" data-correct="false">B) When performing factual question answering</li>
                            <li class="quiz-option" data-correct="false">C) When extracting structured data from text</li>
                            <li class="quiz-option" data-correct="false">D) When performing mathematical calculations</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> A) When generating creative stories or poetry. Higher temperature settings introduce more randomness, allowing for more creative and varied outputs.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q6">
                        <p class="question-title">6. Response and Structured Output: Which scenario would benefit most from a structured output format?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) A bedtime story for children</li>
                            <li class="quiz-option" data-correct="false">B) A personalized email response</li>
                            <li class="quiz-option" data-correct="true">C) Data extraction for a financial dashboard</li>
                            <li class="quiz-option" data-correct="false">D) A creative description of a landscape</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> C) Data extraction for a financial dashboard. Structured output formats like JSON allow other systems to easily process and display the information without needing to parse natural language.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q7">
                        <p class="question-title">7. Reasoning in Foundational Models: Which approach would likely yield the most accurate answer to a multi-step math problem?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="false">A) Asking for just the final answer</li>
                            <li class="quiz-option" data-correct="true">B) Requesting step-by-step reasoning</li>
                            <li class="quiz-option" data-correct="false">C) Using the highest temperature setting</li>
                            <li class="quiz-option" data-correct="true">D) Using the lowest temperature setting</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> B) Requesting step-by-step reasoning and D) Using the lowest temperature setting. Step-by-step reasoning allows the model to work through the problem methodically, catching errors in its reasoning process. A low temperature setting increases determinism and reduces creativity, which is beneficial for mathematical accuracy.
                        </div>
                    </div>
                </div>
                
                <div class="quiz-container">
                    <div class="quiz-question" id="q8">
                        <p class="question-title">8. Claude 3.7 Sonnet Specifications: What task would specifically benefit from Claude 3.7 Sonnet's large context window?</p>
                        <ul class="quiz-options">
                            <li class="quiz-option" data-correct="true">A) Analyzing an entire legal contract at once</li>
                            <li class="quiz-option" data-correct="false">B) Generating a single paragraph response</li>
                            <li class="quiz-option" data-correct="false">C) Converting a short text to JSON</li>
                            <li class="quiz-option" data-correct="false">D) Translating a single sentence</li>
                        </ul>
                        <div class="quiz-feedback">
                            <strong>Answer:</strong> A) Analyzing an entire legal contract at once. Claude 3.7 Sonnet's 200,000 token context window allows it to process lengthy documents entirely, maintaining understanding of references and relationships throughout the text.
                        </div>
                    </div>
                </div>
            </div>
            <div class="section-nav-btns">
                <button id="back-quiz-section">Back</button>
                <button id="next-quiz-section" disabled>Next</button>
            </div>
        </section>
    </div>
    <script src="../assets/js/course-nav.js"></script>
    <script src="../assets/js/module-sidebar.js"></script>
    <script src="../assets/js/quiz.js"></script>
</body>
</html>
