{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Foundations: Hands-On Lab Notebook\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand and experiment with the concepts of context window, tokenization, embeddings, logits, temperature, and response formats in Large Language Models (LLMs).\n",
    "- Use AWS Bedrock to interact with foundational models (preferably Claude Sonnet, or other Claude/Nova/Titan models).\n",
    "- Visualize and manipulate LLM parameters to see their effects on outputs.\n",
    "- Run the lab in both MyBinder and AWS SageMaker environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook is compatible with both [MyBinder](https://mybinder.org/) and AWS SageMaker.\n",
    "\n",
    "**Environment Notes:**\n",
    "- **MyBinder:** You must provide your own AWS credentials (see below).\n",
    "- **AWS SageMaker:** Credentials are pre-configured in your environment.\n",
    "\n",
    "**Install dependencies:**\n",
    "```python\n",
    "!pip install boto3 matplotlib scikit-learn\n",
    "```\n",
    "\n",
    "**AWS Credentials Setup:**\n",
    "- **MyBinder:** Set your credentials as environment variables in a cell (do NOT share credentials):\n",
    "```python\n",
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'  # or your region\n",
    "```\n",
    "- **SageMaker:** No action needed unless you want to override the default region.\n",
    "\n",
    "**Licensing Notes:**\n",
    "- [boto3](https://github.com/boto/boto3) (Apache 2.0)\n",
    "- [matplotlib](https://matplotlib.org/stable/users/project/license.html) (PSF)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/about.html#license) (BSD)\n",
    "- Any custom code in this notebook is MIT licensed.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set the Claude model ID to use for all Bedrock calls\n",
    "# Update this value as needed for your Bedrock account.\n",
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'  # Example: Claude 3.7 Sonnet\n",
    "# You can find available model IDs using the earlier Bedrock listing cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context Window Exploration\n",
    "\n",
    "**What is a context window?**\n",
    "The context window is the maximum number of tokens (input + output) a model can consider at once. If your prompt is too long, there will be less room for the model's response. If you exceed the limit, the model will stop or truncate its output.\n",
    "\n",
    "**Activity Instructions:**\n",
    "- Try different prompt lengths and response lengths.\n",
    "- Observe what happens as you approach or exceed the model's context window.\n",
    "- Note: Most modern LLMs have context windows between 8,000 and 200,000 tokens.\n",
    "\n",
    "**Expected Output:**\n",
    "- The model's response, or an error/truncation if the context window is exceeded.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Connect to AWS Bedrock and list available Claude models\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "\n",
    "region = os.environ.get('AWS_DEFAULT_REGION', 'us-east-1')\n",
    "bedrock = boto3.client('bedrock-runtime', region_name=region)\n",
    "bedrock_available = True\n",
    "claude_models = []\n",
    "try:\n",
    "    resp = bedrock.list_foundation_models()\n",
    "    claude_models = [m for m in resp.get('modelSummaries', []) if 'anthropic' in m['modelId'].lower()]\n",
    "    if not claude_models:\n",
    "        print('Claude models not found in your Bedrock account.')\n",
    "        bedrock_available = False\n",
    "    else:\n",
    "        print('Available Claude models:')\n",
    "        for m in claude_models:\n",
    "            print('-', m['modelId'])\n",
    "except (NoCredentialsError, ClientError) as e:\n",
    "    print('Bedrock/Claude not available or credentials missing:', e)\n",
    "    bedrock_available = False\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment: Try different prompt and response lengths\n",
    "if bedrock_available and claude_models:\n",
    "    model_id = claude_models[0]['modelId']\n",
    "    prompt = input('Enter your prompt: ')\n",
    "    max_tokens = int(input('Max response tokens (e.g., 100): '))\n",
    "    body = {\n",
    "        'prompt': prompt,\n",
    "        'max_tokens_to_sample': max_tokens\n",
    "    }\n",
    "    import json\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(body),\n",
    "        contentType='application/json'\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    print('Model response:')\n",
    "    print(result.get('completion', result))\n",
    "else:\n",
    "    print('Bedrock/Claude not available or no Claude models found. Skipping this cell.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "**What is tokenization?**\n",
    "Tokenization is the process of splitting text into small pieces called tokens, which the model can process. Different models may tokenize the same text differently.\n",
    "\n",
    "**Activity Instructions:**\n",
    "- Enter different sentences and see how they are tokenized.\n",
    "- Compare token counts for simple vs. complex/technical text.\n",
    "- Try sentences with common words, rare words, numbers, and punctuation.\n",
    "\n",
    "**Expected Output:**\n",
    "- The list of tokens and the total token count for your input.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tokenization demo: Try different sentences\n",
    "# If tiktoken is available, use it (MIT license). Otherwise, use a simple whitespace/character tokenizer.\n",
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding('cl100k_base')  # OpenAI's encoding, works for most LLMs\n",
    "    def tokenize(text):\n",
    "        return enc.encode(text)\n",
    "    def detokenize(tokens):\n",
    "        return enc.decode(tokens)\n",
    "    print('Using tiktoken for tokenization.')\n",
    "except ImportError:\n",
    "    def tokenize(text):\n",
    "        # Simple fallback: split on whitespace and punctuation\n",
    "        import re\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    def detokenize(tokens):\n",
    "        return ' '.join(tokens)\n",
    "    print('tiktoken not found. Using simple fallback tokenizer.')\n",
    "\n",
    "# Student input\n",
    "text = input('Enter a sentence to tokenize: ')\n",
    "tokens = tokenize(text)\n",
    "print('Tokens:', tokens)\n",
    "print('Token count:', len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings Visualization\n",
    "\n",
    "**What are embeddings?**\n",
    "Embeddings are numerical vector representations of text that capture meaning and relationships. Similar sentences have similar embeddings.\n",
    "\n",
    "**Activity Instructions:**\n",
    "- See how different sentences are represented as vectors.\n",
    "- Visualize these vectors in 2D space.\n",
    "- Sentences with similar meaning should cluster together.\n",
    "\n",
    "**Note:** If AWS Bedrock's embedding API is not available, this activity will use dummy data.\n",
    "\n",
    "**Expected Output:**\n",
    "- A 2D scatter plot showing the relative positions of sentence embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Embeddings visualization: Use Bedrock if available, else dummy data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sentences = [\n",
    "    'I love machine learning.',\n",
    "    'Artificial intelligence is the future.',\n",
    "    'The cat sat on the mat.',\n",
    "    'Dogs are loyal pets.',\n",
    "    'Deep learning powers LLMs.',\n",
    "    'Birds can fly.'\n",
    "]\n",
    "\n",
    "# Dummy embedding generator\n",
    "def get_dummy_embeddings(sentences, dim=8):\n",
    "    np.random.seed(42)\n",
    "    return np.random.randn(len(sentences), dim)\n",
    "\n",
    "# Try to use Bedrock embeddings (if available in the future)\n",
    "bedrock_embeddings_available = False  # Set to True if API becomes available\n",
    "if bedrock_embeddings_available:\n",
    "    # Placeholder for Bedrock embedding API call\n",
    "    # embeddings = ...\n",
    "    print('Bedrock embeddings API not implemented in this example.')\n",
    "    embeddings = get_dummy_embeddings(sentences)\n",
    "else:\n",
    "    print('Using dummy embeddings for visualization.')\n",
    "    embeddings = get_dummy_embeddings(sentences)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], color='#3498db')\n",
    "for i, txt in enumerate(sentences):\n",
    "    plt.annotate(txt, (embeddings_2d[i,0], embeddings_2d[i,1]), fontsize=9, alpha=0.8)\n",
    "plt.title('Sentence Embeddings (2D PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logits, Temperature, and Sampling\n",
    "\n",
    "**What are logits?**\n",
    "Logits are raw scores assigned by the model to each possible next token. These are converted to probabilities using the softmax function.\n",
    "\n",
    "**Temperature, top_p, and top_k:**\n",
    "- **Temperature** controls randomness: low values make the model more deterministic, high values increase creativity.\n",
    "- **top_p (nucleus sampling)** and **top_k** further control which tokens can be selected.\n",
    "\n",
    "**Activity Instructions:**\n",
    "- Experiment with different temperature values and see how the probability distribution changes.\n",
    "- Observe which tokens become more or less likely as you adjust temperature.\n",
    "\n",
    "**Expected Output:**\n",
    "- A bar chart showing token probabilities for a given set of logits and temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Logits, temperature, and softmax demo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(logits, temperature=1.0):\n",
    "    scaled = np.array(logits) / temperature\n",
    "    exps = np.exp(scaled - np.max(scaled))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "logits = [8.2, 4.6, 3.9, 3.2, -5.0]\n",
    "tokens = ['Paris', 'Lyon', 'Nice', 'Marseille', 'banana']\n",
    "\n",
    "def plot_probs(temperature):\n",
    "    probs = softmax(logits, temperature)\n",
    "    plt.figure(figsize=(7,3))\n",
    "    plt.bar(tokens, probs, color='#3498db')\n",
    "    plt.title(f'Softmax Probabilities (Temperature={temperature})')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "# Try different temperatures\n",
    "plot_probs(1.0)  # Default\n",
    "# Students: Try plot_probs(0.5) or plot_probs(1.5) to see the effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment: Claude Sonnet 3.7 with Temperature, top_p, and top_k\n",
    "\n",
    "In this lab, you'll experiment with the Claude Sonnet 3.7 model using different values for temperature, top_p, and top_k.\n",
    "\n",
    "- **Temperature**: Controls randomness. Lower values (e.g., 0.2) make responses more deterministic; higher values (e.g., 1.0) increase creativity.\n",
    "- **top_p (nucleus sampling)**: Limits the model to the smallest set of tokens whose cumulative probability is at least top_p.\n",
    "- **top_k**: Limits the model to the top k most likely tokens.\n",
    "\n",
    "**Instructions:**\n",
    "- Enter a prompt and try different combinations of temperature, top_p, and top_k.\n",
    "- Run the cell multiple times and compare the diversity and style of the responses.\n",
    "- Observe how the output changes as you adjust each parameter.\n",
    "\n",
    "**Expected Output:**\n",
    "- The model's response for each set of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment with Claude Sonnet 3.7: temperature, top_p, top_k\n",
    "if bedrock_available and claude_models:\n",
    "    # Try to select Claude Sonnet 3.7\n",
    "    sonnet_models = [m for m in claude_models if 'sonnet' in m['modelId'].lower()]\n",
    "    if sonnet_models:\n",
    "        model_id = sonnet_models[0]['modelId']\n",
    "        print(f'Using model: {model_id}')\n",
    "        prompt = input('Enter your prompt: ')\n",
    "        temperature = float(input('Temperature (e.g., 0.2 to 1.0): '))\n",
    "        top_p = float(input('top_p (e.g., 0.8 to 1.0): '))\n",
    "        top_k = int(input('top_k (e.g., 1 to 50): '))\n",
    "        body = {\n",
    "            'prompt': prompt,\n",
    "            'max_tokens_to_sample': 200,\n",
    "            'temperature': temperature,\n",
    "            'top_p': top_p,\n",
    "            'top_k': top_k\n",
    "        }\n",
    "        import json\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        result = json.loads(response['body'].read())\n",
    "        print('Model response:')\n",
    "        print(result.get('completion', result))\n",
    "    else:\n",
    "        print('Claude Sonnet 3.7 not found in your Bedrock account.')\n",
    "else:\n",
    "    print('Bedrock/Claude not available or no Claude models found. Skipping this cell.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Format: Free-form vs. Structured Output\n",
    "\n",
    "LLMs can return responses as free-form text or in a structured format (like JSON).\n",
    "\n",
    "- **Free-form text** is natural and easy for humans to read, but harder for programs to process.\n",
    "- **Structured output** (e.g., JSON) is predictable and easy for other software to use.\n",
    "\n",
    "**Instructions:**\n",
    "- Enter a prompt and try it first as a free-form request, then as a request for structured (JSON) output.\n",
    "- Compare the results.\n",
    "- Example: Ask for a weather report as both a natural sentence and as a JSON object.\n",
    "\n",
    "**Expected Output:**\n",
    "- The model's response in both formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Response format: Free-form vs. structured (JSON)\n",
    "if bedrock_available and claude_models:\n",
    "    sonnet_models = [m for m in claude_models if 'sonnet' in m['modelId'].lower()]\n",
    "    if sonnet_models:\n",
    "        model_id = sonnet_models[0]['modelId']\n",
    "        print(f'Using model: {model_id}')\n",
    "        prompt = input('Enter your prompt: ')\n",
    "        structured = input('Request structured output? (yes/no): ').strip().lower() == 'yes'\n",
    "        if structured:\n",
    "            prompt += '\n\nPlease format your response as a JSON object.'\n",
    "        body = {\n",
    "            'prompt': prompt,\n",
    "            'max_tokens_to_sample': 200,\n",
    "            'temperature': 0.5\n",
    "        }\n",
    "        import json\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        result = json.loads(response['body'].read())\n",
    "        print('Model response:')\n",
    "        print(result.get('completion', result))\n",
    "    else:\n",
    "        print('Claude Sonnet 3.7 not found in your Bedrock account.')\n",
    "else:\n",
    "    print('Bedrock/Claude not available or no Claude models found. Skipping this cell.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
} 